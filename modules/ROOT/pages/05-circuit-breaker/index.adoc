= Circuit Breaker
:figures: 05-circuit-breaker

There might be cases where you don't want to keep trying
future requests to a given service after you know it's failing. By doing that, you can
save time wasted in response timeouts and alleviate potential congestion of the
target service. This is especially useful for external service calls when there are no
other resilience mechanisms in place like the service registry with health checks.
For these scenarios, you can use a circuit breaker. The circuit is closed when
everything works fine. After a configurable number of request failures, the circuit
becomes open. Then, the requests are not even tried, and the circuit breaker
implementation returns a predefined response. Now and then, the circuit may
switch to half-open to check again if the target service is working. In that case,
the circuit will transition to close. If it's still failing, it goes back to the open state.

The retry pattern is useful when a downstream service is momentarily unavailable. But
what if it stays down for more than a few instants? At that point we could stop forwarding requests to it until we’re sure that it’s back. Continuing to send requests won’t be
beneficial for the caller or the callee. In that scenario, the circuit breaker pattern
comes in handy. That’s the topic of the next section. 

A circuit breaker ensures fault tolerance when a downstream 
service exceeds the maximum number of failures allowed by blocking any 
communication between upstream and downstream services. The logic is 
based on three states: closed, open, and half-open.

In the world of distributed systems, you can establish circuit breakers at the integration points between components. Think about Edge Service and Catalog Service. In a
typical scenario, the circuit is closed, meaning that the two services can interact over the
network. For each server error response returned by Catalog Service, the circuit breaker
in Edge Service would register the failure. When the number of failures exceeds a certain threshold, the circuit breaker trips, and the circuit transitions to open.
 While the circuit is open, communications between Edge Service and Catalog Service are not allowed. Any request that should be forwarded to Catalog Service will fail
right away. In this state, either an error is returned to the client, or fallback logic is
executed. After an appropriate amount of time to permit the system to recover, the
circuit breaker transitions to a half-open state, allowing the next call to Catalog Service
to go through. That is an exploratory phase to check if there are still issues in contacting the downstream service. If the call succeeds, the circuit breaker is reset and transitions to closed. Otherwise it goes back to being open. 

Unlike with retries, when the circuit breaker trips, no calls to the downstream service
are allowed anymore. Like with retries, the circuit breaker’s behavior depends on a
threshold and a timeout, and it lets you define a fallback method to call. The goal of
resilience is to keep the system available to users, even in the face of failures. In the
worst-case scenario, like when a circuit breaker trips, you should guarantee a graceful
degradation. You can adopt different strategies for the fallback method. For example,
you might decide to return a default value or the last available value from a cache, in
case of a GET request.

the classic design of a circuit breaker, as illustrated in the following state 
diagram:
image::{figures}/circuit-breaker-state-diagram.png[Circuit breaker state diagram]
1. A circuit breaker starts as Closed, allowing requests to be processed.
2. As long as the requests are processed successfully, it stays in the Closed state.
3. If failures start to happen, a counter starts to count up.
4. If a threshold of failures is reached within a specified period of time, the circuit breaker 
will trip, that is, go to the Open state, not allowing further requests to be processed. Both 
the threshold of failures and the period of time are configurable.
5. Instead, a request will fast fail, meaning it will return immediately with an exception.
6. After a configurable period of time, the circuit breaker will enter a Half Open state and 
allow one request to go through, as a probe, to see whether the failure has been resolved.
7. If the probe request fails, the circuit breaker goes back to the Open state.
8. If the probe request succeeds, the circuit breaker goes to the initial Closed state, allowing 
new requests to be processed.

Let’s assume we have a REST service, called myService, that is protected by a circuit breaker 
using Resilience4j.
If the service starts to produce internal errors – for example, because it can’t reach a service it 
depends on – we might get a response from the service such as 500 Internal Server Error. 
After a number of configurable attempts, the circuit will open and we will get a fast failure that 
returns an error message such as CircuitBreaker 'myService' is open. When the error is 
resolved and we make a new attempt (after the configurable wait time), the circuit breaker will 
allow a new attempt as a probe. If the call succeeds, the circuit breaker will be closed again; that 
is, operating normally.


== Resilient applications with Spring
In a fairly large-scale system landscape of cooperating microservices, we must assume that there 
is something going wrong all of the time. Failures must be seen as a normal state, and the system 
landscape must be designed to handle it!

When using Resilience4j together with Spring Boot, we will be able to monitor the state of the 
circuit breakers in a microservice using its Spring Boot Actuator health endpoint. If it operates 
normally, that is, the circuit is closed, it will respond with something such as the following:
{
	"status": "UP",
	"components": {
		"circuitBreakers": {
			"status": "UP",
			"details": {
				"myService": {
					"status": "UP",
					"details": {
						"state": "CLOSED"
					}
				}
			}
		}
	}
}
If something is wrong and the circuit is open, it will respond with something such as the following:
{
	"status": "DOWN",
	"components": {
		"circuitBreakers": {
			"status": "DOWN",
			"details": {
				"myService": {
					"status": "DOWN",
					"details": {
						"state": "OPEN"
					}
				}
			}
		}
	}
}
=== Timeouts
Here are some examples of timeouts:

* Connection timeout—This is the time limit for establishing a communication
channel with a remote resource. you can configure the server.netty.connection-timeout property to limit the time Netty waits for a TCP connection to be established.
* Connection pool timeout—This is the time limit for a client to get a connection
from a pool. you can configure  a timeout for the Hikari connection
pool through the spring.datasource.hikari.connection-timeout property.
* Read timeout—This is the time limit for reading from a remote resource after
establishing the initial connection.you can for example define a read
timeout for the call to the Catalog Service performed by the BookClient class.

== Libraries

=== Resilience4j
Resilience4j is an open source-based fault tolerance library. It comes with a larger range of fault tolerance mechanisms:

• A circuit breaker is used to prevent a chain of failure reaction if a remote service stops 
responding.
• A rate limiter is used to limit the number of requests to a service during a specified time 
period.
• A bulkhead is used to limit the number of concurrent requests to a service.
• Retries are used to handle random errors that might happen from time to time.
• A time limiter is used to avoid waiting too long for a response from a slow or unrespon-
sive service.

=== Spring Cloud Circuit Breaker
The Spring Cloud Circuit Breaker project provides an abstraction for defining
circuit breakers in a Spring application. You can choose between reactive and nonreactive implementations based on Resilience4J (https://resilience4j.readme.io). Netflix Hystrix was the popular choice for microservices architectures, but it entered maintenance mode back in 2018. After that, Resilience4J became the preferred choice
because it provides the same features offered by Hystrix and more.

For simplicity, the fallback for GET requests returns an empty string, whereas the fallback for POST requests returns an HTTP 503 error. In a real scenario, you might want
to adopt different fallback strategies depending on the context, including throwing a
custom exception to be handled from the client or returning the last value saved in
the cache for the original request.

[source,java,attributes]
----
import reactor.core.publisher.Mono;

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.http.HttpStatus;
import org.springframework.web.reactive.function.server.RouterFunction;
import org.springframework.web.reactive.function.server.RouterFunctions;
import org.springframework.web.reactive.function.server.ServerResponse;

@Configuration
public class WebEndpoints {

	@Bean
	public RouterFunction<ServerResponse> routerFunction() {
		return RouterFunctions.route()
                // Fallback response used to handle the GET endpoint
				.GET("/catalog-fallback", request ->
						ServerResponse.ok().body(Mono.just(""), String.class))
                // Fallback response used to handle the POST endpoint
				.POST("/catalog-fallback", request ->
						ServerResponse.status(HttpStatus.SERVICE_UNAVAILABLE).build())
				.build();
	}
	
}
----


