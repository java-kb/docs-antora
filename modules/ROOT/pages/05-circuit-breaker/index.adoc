= Circuit Breaker
:figures: 05-circuit-breaker

There might be cases where you don't want to keep trying
future requests to a given service after you know it's failing. By doing that, you can
save time wasted in response timeouts and alleviate potential congestion of the
target service. This is especially useful for external service calls when there are no
other resilience mechanisms in place like the service registry with health checks.
For these scenarios, you can use a circuit breaker. The circuit is closed when
everything works fine. After a configurable number of request failures, the circuit
becomes open. Then, the requests are not even tried, and the circuit breaker
implementation returns a predefined response. Now and then, the circuit may
switch to half-open to check again if the target service is working. In that case,
the circuit will transition to close. If it's still failing, it goes back to the open state.

The retry pattern is useful when a downstream service is momentarily unavailable. But
what if it stays down for more than a few instants? At that point we could stop forward-
ing requests to it until we’re sure that it’s back. Continuing to send requests won’t be
beneficial for the caller or the callee. In that scenario, the circuit breaker pattern
comes in handy. That’s the topic of the next section. 

A circuit breaker ensures fault tolerance when a downstream 
service exceeds the maximum number of failures allowed by blocking any 
communication between upstream and downstream services. The logic is 
based on three states: closed, open, and half-open.

In the world of distributed systems, you can establish circuit breakers at the integra-
tion points between components. Think about Edge Service and Catalog Service. In a
typical scenario, the circuit is closed, meaning that the two services can interact over the
network. For each server error response returned by Catalog Service, the circuit breaker
in Edge Service would register the failure. When the number of failures exceeds a cer-
tain threshold, the circuit breaker trips, and the circuit transitions to open.
 While the circuit is open, communications between Edge Service and Catalog Ser-
vice are not allowed. Any request that should be forwarded to Catalog Service will fail
right away. In this state, either an error is returned to the client, or fallback logic is
executed. After an appropriate amount of time to permit the system to recover, the
circuit breaker transitions to a half-open state, allowing the next call to Catalog Service
to go through. That is an exploratory phase to check if there are still issues in con-
tacting the downstream service. If the call succeeds, the circuit breaker is reset and transitions to closed. Otherwise it goes back to being open. 

Unlike with retries, when the circuit breaker trips, no calls to the downstream service
are allowed anymore. Like with retries, the circuit breaker’s behavior depends on a
threshold and a timeout, and it lets you define a fallback method to call. The goal of
resilience is to keep the system available to users, even in the face of failures. In the
worst-case scenario, like when a circuit breaker trips, you should guarantee a graceful
degradation. You can adopt different strategies for the fallback method. For example,
you might decide to return a default value or the last available value from a cache, in
case of a GET request.

== Resilient applications with Spring
=== Timeouts
Here are some examples of timeouts:

* Connection timeout—This is the time limit for establishing a communication
channel with a remote resource. you can configure the server.netty.connection-timeout property to limit the time Netty waits for a TCP connec-
tion to be established.
* Connection pool timeout—This is the time limit for a client to get a connection
from a pool. you can configure  a timeout for the Hikari connection
pool through the spring.datasource.hikari.connection-timeout property.
* Read timeout—This is the time limit for reading from a remote resource after
establishing the initial connection.you can for example define a read
timeout for the call to the Catalog Service performed by the BookClient class.
== Libraries

=== Resilience4j

=== Spring Cloud Circuit Breaker
The Spring Cloud Circuit Breaker project provides an abstraction for defining
circuit breakers in a Spring application. You can choose between reactive and non-
reactive implementations based on Resilience4J (https://resilience4j.readme.io). Net-
flix Hystrix was the popular choice for microservices architectures, but it entered main-
tenance mode back in 2018. After that, Resilience4J became the preferred choice
because it provides the same features offered by Hystrix and more.

For simplicity, the fallback for GET requests returns an empty string, whereas the fall-
back for POST requests returns an HTTP 503 error. In a real scenario, you might want
to adopt different fallback strategies depending on the context, including throwing a
custom exception to be handled from the client or returning the last value saved in
the cache for the original request.

[source,java,attributes]
----
import reactor.core.publisher.Mono;

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.http.HttpStatus;
import org.springframework.web.reactive.function.server.RouterFunction;
import org.springframework.web.reactive.function.server.RouterFunctions;
import org.springframework.web.reactive.function.server.ServerResponse;

@Configuration
public class WebEndpoints {

	@Bean
	public RouterFunction<ServerResponse> routerFunction() {
		return RouterFunctions.route()
                // Fallback response used to handle the GET endpoint
				.GET("/catalog-fallback", request ->
						ServerResponse.ok().body(Mono.just(""), String.class))
                // Fallback response used to handle the POST endpoint
				.POST("/catalog-fallback", request ->
						ServerResponse.status(HttpStatus.SERVICE_UNAVAILABLE).build())
				.build();
	}
	
}
----


