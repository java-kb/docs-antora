== link:/blog/rate-limiters[Scaling your API with rate limiters]
:figures: 04-gateway/articles/article04

_Source: https://stripe.com/blog/rate-limiters_

Availability and reliability are paramount for all web applications and
APIs. If you’re providing an API, chances are you’ve already experienced
sudden increases in traffic that affect the quality of your service,
potentially even leading to a service outage for all your users.

The first few times this happens, it’s reasonable to just add more
capacity to your infrastructure to accommodate user growth. However,
when you’re running a production API, not only do you have to make it
robust with techniques like
https://stripe.com/blog/idempotency[idempotency], you also need to build
for scale and ensure that one bad actor can’t accidentally or
deliberately affect its availability.

Rate limiting can help make your API more reliable in the following
scenarios:

* One of your users is responsible for a spike in traffic, and you need
to stay up for everyone else.
* One of your users has a misbehaving script which is accidentally
sending you a lot of requests. Or, even worse, one of your users is
intentionally trying to overwhelm your servers.
* A user is sending you a lot of lower-priority requests, and you want
to make sure that it doesn’t affect your high-priority traffic. For
example, users sending a high volume of requests for analytics data
could affect critical transactions for other users.
* Something in your system has gone wrong internally, and as a result
you can’t serve all of your regular traffic and need to drop
low-priority requests.

At Stripe, we’ve found that carefully implementing a few
https://en.wikipedia.org/wiki/Rate_limiting[rate limiting] strategies
helps keep the API available for everyone. In this post, we’ll explain
in detail which rate limiting strategies we find the most useful, how we
prioritize some API requests over others, and how we started using rate
limiters safely without affecting our existing users’ workflows.

=== Rate limiters and load shedders

A _rate limiter_ is used to control the rate of traffic sent or received
on the network. When should you use a rate limiter? If your users can
afford to change the pace at which they hit your API endpoints without
affecting the outcome of their requests, then a rate limiter is
appropriate. If spacing out their requests is not an option (typically
for real-time events), then you’ll need another strategy outside the
scope of this post (most of the time you just need more infrastructure
capacity).

Our users can make a lot of requests: for example, batch processing
payments causes sustained traffic on our API. We find that clients can
always (barring some extremely rare cases) spread out their requests a
bit more and not be affected by our rate limits.

Rate limiters are amazing for day-to-day operations, but during
incidents (for example, if a service is operating more slowly than
usual), we sometimes need to drop low-priority requests to make sure
that more critical requests get through. This is called _load shedding_.
It happens infrequently, but it is an important part of keeping Stripe
available.

A _load shedder_ makes its decisions based on the whole state of the
system rather than the user who is making the request. Load shedders
help you deal with emergencies, since they keep the core part of your
business working while the rest is on fire.

=== Using different kinds of rate limiters in concert

Once you know rate limiters can improve the reliability of your API, you
should decide which types are the most relevant.

At Stripe, we operate 4 different types of limiters in production. The
first one, the _Request Rate Limiter_, is by far the most important one.
We recommend you start here if you want to improve the robustness of
your API.

==== Request rate limiter

This rate limiter restricts each user to _N_ requests per second.
Request rate limiters are the first tool most APIs can use to
effectively manage a high volume of traffic.

Our rate limits for requests is constantly triggered. It has rejected
millions of requests this month alone, especially for test mode requests
where a user inadvertently runs a script that’s gotten out of hand.

Our API provides the same rate limiting behavior in both test and live
modes. This makes for a good developer experience: scripts won't
encounter side effects due to a particular rate limit when moving from
development to production.

After analyzing our traffic patterns, we added the ability to briefly
burst above the cap for sudden spikes in usage during real-time events
(e.g. a flash sale.)

image:%0A%20%20%20%20%20%20%20%20%0A%20%20%20%20%20%20%20%20%20%20https://images.stripeassets.com/fzn2n1nzq965/70aRVdIQ9Hhbr1wV9NPPDn/c9873fea1fb540a1b5ed9d9eb594549e/image.png?w=1620&q=80%0A%20%20%20%20%20%20%20%20%0A%20%20%20%20%20%20[Blog
++>++ Rate Limiters ++>++ Graph 1,width=523,height=268]

==== Concurrent requests limiter

Instead of “You can use our API 1000 times a second”, this rate limiter
says “You can only have 20 API requests in progress at the same time”.
Some endpoints are much more resource-intensive than others, and users
often get frustrated waiting for the endpoint to return and then retry.
These retries add more demand to the already overloaded resource,
slowing things down even more. The concurrent rate limiter helps address
this nicely.

Our concurrent request limiter is triggered much less often (12,000
requests this month), and helps us keep control of our CPU-intensive API
endpoints. Before we started using a concurrent requests limiter, we
regularly dealt with resource contention on our most expensive endpoints
caused by users making too many requests at one time. The concurrent
request limiter totally solved this.

It is completely reasonable to tune this limiter up so it rejects more
often than the Request Rate Limiter. It asks your users to use a
different programming model of “Fork off X jobs and have them process
the queue” compared to “Hammer the API and back off when I get a HTTP
429”. Some APIs fit better into one of those two patterns so feel free
to use which one is most suitable for the users of your API.

image:%0A%20%20%20%20%20%20%20%20%0A%20%20%20%20%20%20%20%20%20%20https://images.stripeassets.com/fzn2n1nzq965/6n0cFumdlNxMYdkfdz0JRu/24c589e3516573fb72068d8d35a7f3ef/image.png?w=1620&q=80%0A%20%20%20%20%20%20%20%20%0A%20%20%20%20%20%20[Blog
++>++ Rate limiters ++>++ Graph 2,width=529,height=267]

==== Fleet usage load shedder

Using this type of load shedder ensures that a certain percentage of
your fleet will always be available for your most important API
requests.

We divide up our traffic into two types: critical API methods (e.g.
creating charges) and non-critical methods (e.g. listing charges.) We
have a Redis cluster that counts how many requests we currently have of
each type.

We always reserve a fraction of our infrastructure for critical
requests. If our reservation number is 20%, then any non-critical
request over their 80% allocation would be rejected with status code
503.

We triggered this load shedder for a very small fraction of requests
this month. By itself, this isn’t a big deal&mdash;we definitely had the
ability to handle those extra requests. But we’ve had other months where
this has prevented outages.

image:%0A%20%20%20%20%20%20%20%20%0A%20%20%20%20%20%20%20%20%20%20https://images.stripeassets.com/fzn2n1nzq965/70nvphdGeNMKHRsoJIsFFq/889af231f8ec95f95b640baaa629b455/image.png?w=1620&q=80%0A%20%20%20%20%20%20%20%20%0A%20%20%20%20%20%20[Blog
++>++ Rate limiters ++>++ Graph 3,width=536,height=268]

==== Worker utilization load shedder

Most API services use a set of workers to independently respond to
incoming requests in a parallel fashion. This load shedder is the final
line of defense. If your workers start getting backed up with requests,
then this will shed lower-priority traffic.

This one gets triggered very rarely, only during major incidents.

We divide our traffic into 4 categories:

* Critical methods
* POSTs
* GETs
* Test mode traffic

We track the number of workers with available capacity at all times. If
a box is too busy to handle its request volume, it will slowly start
shedding less-critical requests, starting with test mode traffic. If
shedding test mode traffic gets it back into a good state, great! We can
start to slowly bring traffic back. Otherwise, it’ll escalate and start
shedding even more traffic.

It’s very important that shedding and bringing load happen slowly, or
you can end up flapping (“I got rid of testmode traffic! Everything is
fine! I brought it back! Everything is awful!”). We used a lot of trial
and error to tune the rate at which we shed traffic, and settled on a
rate where we shed a substantial amount of traffic within a few minutes.

Only 100 requests were rejected this month from this rate limiter, but
in the past it’s done a lot to help us recover more quickly when we have
had load problems. This load shedder limits the impact of incidents that
are already happening and provides damage control, while the first three
are more preventative.

image:%0A%20%20%20%20%20%20%20%20%0A%20%20%20%20%20%20%20%20%20%20https://images.stripeassets.com/fzn2n1nzq965/5dU2JWBEVCh1kXHlKWakqw/3413cd572b53d2af0ae432dcc90e9585/image.png?w=1620&q=80%0A%20%20%20%20%20%20%20%20%0A%20%20%20%20%20%20[Blog
++>++ Rate limiters ++>++ Graph 4,width=515,height=268]

=== Building rate limiters in practice

Now that we’ve outlined the four basic kinds of rate limiters we use and
what they’re for, let’s talk about their implementation. What rate
limiting algorithms are there? How do you actually implement them in
practice?

We use the https://en.wikipedia.org/wiki/Token_bucket[token bucket
algorithm] to do rate limiting. This algorithm has a centralized bucket
host where you take tokens on each request, and slowly drip more tokens
into the bucket. If the bucket is empty, reject the request. In our
case, every Stripe user has a bucket, and every time they make a request
we remove a token from that bucket.

We implement our rate limiters using Redis. You can either operate the
Redis instance yourself, or, if you use Amazon Web Services, you can use
a managed service like https://aws.amazon.com/elasticache/[ElastiCache].

Here are important things to consider when implementing rate limiters:

* *Hook the rate limiters into your middleware stack safely.* Make sure
that if there were bugs in the rate limiting code (or if Redis were to
go down), requests wouldn’t be affected. This means catching exceptions
at all levels so that any coding or operational errors would fail open
and the API would still stay functional.
* *Show clear exceptions to your users.* Figure out what kinds of
exceptions to show your users. In practice, you should decide if you
want https://tools.ietf.org/html/rfc6585#section-4[HTTP 429] (Too Many
Requests) or https://tools.ietf.org/html/rfc7231#section-6.6.4[HTTP 503]
(Service Unavailable) and what is the most accurate depending on the
situation. The message you return should also be actionable.
* *Build in safeguards so that you can turn off the limiters.* Make sure
you have kill switches to disable the rate limiters should they kick in
erroneously. Having feature flags in place can really help should you
need a human escape valve. Set up alerts and metrics to understand how
often they are triggering.
* *Dark launch each rate limiter to watch the traffic they would block.*
Evaluate if it is the correct decision to block that traffic and tune
accordingly. You want to find the right thresholds that would keep your
API up without affecting any of your users’ existing request patterns.
This might involve working with some of them to change their code so
that the new rate limit would work for them.

=== Conclusion

Rate limiting is one of the most powerful ways to prepare your API for
scale. The different rate limiting strategies described in this post are
not all necessary on day one, you can gradually introduce them once you
realize the need for rate limiting.

Our recommendation is to follow the following steps to introduce rate
limiting to your infrastructure:

* Start by building a Request Rate Limiter. It is the most important one
to prevent abuse, and it’s by far the one that we use the most
frequently.
* Introduce the next three types of rate limiters over time to prevent
different classes of problems. They can be built slowly as you scale.
* Follow good launch practices as you're adding new rate limiters to
your infrastructure. Handle any errors safely, put them behind feature
flags to turn them off easily at any time, and rely on very good
observability and metrics to see how often they’re triggering.

To help you get started, we’ve created a
https://gist.github.com/ptarjan/e38f45f2dfe601419ca3af937fff574d[GitHub
gist] to share implementation details based on the code we actually use
in production at Stripe.