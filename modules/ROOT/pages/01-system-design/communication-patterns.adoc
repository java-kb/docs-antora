= Communication Patterns
:figures: 01-system-design

A summary of how you can combine patterns and tools. Keep in mind
that this is just a recommendation. you might have your own
preferences to implement these patterns using different tooling.

|===
| Pattern | Type | Implementation

| Request/response
| Synchronous
| REST API

| Commands that require blocking
| Synchronous
| REST API

| Commands that don't require blocking
| Asynchronous
| Message broker

| Events
| Asynchronous
| Message broker
|===

It's worth noting that, even though the end-to-end communication can be
asynchronous, you'll get a synchronous interface with the message broker from your
applications. That's an important characteristic. When you publish a message, you
want to be sure the broker received it before continuing with something else. The same
applies to subscribers, where the broker requires acknowledgment after consuming
messages to mark them as processed and move to the next ones. These two steps
are critical to keep your data safe and make your system reliable.

== Synchronous communication

== Asynchronous communication

=== REST API calls with retry pattern
Options to model the exchange between two applications

* Create a shared library—One option is to create a shared library with the classes used
by both applications, and import it as a dependency into both projects. As per the
15-factor methodology, such a library would be tracked in its own codebase. Doing
so would ensure that the model used by both applications is consistent and never
out of sync. However, it would mean adding implementation coupling.
* Duplicate the class—The other option is to replicate the class into the upstream
application. By doing so, you wouldn’t have implementation coupling, but
you would have to take care of evolving the replicated model as the original
one changes in the downstream application. There are a few techniques like
consumer-driven contracts that can identify, through automated tests, when the
called API changes. Besides checking the data model, those tests would also ver-
ify other aspects of the exposed API, like HTTP methods, response statuses,
headers, variables, and so on, check the Spring Cloud Contract project if you’re interested (https://
spring.io/projects/spring-cloud-contract).

Both are viable options. Which strategy you adopt is up to your project require-
ments and your organization’s structure. 

=== Publish-Subscribe Pattern
The Publish-Subscribe pattern is a messaging pattern that allows decoupling of the components in a system. In this pattern, publishers send messages to a message broker, which then distributes those messages to subscribers that have expressed interest in receiving them.
This pattern is particularly useful in event-driven architectures, where components can react to events without being tightly coupled to the source of those events. It allows for greater flexibility and scalability, as new subscribers can be added or removed without affecting the publishers or other subscribers.
The Publish-Subscribe pattern is often implemented using message brokers or message queues, which handle the routing and delivery of messages between publishers and subscribers.

=== Event-driven Architecture

event-driven architecture is an approach to software architecture that focuses on responding to events that either occur within a system or in the external environment.

The event-driven architecture uses the Publish-Subscribe pattern and involves publishers sending events to subscribers without knowledge of their specific destinations. As a result, subscribers do not need to understand the publishers`' logic.
In event-driven architectures, an event indicates that something happened in the system. Events get published to a messaging channel (e.g., a message broker) by the business logic that owns the domain where those events happened. Other components in the architecture that are interested in a given event type subscribe to the channel to consume all the subsequent event instances. events relate to the publish-
subscribe pattern, so they are linked to message brokers or buses too

Event-driven architectures describe distributed systems that interact by producing
and consuming events. The interaction is asynchronous, solving the problem of temporal coupling. 

*Benefits*

When you use events with a message broker, you can better isolate all the components in your software architecture. Publishers and subscribers can function independently without needing to be aware of each other. This approach is ideal for microservice architecture since you aim to keep microservices as independent as possible. By following this strategy, you can introduce new microservices that consume
events from the channels without having to modify the microservice that publishes those events, or any other subscribers.

*Challenges*

One such challenge of the event-driven architecture is ensuring that events are handled in the correct order.
Additionally, ensuring that events are not lost or duplicated can be another challenge. Without proper planning,
designing the system to handle a large number of events can be difficult to manage. Event-driven architecture can be
more complex to design and implement than traditional architectures, as they might require additional resources and expertise.
Avoid including business logic in the communication channel as much as possible. Keep that logic in your distributed system, respecting a domain-driven design approach.

. Dropped messages
+
Configure the broker to fulfill the at-least-once guarantee. This policy ensures the messages are delivered at least once by the broker, although they could be duplicated.

. Duplicated messages
+
Make the event consumption idempotent
marking the events that you already processed(e.g., in the database) and ignoring any repeated ones.
configure the broker to fulfill the at-most-once guarantee, and that helps prevent duplicates.

. Unordered messages
+
You have to prepare your code to cope with that

. Broker's downtime
+
Both publishers and subscribers should try to deal with that situation (e.g., with a retry strategy or a cache).
You could also flag the services as unhealthy and stop accepting new operations

== Reactive Systems

When you have applications extensively relying on I/O operations such as database calls or inter-
actions with other services like HTTP request/response communications, the thread-
per-request model begins to expose its technical limits.

In the thread-per-request model, each request is bound to a thread exclusively allo-
cated to its processing. If database or service calls are part of the processing, the
thread will send out a request and then block, waiting for a response. During idle
time, the resources allocated for that thread are wasted, since they cannot be used for
anything else. The reactive programming paradigm solves this problem and improves
scalability, resilience, and cost-effectiveness for all I/O-bound applications.

Reactive applications operate asynchronously and in a non-blocking way, meaning
that computational resources are used more effectively. That’s a huge advantage in
the cloud, since you pay for what you use. When a thread sends a call to a backing ser-
vice, it will not wait idle, but it will move on to executing other operations. This elimi-
nates the linear dependency between the number of threads and the number of
concurrent requests, leading to more scalable applications. With the same amount of
computational resources, reactive applications can serve more users than their non-
reactive counterparts.
A reactive system is a set of design principles to apply in software architecture to make the system responsive (responds on time), resilient (stays responsive if there are failures), elastic (adapts to be responsive under different workloads), and message-driven (ensures loose coupling and boundary isolation).

The Reactive Manifesto (www.reactivemanifesto.org) describes a reactive system as
responsive, resilient, elastic, and message-driven. Its mission to build loosely coupled,
scalable, resilient, and cost-effective applications is fully compatible with our defini-
tion of cloud native.

non-reactive applications allocate a thread per request. Until
a response is returned, the thread will not be used for anything. That is the thread-per-
request model. When the request handling involves intensive operations like I/O, the
thread will block until those operations are completed. For example, if a database
read is required, the thread will wait until data is returned from the database. During
the waiting time, the resources allocated to the handling thread are not used effi-
ciently. If you want to support more concurrent users, you’ll have to ensure you have
enough threads and resources available. In the end, this paradigm sets constraints on
the application’s scalability and doesn’t use computational resources in the most effi-
cient way possible.

image::{figures}/The-thread-per-request-model.png[In the thread-per-request model, each request is handled by a thread dedicated exclusively to its handling.]

Reactive applications are more scalable and efficient by design. Handling requests
in a reactive application doesn’t involve allocating a given thread exclusively—requests
are fulfilled asynchronously based on events. For example, if a database read is required,
the thread handling that part of the flow will not wait until data is returned from the
database. Instead, a callback is registered, and whenever the information is ready, a
notification is sent, and one of the available threads will execute the callback. During that time, the thread that requested the data can be used to process other requests
rather than waiting idle.

This paradigm, called event loop, doesn’t set hard constraints on the application’s
scalability. It actually makes it easier to scale, since an increase in the number of con-
current requests does not strictly depend on the number of threads. 

image::{figures}/The-event-loop-model.png[In the event loop model, requests are handled by threads that don’t block while waiting for an  intensive operation, allowing them to process other requests in the meantime.]

Scale and cost optimization are two critical reasons for moving to the cloud, so the
reactive paradigm perfectly fits cloud native applications. Scaling applications to sup-
port a workload increase becomes less demanding. By using resources more efficiently,
you can save money on the computational resources offered by a cloud provider.
Another reason for moving to the cloud is resilience, and reactive applications also
help with that.

One of the essential features of reactive applications is that they provide non-
blocking backpressure (also called control flow). This means that consumers can con-
trol the amount of data they receive, which lowers the risk of producers sending more
data than consumers can handle, which can cause a DoS attack, slowing the applica-
tion, cascading the failure, or even leading to a total crash.

The reactive paradigm is a solution to the problem of blocking I/O operations that
require more threads to handle high concurrency and which may lead to slow or entirely
unresponsive applications. Sometimes the paradigm is mistaken as a way to increase the
speed of an application. Reactive is about improving scalability and resilience, not speed.

Going reactive is an excellent
choice when you expect high traffic and concurrency with fewer computational
resources or in streaming scenarios. However, you should also be aware of the addi-
tional complexity introduced by such a paradigm. Besides requiring a mindset shift to
think in an event-driven way, reactive applications are more challenging to debug and
troubleshoot because of the asynchronous I/O. Before rushing to rewrite all your
applications to make them reactive, think twice about whether that’s necessary, and
consider both the benefits and drawbacks.

When you expect high traffic and concurrency with fewer computational
resources, the reactive paradigm can improve the application’s scalability, resil-
ience, and cost-effectiveness at the expense of a steeper initial learning curve.
