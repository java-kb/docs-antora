= Serverless applications

Serverless is a further abstraction layer on top of virtual
machines and containers, moving even more responsibilities from product teams to
the platform. Following the serverless computing model, developers focus on implementing the business logic for their applications. Using an orchestrator like Kubernetes still requires infrastructure provisioning, capacity planning, and scaling. In
contrast, a serverless platform takes care of setting up the underlying infrastructure
needed by the applications to run, including virtual machines, containers, and dynamic
scaling.

Serverless applications typically only run when there is an event to handle, such as
an HTTP request (request-driven) or a message (event-driven). The event can be external or produced by another function. For example, whenever a message is added to a
queue, a function might be triggered, process the message, and then exit the execution. When there is nothing to process, the platform shuts down all the resources
involved with the function, so you can really pay for your actual usage.

In the other cloud native topologies like CaaS or PaaS, there is always a server
involved running 24/7. Compared to traditional systems, you get the advantage of
dynamic scalability, reducing the number of resources provisioned at any given time.
Still, there is always something up and running that has a cost. In the serverless model,
however, resources are provisioned only when necessary. If there is nothing to process,
everything is shut down. That’s what we call scaling to zero, and it’s one of the main features offered by serverless platforms.

A consequence of scaling applications to zero is that when eventually there’s a
request to handle, a new application instance is started, and it must be ready to pro-
cess the request very quickly. Standard JVM applications are not suitable for serverless
applications, since it’s hard to achieve a startup time lower than a few seconds. That’s
why GraalVM native images became popular. Their instant startup time and reduced
memory consumption make them perfect for the serverless model. The instant startup
time is required for scaling. The reduced memory consumption helps reduce costs, which is
one of the goals of serverless and cloud native in general.

Besides cost optimization, serverless technologies also move some extra responsi-
bility from the application to the platform. That might be an advantage, since it allows
developers to focus exclusively on the business logic. But it’s also essential to consider
what degree of control you would like to have and how you will deal with vendor lock-in.

Each serverless platform has its own features and APIs. Once you start writing func-
tions for a specific platform, you can’t move them easily to another, as you would do
with containers. You might compromise to gain responsibility and scope and lose on
control and portability more than with any other approach. That’s why Knative became
popular quickly: it’s built on Kubernetes, which means that you can easily move your
serverless workloads between platforms and vendors.

When working locally, it’s convenient to run and test serverless applications on the
JVM rather than using GraalVM due to the shorter build time and the less resource-demanding process. However, to achieve better quality and catch errors earlier, we
should run and verify the applications in native mode as early in the delivery process
as possible. The commit stage is where we compile and test our applications, so it
might be a good place to add those additional steps.