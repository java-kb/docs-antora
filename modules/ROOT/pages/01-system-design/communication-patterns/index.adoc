= Communication Patterns
:figures: 01-system-design/communication-patterns

A summary of how you can combine patterns and tools. Keep in mind
that this is just a recommendation. you might have your own
preferences to implement these patterns using different tooling.

|===
| Pattern | Type | Implementation

| Request/response
| Synchronous
| REST API

| Commands that require blocking
| Synchronous
| REST API

| Commands that don't require blocking
| Asynchronous
| Message broker

| Events
| Asynchronous
| Message broker
|===

It's worth noting that, even though the end-to-end communication can be
asynchronous, you'll get a synchronous interface with the message broker from your
applications. That's an important characteristic. When you publish a message, you
want to be sure the broker received it before continuing with something else. The same
applies to subscribers, where the broker requires acknowledgment after consuming
messages to mark them as processed and move to the next ones. These two steps
are critical to keep your data safe and make your system reliable.

== Synchronous communication

== Asynchronous communication

=== REST API calls with retry pattern
Options to model the exchange between two applications

* Create a shared library—One option is to create a shared library with the classes used
by both applications, and import it as a dependency into both projects. As per the
15-factor methodology, such a library would be tracked in its own codebase. Doing
so would ensure that the model used by both applications is consistent and never
out of sync. However, it would mean adding implementation coupling.
* Duplicate the class—The other option is to replicate the class into the upstream
application. By doing so, you wouldn’t have implementation coupling, but
you would have to take care of evolving the replicated model as the original
one changes in the downstream application. There are a few techniques like
consumer-driven contracts that can identify, through automated tests, when the
called API changes. Besides checking the data model, those tests would also ver-
ify other aspects of the exposed API, like HTTP methods, response statuses,
headers, variables, and so on, check the Spring Cloud Contract project if you’re interested (https://
spring.io/projects/spring-cloud-contract).

Both are viable options. Which strategy you adopt is up to your project require-
ments and your organization’s structure. 

=== Event-driven Architecture

== Reactive Systems

When you have applications extensively relying on I/O operations such as database calls or inter-
actions with other services like HTTP request/response communications, the thread-
per-request model begins to expose its technical limits.

In the thread-per-request model, each request is bound to a thread exclusively allo-
cated to its processing. If database or service calls are part of the processing, the
thread will send out a request and then block, waiting for a response. During idle
time, the resources allocated for that thread are wasted, since they cannot be used for
anything else. The reactive programming paradigm solves this problem and improves
scalability, resilience, and cost-effectiveness for all I/O-bound applications.

Reactive applications operate asynchronously and in a non-blocking way, meaning
that computational resources are used more effectively. That’s a huge advantage in
the cloud, since you pay for what you use. When a thread sends a call to a backing ser-
vice, it will not wait idle, but it will move on to executing other operations. This elimi-
nates the linear dependency between the number of threads and the number of
concurrent requests, leading to more scalable applications. With the same amount of
computational resources, reactive applications can serve more users than their non-
reactive counterparts.
A reactive system is a set of design principles to apply in software architecture to make the system responsive (responds on time), resilient (stays responsive if there are failures), elastic (adapts to be responsive under different workloads), and message-driven (ensures loose coupling and boundary isolation).

The Reactive Manifesto (www.reactivemanifesto.org) describes a reactive system as
responsive, resilient, elastic, and message-driven. Its mission to build loosely coupled,
scalable, resilient, and cost-effective applications is fully compatible with our defini-
tion of cloud native.

non-reactive applications allocate a thread per request. Until
a response is returned, the thread will not be used for anything. That is the thread-per-
request model. When the request handling involves intensive operations like I/O, the
thread will block until those operations are completed. For example, if a database
read is required, the thread will wait until data is returned from the database. During
the waiting time, the resources allocated to the handling thread are not used effi-
ciently. If you want to support more concurrent users, you’ll have to ensure you have
enough threads and resources available. In the end, this paradigm sets constraints on
the application’s scalability and doesn’t use computational resources in the most effi-
cient way possible.

image::{figures}/The-thread-per-request-model.png[In the thread-per-request model, each request is handled by a thread dedicated exclusively to its handling.]

Reactive applications are more scalable and efficient by design. Handling requests
in a reactive application doesn’t involve allocating a given thread exclusively—requests
are fulfilled asynchronously based on events. For example, if a database read is required,
the thread handling that part of the flow will not wait until data is returned from the
database. Instead, a callback is registered, and whenever the information is ready, a
notification is sent, and one of the available threads will execute the callback. During that time, the thread that requested the data can be used to process other requests
rather than waiting idle.

This paradigm, called event loop, doesn’t set hard constraints on the application’s
scalability. It actually makes it easier to scale, since an increase in the number of con-
current requests does not strictly depend on the number of threads. 

image::{figures}/The-event-loop-model.png[In the event loop model, requests are handled by threads that don’t block while waiting for an  intensive operation, allowing them to process other requests in the meantime.]

Scale and cost optimization are two critical reasons for moving to the cloud, so the
reactive paradigm perfectly fits cloud native applications. Scaling applications to sup-
port a workload increase becomes less demanding. By using resources more efficiently,
you can save money on the computational resources offered by a cloud provider.
Another reason for moving to the cloud is resilience, and reactive applications also
help with that.

One of the essential features of reactive applications is that they provide non-
blocking backpressure (also called control flow). This means that consumers can con-
trol the amount of data they receive, which lowers the risk of producers sending more
data than consumers can handle, which can cause a DoS attack, slowing the applica-
tion, cascading the failure, or even leading to a total crash.

The reactive paradigm is a solution to the problem of blocking I/O operations that
require more threads to handle high concurrency and which may lead to slow or entirely
unresponsive applications. Sometimes the paradigm is mistaken as a way to increase the
speed of an application. Reactive is about improving scalability and resilience, not speed.

Going reactive is an excellent
choice when you expect high traffic and concurrency with fewer computational
resources or in streaming scenarios. However, you should also be aware of the addi-
tional complexity introduced by such a paradigm. Besides requiring a mindset shift to
think in an event-driven way, reactive applications are more challenging to debug and
troubleshoot because of the asynchronous I/O. Before rushing to rewrite all your
applications to make them reactive, think twice about whether that’s necessary, and
consider both the benefits and drawbacks.

When you expect high traffic and concurrency with fewer computational
resources, the reactive paradigm can improve the application’s scalability, resil-
ience, and cost-effectiveness at the expense of a steeper initial learning curve.

When developing reactive microservices, it is not always obvious when to use non-blocking synchro-
nous APIs and when to use event-driven asynchronous services. In general, to make a microservice 
robust and scalable, it is important to make it as autonomous as possible, for example, by minimizing 
its runtime dependencies. This is also known as loose coupling. Therefore, the asynchronous mes-
sage passing of events is preferable over synchronous APIs. This is because the microservice will only 
depend on access to the messaging system at runtime, instead of being dependent on synchronous 
access to a number of other microservices.
There are, however, a number of cases where synchronous APIs could be favorable. For example:

* For read operations where an end user is waiting for a response
* Where the client platforms are more suitable for consuming synchronous APIs, for example, 
mobile apps or SPA web applications
* Where the clients will connect to the service from other organizations – where it might be 
hard to agree on a common messaging system to use across organizations

[tabs]
======
CaveatEmptor::
+

Cities API::
+

Multiplication microservices::
+

Microservices with Spring Boot 3 and Spring Cloud::
+
However, one major concern was identified . Updating (creating or deleting) a composite 
entity—an entity whose parts are stored in a number of microservices—using synchronous APIs can 
lead to inconsistencies, if not all involved microservices are updated successfully. This is, in general, 
not acceptable. This leads us into reactive microservices, where we will look into why and how to build 
reactive microservices, that is, microservices that are scalable and robust.
+
For the system landscape in this book, we will use the following:
+
* The create, read, and delete services exposed by the product composite microservice will be 
based on non-blocking synchronous APIs. The composite microservice is assumed to have 
clients on both web and mobile platforms, as well as clients coming from other organizations 
rather than the ones that operate the system landscape. Therefore, synchronous APIs seem 
like a natural match.
* The read services provided by the core microservices will also be developed as non-blocking 
synchronous APIs since there is an end user waiting for their responses.
* The create and delete services provided by the core microservices will be developed as 
event-driven asynchronous services, meaning that they will listen for create and delete events 
on topics dedicated to each microservice.
* The synchronous APIs provided by the composite microservices to create and delete aggregated 
product information will publish create and delete events on these topics. If the publish opera-
tion succeeds, it will return with a 202 (Accepted) response; otherwise, an error response will 
be returned. The 202 response differs from a normal 200 (OK) response – it indicates that the 
request has been accepted, but not fully processed. Instead, the processing will be completed 
asynchronously and independently of the 202 response.
+
images::{figures}/Microservices-with-Spring-Boot-and-Spring-Cloud-communication-pattern.png

Polar Book Shop::
+

======