= Kustomize
:figures: 16-deployment/kubernetes

_TL;DR: Managing several Kubernetes manifests to deploy an application is not very
intuitive. Kustomize provides a convenient way to manage, deploy, configure,
and upgrade an application in Kubernetes._

Kustomize provides generators to build ConfigMaps and Secrets, and a way to trigger a rolling restart whenever they are updated.

When using Kustomize, the first step is composing related manifests together so that we can handle them as a single unit. Kustomize does that via a Kustomization resource. In the end, we want to let
Kustomize manage, process, and generate Kubernetes manifests for us.

== Using Kustomize to manage and configure Spring Boot applications
Open your service project and create a kustomization.yml file inside the k8s folder.

We’ll first instruct Kustomize about which Kubernetes manifests it should use as a foundation for future customizations. For now, we’ll use the existing Deployment and Service manifests.

We could have include a configmap.yml file, but Kustomize offers a better way. Instead of referencing a ConfigMap directly, we can
provide a property file and let Kustomize use it to generate a ConfigMap.

[source,yml,attributes]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - deployment.yml
  - service.yml
# The section containing information to generate ConfigMaps
configMapGenerator:
  - name: catalog-config
    # Uses a property file as the source for a ConfigMap
    files:
      - application.yml
    options:
      # Defines the labels to assign to the generated ConfigMap
      labels:
        app: catalog-service
----
``application.yml``
[source,yml,attributes]
----
polar:
  greeting: Welcome to the book catalog from a development Kubernetes environment!
spring:
  datasource:
    url: jdbc:postgresql://polar-postgres/polardb_catalog
  security:
    oauth2:
      resourceserver:
        jwt:
          issuer-uri: http://polar-keycloak/realms/PolarBookshop
----

In a similar way, Kustomize can also generate Secrets starting with literal values or files.

When applying a Kustomization, we use the -k flag:

kubectl apply -k k8s

ConfigMaps and Secrets generated by Kustomize are named with a unique suffix (a hash) when they’re deployed. You can verify the actual name assigned to the catalog-
config ConfigMap with the following command:

kubectl get cm -l app=catalog-service

== rolling restart
Every time you update the input to the generators, Kustomize creates a new manifest
with a different hash, which triggers a rolling restart of the containers where the updated
ConfigMaps or Secrets are mounted as volumes. That is a highly convenient way to
achieve an automated configuration refresh without implementing or configuring any
additional components.

To verify that it’s true. First, update the value for the polar.greeting property
in the application.yml file used by Kustomize to generate the ConfigMap.
[source,yml,attributes]
----
polar:
  greeting: Welcome to the book catalog from a development Kubernetes environment! using Kustomize
----

Then apply the Kustomization again (kubectl apply -k k8s).

Kustomize will generate a new ConfigMap with a different suffix hash, triggering a rolling restart of all the Catalog Service instances. 

The fact that the instances are restarted one at a time means that the update happens with zero downtime, which is what we aim for in the cloud.

you could compare this result with what would happen when updating a ConfigMap without Kustomize. Kubernetes would update the volume mounted
to the Catalog Service container, but the application would not be restarted and would still return the old value.

Depending on your requirements, you might need to avoid a rolling restart and have the applications reload their configuration at runtime. In that case, you can disable the hash suffix strategy with the disableNameSuffixHash: true generator option and perhaps rely on something like Spring Cloud Kubernetes Configuration Watcher to notify the applications whenever a ConfigMap or Secret is changed.

== Managing Kubernetes configuration for multiple environments with Kustomize
The Kustomize approach to configuration customization is based on the concepts
of bases and overlays. The k8s folder we created above can be
considered a base: a directory with a kustomization.yml file that combines Kubernetes
manifests and customizations. An overlay is another directory with a kustomization.yml
file. What makes it special is that it defines customizations in relation to one or more
bases and combines them. Starting from the same base, you can specify an overlay for
each deployment environment (such as development, test, staging, and production).

As shown in next figure, each Kustomization includes a kustomization.yml file.
The one acting as the base composes together several Kubernetes resources like
Deployments, Services, and ConfigMaps. Also, it’s not aware of the overlays, so it’s completely independent of them. The overlays use one or more bases as a foundation
and provide additional configuration via patches.
image::{figures}/Customizing-configuration-for-multiple-environments-with-Kustomize.png[Kustomize bases can be used as the foundation for further customizations (overlays) depending on the deployment environment.]

Bases and overlays can be defined either in the same repository or different ones. Similar to application codebases, you can decide whether to keep
your deployment configuration in the same repository as your application or not. the benefits for using a separate repository are:

- It makes it possible to control the deployment of all the system components from a single place.
- It allows focused version-control, auditing, and compliance checks before deploying anything to production.
- It fits the GitOps approach, where delivery and deployment tasks are decoupled.

As an example, next figure shows how the Kustomize manifests could be structured in the case of Catalog Service, having bases and overlays in two separate repositories.
image::{figures}/Customizing-Catalog-Service-for-multiple-environments-with-Kustomize.png[Kustomize bases and overlays can be stored in the same repository or two separate ones. Overlays can be used to customize deployments for different environments.]

Another decision to make is whether to keep the base Kubernetes manifests together with the application source code or move them to the deployment repository. 

One of the benefits of keeping the base Kubernetes manifests together with the application source code (the first approach) is that it makes it
simple to run each application on a local Kubernetes cluster during development,
either directly or using Tilt. Depending on your requirements, you might decide to
use one approach or the other. Both are valid and used in real-world scenarios.

=== Defining a configuration base for multiple environments
``catalog-service/k8s/application.yml``
[source,yml,attributes]
----
polar:
  greeting: Welcome to the book catalog from a development Kubernetes environment!
spring:
  datasource:
    url: jdbc:postgresql://polar-postgres/polardb_catalog
  security:
    oauth2:
      resourceserver:
        jwt:
          issuer-uri: http://polar-keycloak/realms/PolarBookshop
----
``catalog-service/k8s/deployment.yml``
[source,yml,attributes]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: catalog-service
  labels:
    app: catalog-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: catalog-service
  template:
    metadata:
      labels:
        app: catalog-service
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/path: /actuator/prometheus
        prometheus.io/port: "9001"
    spec:
      containers:
        - name: catalog-service
          image: catalog-service
          imagePullPolicy: IfNotPresent
          lifecycle:
            preStop:
              exec:
                command: [ "sh", "-c", "sleep 5" ]
          ports:
            - containerPort: 9001
          env:
            - name: BPL_JVM_THREAD_COUNT
              value: "50"
            - name: SPRING_PROFILES_ACTIVE
              value: testdata
          livenessProbe:
            httpGet:
              path: /actuator/health/liveness
              port: 9001
            initialDelaySeconds: 10
            periodSeconds: 5
          readinessProbe:
            httpGet:
              path: /actuator/health/readiness
              port: 9001
            initialDelaySeconds: 5
            periodSeconds: 15
          volumeMounts:
            - name: catalog-config-volume
              mountPath: /workspace/config
      volumes:
        - name: catalog-config-volume
          configMap:
            name: catalog-config
----
``catalog-service/k8s/service.yml``
[source,yml,attributes]
----
apiVersion: v1
kind: Service
metadata:
  name: catalog-service
  labels:
    app: catalog-service
spec:
  type: ClusterIP
  selector:
    app: catalog-service
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9001
----

``catalog-service/k8s/kustomization.yml``
[source,yml,attributes]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - deployment.yml
  - service.yml
# The section containing information to generate ConfigMaps
configMapGenerator:
  - name: order-config
    # Uses a property file as the source for a ConfigMap
    files:
      - application.yml
    options:
      # Defines the labels to assign to the generated ConfigMap
      labels:
        app: order-service
----

=== Defining a configuration overlay for multiple environments
Any customization (base or overlay) requires a kustomization.yml file.

``project-deployment/kubernetes/applications/catalog-service/staging/kustomization.yml``
[source,yml,attributes]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - github.com/polarbookshop/catalog-service/k8s?ref=main <1>

patchesStrategicMerge: <2>
  - patch-env.yml <3>

configMapGenerator:
  - behavior: merge <4>
    files:
      - application-staging.yml <5>
    name: catalog-config <6>

images:
  - name: catalog-service <7>
    newName: ghcr.io/polarbookshop/catalog-service <8>
    newTag: latest <9>

replicas:
  - name: catalog-service <10>
    count: 2 <11>
----
<1> Uses the manifests in your Catalog Service repo on GitHub as the base for further customizations
<2> Section containing the list of patches to apply to the base manifests according to the strategic merge strategy
<3> The patch for customizing the environment variables passed to the Catalog Service container
<4> Merges this ConfigMap with the one defined in the base Kustomization
<5> The additional property file added to the ConfigMap
<6> The same ConfigMap name used in the base Kustomization
<7> The name of the container as defined in the Deployment manifest
<8> The new image name for the container (with your GitHub username cin lowercase)
<9> The new tag for the container
<10> The name of the Deployment for which to define the number of replicas
<11> The number of replicas
=== Customizing environment variables
The first customization we could apply is an environment variable to activate the
staging Spring profile for Catalog Service. Most customizations can be applied via
patches following a merge strategy. Much like Git merges changes from different
branches, Kustomize produces final Kubernetes manifests with changes coming from
different Kustomization files (one or more bases and an overlay).

A best practice when defining Kustomize patches is to keep them small and
focused. To customize environment variables, create a patch-env.yml file within the
staging overlay for Catalog Service (kubernetes/applications/catalog-service/stag-
ing). We need to specify some contextual information so Kustomize can figure out
where to apply the patch and how to merge the changes. When the patch is for cus-
tomizing a container, Kustomize requires us to specify the kind and name of the
Kubernetes resource (that is, Deployment) and the name of the container. This cus-
tomization option is called a strategic merge patch.

``project-deployment/kubernetes/applications/catalog-service/staging/patch-env.yml``
[source,yml,attributes]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: catalog-service
spec:
  template:
    spec:
      containers:
        - name: catalog-service
          env:
            - name: SPRING_PROFILES_ACTIVE <1>
              value: staging
----
<1> Defines which Spring profiles should be activated

You can use this same approach to customize many aspects of a Deployment, such as
the number of replicas, liveness probe, readiness probe, graceful shutdown timeout,
environment variables, volumes, and more.
=== Customizing ConfigMaps
The base Kustomization for Catalog Service instructs Kustomize to generate a catalog-config ConfigMap starting from an application.yml file. To customize the values
in that ConfigMap, we have two main options: replace the entire ConfigMap or overwrite only the values that should be different in staging. In this second case, we could
generally rely on some advanced Kustomize patching strategy to overwrite specific values in the ConfigMap.

When working with Spring Boot, we can take advantage of the power of Spring
profiles. Instead of updating values in the existing ConfigMap, we can add an application-staging.yml file, which we know takes precedence over application.yml when the
staging profile is active. The final result will be a ConfigMap containing both files.

create an application-staging.yml file within the staging overlay for Catalog Service. We’ll use this property file to define a different values for the service properties.

the above manifests will represent the base for multiple customizations applied for each environment as overlays. 

``project-deployment/kubernetes/applications/catalog-service/staging/application-staging.yml``

[source,yml,attributes]
----
polar:
  greeting: Welcome to the book catalog from a staging Kubernetes environment!
----
Next we can rely on the ConfigMap Generator provided by Kustomize to combine
the application-staging.yml file (defined in the staging overlay) with the application.yml file (defined in the base Kustomization) within the same catalog-config ConfigMap.


=== Customizing image name and version
The base Deployment manifest defined in the Catalog Service repository (catalog-service/k8s/deployment.yml) is configured to use a local container image and doesn’t
specify a version number (which means the latest tag is used). That’s convenient in
the development phase, but it doesn’t work for other deployment environments.

If you followed along, you should have your Catalog Service source code tracked in
a catalog-service repository on GitHub and a ghcr.io/<your_github_username>/
catalog-service:latest container image published to GitHub Container Registry
(as per the Commit Stage workflow). 

Similar to what we’ve done for environment variables, we could use a patch to change
the image that’s used by the Catalog Service Deployment resource. Since it’s a very
common customization and would need to be changed every time we deliver a new
version of our applications, however, Kustomize provides a more convenient way to
declare which image name and version we want to use for each container. Furthermore, we can either update the kustomization.yml file directly or rely on the Kustomize CLI (installed as part of the Kubernetes CLI).

[source,console,attributes]
----
kustomize edit set image \
 catalog-service=ghcr.io/<your_github_username>/catalog-service:latest
----

``project-deployment/kubernetes/applications/catalog-service/staging/kustomization.yml``
[source,yml,attributes]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
images:
  - name: catalog-service <7>
    newName: ghcr.io/polarbookshop/catalog-service <8>
    newTag: latest <9>
----

===  Customizing the number of replicas
Kustomize provides a convenient way to update the number of replicas for a given Pod.

Open the kustomization.yml file in the staging overlay for Catalog Service (kubernetes/applications/catalog-service/staging) and configure two replicas for the application.

``project-deployment/kubernetes/applications/catalog-service/staging/kustomization.yml``
[source,yml,attributes]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

replicas:
  - name: catalog-service <10>
    count: 2 <11>
----

== Defining a configuration overlay for production
create a new “production” folder within kubernetes/applications/catalog-service. We’ll use it to store all customizations related to the production environment. Any base or overlay requires a kustomization.yml file

``project-deployment/kubernetes/applications/catalog-service/production/kustomization.yml``
[source,yml,attributes]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  # The git commit hash (sha) identifying your latest release candidate
  - github.com/polarbookshop/catalog-service/k8s?ref=main <1>
# Section containing the list of patches to apply, according to the strategic merge strategy
patchesStrategicMerge:
  # The patch for customizing the environment variables passed to the Catalog Service container
  - patch-env.yml <2>
  - patch-resources.yml <3>
  - patch-volumes.yml <4>

configMapGenerator:
  # Merges this ConfigMap with the one defined in the base Kustomization
  - behavior: merge
    files:
      # The additional property file added to the ConfigMap
      - application-prod.yml <5>
    name: catalog-config

images:
  # The name of the container as defined in the Deployment manifest
  - name: catalog-service
    # The new image name for the container (with your GitHub username in lowercase)
    newName: ghcr.io/polarbookshop/catalog-service <6>
    # The new tag for the container (with your release candidate’s unique identifier)
    newTag: latest

replicas:
  - count: <7>
    name: catalog-service
----

=== Customizing Environment Variables
The first customization we’ll apply is an environment variable to activate the prod
Spring profile for Catalog Service. Following the same approach as in the previous
chapter, create a patch-env.yml file within the production overlay for Catalog Service
(kubernetes/applications/catalog-service/production).

``project-deployment/kubernetes/applications/catalog-service/production/patch-env.yml``
[source,yml,attributes]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: catalog-service
spec:
  template:
    spec:
      containers:
        - name: catalog-service
          env:
            - name: BPL_JVM_THREAD_COUNT
              value: "100"
            - name: SPRING_PROFILES_ACTIVE
              value: prod
----
Next, we need to instruct Kustomize to apply the patch. In the kustomization.yml file
for the production overlay of Catalog Service, list the patch-env.yml file as follows.

=== CUSTOMIZING SECRETS AND VOLUMES
Create a patch-volumes.yml file within the production overlay for Catalog Service
(kubernetes/applications/catalog-service/production), and configure the patch as
shown in listing 15.8. When Kustomize applies this patch to the base deployment man-
ifests, it will merge the ConfigMap volume defined in the base with the Secret volumes
defined in the patch.
[source,yml,attributes]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: catalog-service
spec:
  template:
    spec:
      containers:
        - name: catalog-service
          volumeMounts:
            # Mounts the volume with the Secret containing the PostgreSQL credentials
            - name: postgres-credentials-volume
              mountPath: /workspace/secrets/postgres
              # Mounts the volume with the Secret containing the Keycloak issuer URL
            - name: keycloak-issuer-resourceserver-secret-volume
              mountPath: /workspace/secrets/keycloak
      volumes:
        # Defines a volume from the Secret containing the PostgreSQL credentials
        - name: postgres-credentials-volume
          secret:
            secretName: polar-postgres-catalog-credentials
        # Defines a volume from the Secret containing the Keycloak issuer URL
        - name: keycloak-issuer-resourceserver-secret-volume
          secret:
            secretName: keycloak-issuer-resourceserver-secret
----
then we need to reference the patch in the kustomization.yml file for the production overlay.

=== Customizing Configmaps
The base Kustomization for Catalog Service instructs Kustomize to generate a
catalog-config ConfigMap starting with an application.yml file. As you learned in
the previous chapter, we can ask Kustomize to add an additional file to that same
ConfigMap, application-prod.yml, which we know takes precedence over the base
application.yml file. 

create an application-prod.yml file within the production overlay for Catalog
Service (kubernetes/applications/catalog-service/production). We’ll use this prop-
erty file to configure a custom greeting. We also need to instruct Spring Boot to load
the Secrets as config trees, using the spring.config.import property.

``project-deployment/kubernetes/applications/catalog-service/production/application-prod.yml``
[source,yml,attributes]
----
polar:
  greeting: Welcome to our book catalog from a production Kubernetes environment!
spring:
  config:
    # Imports configuration from the path where volumes with Secrets are mounted. Make sure you include the final slash, or the import will fail.
    import: configtree:/workspace/secrets/*/
----
Next, we can rely on the ConfigMap Generator provided by Kustomize to combine the
application-prod.yml file (defined in the production overlay) with the application.yml
file (defined in the base Kustomization), within the same catalog-config ConfigMap.

Then update the kustomization.yml file for the production overlay.

=== Customizing Image Name And Version
The next step is updating the image name and version
[source,yml,attributes]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
images:
  # The name of the container as defined in the Deployment manifest
  - name: catalog-service
    # The new image name for the container (with your GitHub username in lowercase)
    newName: ghcr.io/polarbookshop/catalog-service <6>
    # The new tag for the container (with your release candidate’s unique identifier)
    newTag: latest
----
=== Customizing The Number Of Replicas
Cloud native applications are supposed to be highly available, but only one instance of Catalog Service is deployed by default. Similar to what we did for the staging environment.

In a real scenario, you would probably want Kubernetes to dynamically
scale applications in and out depending on the current workload, rather than
providing a fixed number. Dynamic scaling is a pivotal feature of any cloud
platform. In Kubernetes, it’s implemented by a dedicated component called
Horizontal Pod Autoscaler based on well-defined metrics, such as the CPU
consumption per container. For more information, refer to the Kubernetes
documentation (https://kubernetes.io/docs).

[source,yml,attributes]
----
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
replicas:
  - count: <7>
    name: catalog-service
----

=== block requests to the Actuator endpoints
Edge Service is the only application available through the public internet, and it requires
an additional patch to configure the Ingress to block requests to the Actuator endpoints
from outside the cluster. 

A simple way of fixing that is configuring
the Ingress to block any request to the /actuator/** endpoints from outside the
cluster. They will all still be available from within the cluster so that the health probes
can work. We are using an NGINX-based Ingress Controller, so we can use its config-
uration language to express a deny rule for the Actuator endpoints.

create a patch-ingress.yml file within the production overlay for Edge Service

``polar-deployment/kubernetes/applications/edge-service/production/patch-ingress.yml``

[source,yml,attributes]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: polar-ingress
  annotations:
    nginx.ingress.kubernetes.io/server-snippet: |
      location ~* "^/actuator" {
        deny all;
        return 403;
      }
----


[[Configuring-CPU-and-memory]]
=== Configuring CPU and memory
==== Assigning Resource Requests And Limits To A Container
Create a patch-resources.yml file within the production overlay
for Catalog Service (kubernetes/applications/catalog-service/production), and define
both requests and limits for the container resources. 

In a real-world scenario, you might want to analyze more carefully which requests and limits would be appropriate for your use case.

``polar-deployment/kubernetes/applications/catalog-service/production/patch-resources.yml``
[source,language,attributes]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: catalog-service
spec:
  template:
    spec:
      containers:
        - name: catalog-service
          resources:
            # Minimum amount of resources required by the container to operate
            requests:
              # The container is guaranteed 756 MiB.
              memory: 756Mi
              # The container is guaranteed CPU ccycles equivalent to 0.1 CPU.
              cpu: "0.1"
              # Maximum amount of resources the container is allowed to consume
            limits:
              # The container can consume 756 MiB at most.
              memory: 756Mi
              # The container can consume CPU cycles equivalent to 2 CPUs at most.
              cpu: "2"
----
Next, open the kustomization.yml file in the production overlay for Catalog Service, and configure Kustomize to apply the patch.

[[Configuring-Resources-For-The-JVM]]
==== Configuring Resources For The JVM
Open the patch-env.yml file in the production
overlay for Catalog Service (kubernetes/applications/catalog-service/production), and
update the JVM thread count as follows.

``project-deployment/kubernetes/applications/catalog-service/production/patch-env.yml``
[source,yml,attributes]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: catalog-service
spec:
  template:
    spec:
      containers:
        - name: catalog-service
          env:
            - name: BPL_JVM_THREAD_COUNT <1>
              value: "100"
            - name: SPRING_PROFILES_ACTIVE
              value: prod
----