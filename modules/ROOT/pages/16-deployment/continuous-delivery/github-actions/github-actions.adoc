= GitHub Actions
:figures: 16-deployment/continuous-delivery/github-actions

GitHub Actions is a platform built into GitHub that lets you automate software workflows directly from your code repositories.

A workflow is an automated process. Each workflow
listens to specific events that trigger its execution.
For example, a workflow can be triggered by a code commit, a pull request, or a
scheduled time. Workflows can also be triggered manually by a user.
Workflows are defined in YAML files and can be stored in the .github/workflows folder of a GitHub repository. They can be used to build, test, package, release, and deploy applications.
Workflows can also be used to automate tasks such as sending notifications, updating documentation, or managing issues and pull requests.

Each workflow is organized into jobs that run in parallel. Each job is executed on a
runner instance, which is a server provided by GitHub. You can choose between
Ubuntu, Windows, and macOS.

Each job is composed of steps, which are executed sequentially. A step could be either
a shell command or an action. Actions are custom applications used to perform com-
plex tasks in a more structured and reproducible way. For example, you could have
actions for packaging an application into an executable, running tests, creating a container image, or pushing an image to a container registry. The GitHub organiza-
tion provides a basic set of actions, but there's also a marketplace with many more
actions developed by the community.

When using actions from the GitHub marketplace, handle them
like any other third-party application and manage the security risks accord-
ingly. Prefer using trusted actions provided by GitHub or verified organiza-
tions over other third-party options.


== Create Workflows

Workflows should be defined in a .github/workflows folder in a GitHub repository
root, and they should be described following the YAML format provided by GitHub Actions.

== Deployment pipeline(commit-stage)

=== Deployment pipeline(commit-stage): Versioning release candidates for continuous delivery
[source,yml,attributes]
----
    env:
      REGISTRY: ghcr.io 
      IMAGE_NAME: ${{ github.repository_owner }}/${{ matrix.project }} 
      # Publishes a release candidate with a version equal to the Git commit hash
      VERSION: ${{ github.sha }} <1>
  package: 
    name: Package and Publish
    # This job runs only on the main branch
    if: ${{ github.ref == 'refs/heads/main' }} 
    # Runs the job only if the “build” job completes successfully
    needs: [ build ] 
    steps:
      - name: Publish container image >2>
        run: docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
        # Adds the “latest” tag to the newest release candidate
      - name: Publish container image (latest) <3>
        run: |
          docker tag ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }} \
            ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
          docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
----
After updating the workflow, commit your changes and push them to GitHub. That
will trigger the execution of the commit stage workflow. The outcome
will be a container image published to GitHub Container Registry, versioned with the
current Git commit hash and the additional latest tag.

=== Deployment pipeline(commit-stage): static code analysis, compilation, unit tests, and integration tests
create a commit-stage.yml
file under a new .github/workflows folder. This workflow will be triggered whenever
new code is pushed to the repository.

[,yml]
----
name: Commit Stage
# The workflow is triggered when new code is pushed to the repository.
on:
  push:

jobs:
  build:
    name: Build and Test
    runs-on: ubuntu-22.04
    permissions:
      contents: read
      security-events: write
    steps:
        # Checks out the currentGit repository
      - name: Checkout source code
        uses: actions/checkout@v4
        # Installs and configures a Java runtime
      - name: Set up JDK
        uses: actions/setup-java@v4
        # Defines which version, distribution, and cache type to use
        with:
          distribution: temurin
          java-version: 17
          cache: maven
      - name: Build, unit tests and integration tests
        # Ensures the Gradle wrapper is executable, solving Windows incompatibilities
        # Runs the Maven build task, which incompatibilities compiles the codebase and runs unit and integration tests
        run: |
          chmod +x mvnw
          ./mvnw install
      - name: Code vulnerability scanning
        uses: anchore/scan-action@v3
        # Assigns an identifier to the current step so that it can be referenced from subsequent steps
        id: scan
        with:
          # The path to the checked-out repository
          path: "${{ github.workspace }}"
          # Whether to fail the build in the event of security vulnerabilities
          fail-build: false
          #The minimum security category to be considered as an error (low, medium, high, critical)
          severity-cutoff: high
      - name: Upload vulnerability report
        # Uploads the security vulnerability report to itHub (SARIF format)
        uses: github/codeql-action/upload-sarif@v3
        # Uploads the report even if the previous step fails
        if: success() || failure()
        with:
          # Fetches the report from the output of the previous step
          sarif_file: ${{ steps.scan.outputs.sarif }}
      # - name: Setup tools
      #   uses: alexellis/setup-arkade@v3
      # - name: Install tools
      #   uses: alexellis/arkade-get@master
      #   with:
      #     kustomize: latest
      #     kubeconform: latest
      # - name: Validate Kubernetes manifests
      #   run: |
      #     kustomize build k8s | kubeconform --strict -
----

After completing the declaration of the initial commit stage for the deployment pipe-
line, commit your changes and push them to the remote GitHub repository. The
newly created workflow will be immediately triggered. You can see the execution
results on your GitHub repository page on the Actions tab.
image::{figures}/image.png[The commit stage workflow is executed after you push new changes to the remote repository.]
By keeping the result of
the commit stage green, you can be quite sure that you haven't broken anything or
introduced new regressions (assuming that you have proper tests in place).

After scanning the Java project for vulnerabilities, we also included a step to fetch
the security report generated by grype and upload it to GitHub, independently of
whether the build succeeds or not. If any security vulnerability is found, you can see
the results in the Security tab of your GitHub repository page

=== Deployment pipeline(commit-stage): Package and publish

define a few environment variables to store some essential facts you’ll need when building a container image for the application. By using environment variables, you can easily change
which container registry you use or the version for the release artifact. 
[source,yml,attributes]
----
env:
 REGISTRY: ghcr.io  
 IMAGE_NAME: <your_github_username>/catalog-service
 VERSION: latest 
----

add a new “Package and Publish” job to the workflow
[source,yml,attributes]
----
  package: 
    name: Package and Publish
    # This job runs only on the main branch
    if: ${{ github.ref == 'refs/heads/main' }} 
    # Runs the job only if the “build” job completes successfully
    needs: [ build ] 
    runs-on: ubuntu-22.04 
    permissions:
      # Grants the job permissions to read the repository contents and write to the package registry
      # This is necessary for the job to be able to push the Docker image to the registry
      contents: read 
      packages: write 
      # Grants the job permissions to write security events, such as security vulnerability reports
      security-events: write 
    steps:
      - name: Checkout source code
        uses: actions/checkout@v3 
      - name: Set up JDK
        uses: actions/setup-java@v3 
        with:
          distribution: temurin
          java-version: 17
          cache: maven
      - name: Build container image
        run: |
          chmod +x mvnw
          ./mvnw spring-boot:build-image --imageName ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
----
After packaging the application as a container image, update the commit-
stage.yml file to use grype to scan the image for vulnerabilities and publish a report to
GitHub

add a new “Package and Publish” job to the workflow
[source,yml,attributes]
----
      - name: OCI image vulnerability scanning
        uses: anchore/scan-action@v3 
        id: scan
        with: 
          image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
          fail-build: false 
          severity-cutoff: high 
      #Scans the release candidate image for vulnerabilities using grype
      - name: Upload vulnerability report
        uses: github/codeql-action/upload-sarif@v2 
        if: success() || failure()
        with:
          sarif_file: ${{ steps.scan.outputs.sarif }}
----
Finally, we can authenticate with the con-
tainer registry and push the image representing our release candidate.
[source,yml,attributes]
----
      - name: Log into container registry
        uses: docker/login-action@v2 
        with:
          registry: ${{ env.REGISTRY }} 
          username: ${{ github.actor }} 
          password: ${{ secrets.GITHUB_TOKEN }} 
      - name: Publish container image 
        run: docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
----
=== Deployment pipeline(commit-stage): Validate Kubernetes manifests
Since a manifest specifies the desired state of an object, we should ensure that
our specification complies with the API exposed by Kubernetes. It’s a good idea to
automate this validation in the commit stage of a deployment pipeline to get fast
feedback in case of errors (rather than waiting until the acceptance stage, where we
need to use those manifests to deploy the application in a Kubernetes cluster)

Go to your Catalog Service project (catalog-service), and open the commit-stage.yml
file within the .github/workflows folder. 

[source,yml,attributes]
----
- name: Setup tools
  uses: alexellis/setup-arkade@v3
- name: Install tools
  uses: alexellis/arkade-get@master
  with:
    kustomize: latest
    kubeconform: latest
- name: Validate Kubernetes manifests
  run: |
    kustomize build k8s | kubeconform --strict -
----
=== Deployment pipeline(commit-stage): Build native images
When working locally, it’s convenient to run and test serverless applications on the
JVM rather than using GraalVM due to the shorter build time and the less resource-
demanding process. However, to achieve better quality and catch errors earlier, we
should run and verify the applications in native mode as early in the delivery process
as possible. The commit stage is where we compile and test our applications, so it
might be a good place to add those additional steps.

the commit stage execution after adding this steps takes quite a bit
longer than without building native image. As the commit stage is supposed to be fast, possibly under five minutes, to provide developers with fast feedback about their changes and allow them to move on to the next
task, in the spirit of continuous integration. The additional steps using GraalVM that
we have just added might slow down the workflow too much. In that case, you might
consider moving this check to the acceptance stage, where we allow the overall process to take longer.

[source,yml,attributes]
----
name: Commit Stage
on: push

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: polarbookshop/quote-function
  VERSION: ${{ github.sha }}

jobs:
  build:
    name: Build and Test
    runs-on: ubuntu-22.04
    permissions:
      contents: read
      security-events: write
    steps:
      - name: Checkout source code
        uses: actions/checkout@v4
      - name: Set up JDK
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: 17
          cache: gradle
      - name: Build, unit tests and integration tests
        run: |
          chmod +x gradlew
          ./gradlew build
      - name: Code vulnerability scanning
        uses: anchore/scan-action@v3
        id: scan
        with:
          path: "${{ github.workspace }}"
          fail-build: false
          severity-cutoff: high
      - name: Upload vulnerability report
        uses: github/codeql-action/upload-sarif@v3
        if: success() || failure()
        with:
          sarif_file: ${{ steps.scan.outputs.sarif }}
  native: 
    name: Build and Test (Native)
    runs-on: ubuntu-22.04
    permissions:
      contents: read
    steps:
      - name: Checkout source code
        uses: actions/checkout@v4
      - name: Set up GraalVM <1>
        uses: graalvm/setup-graalvm@v1
        with:
          java-version: '17'
          distribution: 'graalvm'
          github-token: ${{ secrets.GITHUB_TOKEN }}
      - name: Build, unit tests and integration tests (native) <2>
        run: |
          chmod +x gradlew
          ./gradlew nativeBuild
  package:
    name: Package and Publish
    if: ${{ github.ref == 'refs/heads/main' }}
    needs: [ build, native ] <3>
    runs-on: ubuntu-22.04
    permissions:
      contents: read
      packages: write
      security-events: write
    steps:
      - name: Checkout source code
        uses: actions/checkout@v4
      - name: Set up JDK
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: 17
          cache: gradle
      - name: Build container image
        run: |
          chmod +x gradlew
          ./gradlew bootBuildImage \
            --imageName ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
      - name: OCI image vulnerability scanning
        uses: anchore/scan-action@v3
        id: scan
        with:
          image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
          fail-build: false
          severity-cutoff: high
      - name: Upload vulnerability report
        uses: github/codeql-action/upload-sarif@v3
        if: success() || failure()
        with:
          sarif_file: ${{ steps.scan.outputs.sarif }}
      - name: Log into container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Publish container image
        run: docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
      - name: Publish container image (latest)
        run: |
          docker tag ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }} \
            ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
          docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
----
<1> Installs and configures GraalVM with Java 17 and the native image component
<2> Compiles the application as a native executable and runs unit and integration tests
<3> The “Package and Publish” job runs only if both of the previous jobs complete successfully.

== Deployment pipeline(acceptance-stage)
The acceptance stage is triggered
whenever a new release candidate is published to the artifact repository. One option
for defining such a trigger is listening for the events published by GitHub whenever
the commit stage workflow has completed a run.

create a new acceptance-stage.yml file within the .github/workflows folder

Following the continuous integration principles, developers commit often during the day and repeatedly trigger the commit stage. Since the
commit stage is much faster than the acceptance stage, we risk creating a bottleneck.
When an acceptance stage run has completed, we are not interested in verifying all he release candidates that have queued up in the meantime. We are only interested
in the newest one, so the others can be discarded. GitHub Actions provides a mechanism for handling this scenario via concurrency controls.

Next, you would define several jobs to run in parallel against a production-like envi-
ronment, accomplishing functional and non-functional acceptance tests. For our
example, we’ll simply print a message, since we haven’t implemented the autotests for
this stage.

[[production-stage]]
== Deployment pipeline(production-stage)
Compared to the previous stages, implementing the production stage of a deployment pipeline can differ a lot depending on several factors. Let’s start by focusing on
the first step of the production stage.

At the end of the acceptance stage, we have a release candidate that’s proven to be
ready for production. After that, we need to update the Kubernetes manifests in our
production overlay with the new release version. When we’re keeping both the application source code and deployment scripts in the same repository, the production
stage could be listening to a specific event published by GitHub whenever the acceptance stage completes successfully, much like how we configured the flow between the
commit and acceptance stages.

If we are keeping the deployment scripts in a separate repository,
which means that whenever the acceptance stage workflow completes its execution
in the application repository, we need to notify the production stage workflow in the deployment repository. GitHub Actions provides the option of implementing this
notification process via a custom event.

Open your Catalog Service project (catalog-service), and go to the acceptance-
stage.yml file within the .github/workflows folder. After all the acceptance tests have
run successfully, we have to define a final step that will send a notification to the polar-
deployment repository and ask it to update the Catalog Service production manifests
with the new release version. That will be the trigger for the production stage.

``.github/workflows/acceptance-stage.yml``
[source,yml,attributes]
----
  deliver:
    name: Deliver release candidate to production
    needs: [ functional, performance, security ]
    runs-on: ubuntu-22.04
    steps:
      - name: Log into container registry
        uses: docker/login-action@v2 
        with:
          registry: ${{ env.REGISTRY }} 
          username: ${{ github.repository_owner }} 
          password: ${{ secrets.GITHUB_TOKEN }} 
      - name: Deliver application to production
        # An action to send an event to another repository and trigger a workflow
        uses: peter-evans/repository-dispatch@v3
        with:
          token: ${{ secrets.DISPATCH_TOKEN }}
          repository: ${{ env.OWNER }}/${{ env.DEPLOY_REPO }}
          # A name to identify the event (this is up to you)
          event-type: app_delivery
          # The payload of the message sent to the other repository. Add any information that the other repository might need to perform its operations.
          client-payload: '{
            "app_image": "${{ env.REGISTRY }}/${{ env.OWNER }}/${{ matrix.project }}",
            "app_name": "${{ matrix.project }}",
            "app_version": "${{ env.VERSION }}"}'
----
With this new step, if no error is found during the execution of the acceptance
tests, a notification is sent to the repository to trigger an update for this service.

By default, GitHub Actions doesn’t allow you to trigger workflows located in other
repositories, even if they both belong to you or your organization. Therefore, we need
to provide the repository-dispatch action with an access token that grants it such
permissions. The token can be a personal access token (PAT), a GitHub tool>

Go to your GitHub account, navigate to Settings > Developer Settings > Personal
Access Token, and choose Generate New Token. Input a meaningful name, and assign
it the workflow scope to give the token permissions to trigger workflows in other
repositories. Finally, generate the token and copy its value. GitHub will
show you the token value only once. Make sure you save it since you’ll need it soon.

image::{figures}/personal-access-token-granting-permissions-to-trigger-workflows-in-other-repositories.png[A personal access token (PAT) granting permissions to trigger workflows in other repositories]

Next, go to your Catalog Service repository on GitHub, navigate to the Settings tab,
and then select Secrets > Actions. On that page, choose New Repository Secret, name it DISPATCH_TOKEN (the same name we used in listing above), and input the value of
the PAT you generated earlier. Using the Secrets feature provided by GitHub, we can
provide the PAT securely to the acceptance stage workflow.

create a production-stage.yml file within a new .github/workflows folder

The production stage is triggered
whenever the acceptance stage from an application repository dispatches an app_delivery event. The event itself contains contextual information about the application name, image, and version for the newest release candidate. Since the application specific information is parameterized, we can use this workflow for all the applications

The first job of the production stage is updating the production Kubernetes manifests with the new release version. This job will consist of three steps:

1. Check out the  source code.
2. Update the production Kustomization with the new version for the given application.
3. Commit the changes to the  repository.

``.github/workflows/production-stage.yml``
[source,yml,attributes]
----
name: Production Stage

on:
  # Executes the workflow only when a new app_delivery event is received, dispatched from another repository
  repository_dispatch: <1>
    types: [app_delivery]

jobs:
  update:
    name: Update application version
    runs-on: ubuntu-22.04
    permissions:
      contents: write
    env:
      # Saves the event payload data as environment variables for convenience
      APP_IMAGE: ${{ github.event.client_payload.app_image }} <2>
      APP_NAME: ${{ github.event.client_payload.app_name }}
      APP_VERSION: ${{ github.event.client_payload.app_version }}
    steps:
        # Checks out the repository
      - name: Checkout source code <3>
        uses: actions/checkout@v4
      - name: Update image version <4>
        # Navigates to the production overlay for the given application
        # Updates the image name and version via Kustomize for the given application
        # Updates the tag used by Kustomize to access the correct base manifests stored in the application repository
        run: |
          cd polar-deployment/kubernetes/applications/${{ env.APP_NAME }}/production
          echo "image: ${{ env.APP_NAME }}=${{ env.APP_IMAGE }}:${{ env.APP_VERSION }}"
          kustomize edit set image ${{ env.APP_NAME }}=${{ env.APP_IMAGE }}:${{ env.APP_VERSION }}
          sed -i 's/ref=[\w+]/${{ env.APP_VERSION }}/' kustomization.yml
      # An action to commit and push the changes applied to the current repository from the previous step
      - name: Commit updated manifests <5>
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
            # Details about the commit operation
          commit_message: "Update ${{ env.APP_NAME }} to version ${{ env.APP_VERSION }}"
          branch: main
----
The new commit to the repository will trigger the deployment
pipeline. First, the commit stage will produce a container image (our release candidate) and publish it to GitHub Container Registry. Then the acceptance stage will fictitiously run further tests on the application and finally send a notification (a custom
app_delivery event) to the repository. The event triggers the production stage, which will update the production Kubernetes manifests for project services and commit the changes to the repository

image::{figures}/Deployment-pipeline-from-code-commit-to-ready-for-production-deployment.png[The commit stage goes from code commit to a release candidate, which goes through the acceptance stage. If it passes all the tests, the production stage updates the deployment manifests.]

