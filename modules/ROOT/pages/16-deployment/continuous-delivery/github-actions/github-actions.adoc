= GitHub Actions
:figures: 16-deployment/continuous-delivery/github-actions

GitHub Actions is a platform built into GitHub that lets you automate software workflows directly from your code repositories.

A workflow is an automated process. Each workflow
listens to specific events that trigger its execution.
For example, a workflow can be triggered by a code commit, a pull request, or a
scheduled time. Workflows can also be triggered manually by a user.
Workflows are defined in YAML files and can be stored in the .github/workflows folder of a GitHub repository. They can be used to build, test, package, release, and deploy applications.
Workflows can also be used to automate tasks such as sending notifications, updating documentation, or managing issues and pull requests.

Each workflow is organized into jobs that run in parallel. Each job is executed on a
runner instance, which is a server provided by GitHub. You can choose between
Ubuntu, Windows, and macOS.

Each job is composed of steps, which are executed sequentially. A step could be either
a shell command or an action. Actions are custom applications used to perform com-
plex tasks in a more structured and reproducible way. For example, you could have
actions for packaging an application into an executable, running tests, creating a container image, or pushing an image to a container registry. The GitHub organiza-
tion provides a basic set of actions, but there's also a marketplace with many more
actions developed by the community.

When using actions from the GitHub marketplace, handle them like any other third-party application and manage the security risks accordingly. Prefer using trusted actions provided by GitHub or verified organizations over other third-party options.

== Setting up CI/CD pipelines for automating build with GitHub Actions
1. create or choose a repository and project
+
Start by selecting a repository in which you wish to set up your CI/CD pipeline. This can be an existing 
project or a new one you’re working on.
2. open GitHub Actions in your project repository
+
Now, go to the GitHub Actions tab in the top navigation bar of your repository. Here, you’ll find a 
variety of CI/CD automation templates and workflows tailored to your project’s technology stack. 
GitHub Actions offers a wide range of predefined workflows and lets you create your own from scratch.
3. define your CI/CD workflow
+
choose the GitHub Actions template dedicated to yout project

[tabs]
======

Calaculator Example(Angular)::
+
1. Step 1 – create or choose a repository and project
+
In our case, it will be this repository on GitHub: https://github.com/books-web/Mastering-Angular-Test-Driven-Development
2. open GitHub Actions in your project repository
+
Now, go to the GitHub Actions tab in the top navigation bar of your repository
3. define your CI/CD workflow
+
Our project is an Angular project, so it runs on Node.js. We’ll therefore choose the GitHub Actions 
template dedicated to Node.js, which we’ll modify as we go along so that it meets our needs. You 
need to search using the node keyword in the GitHub Actions template search bar, filtering by the 
Continuous integration category.
+
choose (Node.js By GitHub Actions). We can now click on the Configure button
+
We can now begin the modifications. First, we’ll change the name of the file at the top to angular-tdd.yml,
+
Next, we can modify the value of name at the beginning of our file. Instead of Node.js CI, we’ll 
call it Angular TDD CI/CD:
+
Next, we can change the –version: [14.x, 16.x, 18.x] array node to node-version: [18.x]:
+
Finally, we’ll delete the last line of our file (i.e., - run: npm test) because we don’t have any tests at the moment
+
Now, we can save the file by clicking on the Commit changes button.
+
This workflow is designed to automate the process of installing Node.js dependencies, caching them 
for faster future builds and building the Angular project. The following is a decomposition of the key 
components of our workflow and their functions:
+
* Job definition: The workflow defines a single job named build. This job runs on the latest 
Ubuntu virtual machine provided by GitHub Actions.
* Working directory: The defaults section sets the working directory for all stages of the 
build job to ./Chapter 9/getting-started-angular-tdd/. This ensures that 
commands are executed in the correct location where your Angular project is located.
* Node.js matrix: The strategy section defines a matrix that runs the job multiple times, each 
time with a different Node.js version. In this example, the matrix includes only one version: 
18.x. You can expand this to include more versions for broader compatibility testing.
* Checkout: The first step (uses: actions/checkout@v3) uses the official GitHub Actions 
checkout action to clone the repository’s code onto the runner.
* Set up Node.js: The second step (uses: actions/setup-node@v3) uses the official 
GitHub Actions setup-node action to install and configure the specified Node.js version 
(18.x) on the runner.
* The cache parameter is set to npm to enable caching of Node.js modules between workflow 
runs, potentially speeding up subsequent executions. cache-dependency-path is set 
to **/package-lock.json to ensure the cache is invalidated if the package-lock.
json file changes (indicating a change in dependencies).
* Install dependencies: The third step (run: npm ci) runs the npm ci command to install 
the project’s dependencies from the package-lock.json file. This ensures a consistent 
dependency state across different environments.
* Build the application: The fourth step (run: npm run build --if-present) 
conditionally runs the npm run build command if it exists in the project’s package.
json file. This allows for flexibility in different project setups, where not all projects might 
have a build script defined.
+
we’ll update our previous workflow for running tests. Logic dictates that tests 
should be run before the build. Here’s the test workflow:
test:
 runs-on: ubuntu-latest
 defaults:
 run:
 working-directory: './Chapter 9/getting-started-angular-tdd/'
 strategy:
 matrix:
 node-version: [18.x]
 # See supported Node.js release schedule at https://nodejs.
org/en/about/releases/
[tabs]
====

angular.json::
+
[source, yml]
----
----

angular.json::
+
[source, yml]
----
        "test": {
          "builder": "@angular/build:karma",
          "options": {
            "tsConfig": "tsconfig.spec.json",
            "assets": [
              {
                "glob": "**/*",
                "input": "public"
              }
            ],
            "styles": [
              "src/styles.css"
            ],
            "karmaConfig": "karma.conf.js"
          },
          "configurations": {
            "ci": {
              "watch": false,
              "progress": false,
              "browsers": "ChromeHeadlessCI"
            }
          }
        }
----

package.json::
+
[source, yml]
----
  "pnpm": {
    "onlyBuiltDependencies": [
      "puppeteer"
    ]
  },
  "scripts": {
    "ng": "ng",
    "start": "ng serve",
    "build": "ng build",
    "watch": "ng build --watch --configuration development",
    "test": "ng test",
    "cypress:open": "cypress open",
    "postinstall": "npx puppeteer browsers install"
  },
----

karma.conf.js::
+
[source, yml]
----
// Karma configuration file, see link for more information
// https://karma-runner.github.io/1.0/config/configuration-file.html

process.env.CHROME_BIN = require("puppeteer").executablePath();

module.exports = function (config) {
  config.set({
    basePath: '',
    frameworks: ['jasmine'],
    plugins: [
      require('karma-jasmine'),
      require('karma-chrome-launcher'),
      require('karma-firefox-launcher'),
      require('karma-jasmine-html-reporter'),
      require('karma-coverage'),
      require("karma-coverage-istanbul-reporter"),
      //require("@angular-devkit/build-angular/plugins/karma"),
    ],
    client: {
      jasmine: {
        // you can add configuration options for Jasmine here
        // the possible options are listed at https://jasmine.github.io/api/edge/Configuration.html
        // for example, you can disable the random execution with `random: false`
        // or set a specific seed with `seed: 4321`
      },
    },
    jasmineHtmlReporter: {
      suppressAll: true // removes the duplicated traces
    },
    coverageReporter: {
      dir: require('path').join(__dirname, './coverage/getting-started-angular-tdd'),
      subdir: '.',
      reporters: [
        { type: 'html' },
        { type: 'text-summary' }
      ]
    },
    coverageIstanbulReporter: {
      dir: require("path").join(__dirname, "../coverage"),
      reports: ["html", "lcovonly"],
      fixWebpackSourcePaths: true,
    },
    reporters: ['progress', 'kjhtml', 'coverage'],
    browsers: ['Firefox'],
    customLaunchers: {
      ChromeHeadlessCI: {
        base: "ChromeHeadless",
        flags: ["--no-sandbox", "--disable-gpu"],
      },
    },
    singleRun: false,
    restartOnFileChange: true,
    check: {
      global: {
        statements: 80,
        branches: 80,
        functions: 80,
        lines: 80
      }
    }
  });
};

----

puppeteer.config.cjs::
+
[source, yml]
----
const {join} = require('path');

/**
 * @type {import("puppeteer").Configuration}
 */
module.exports = {
  cacheDirectory: join(__dirname, '.cache', 'puppeteer'),
};

----

.github/workflows/angular-tdd.yml::
+
[source, yml]
----
# This workflow will do a clean installation of node dependencies, cache/restore them, build the source code and run tests across different versions of node
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-nodejs

name: Angular TDD CI/CD

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  test-and-build:

    runs-on: ubuntu-latest

    strategy:
      matrix:
        node-version: [24.x]
        # See supported Node.js release schedule at https://nodejs.org/en/about/releases/

    steps:
    - uses: actions/checkout@v4
    - name: Install pnpm
      uses: pnpm/action-setup@v4
      with:
        version: 10
    - name: Use Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'pnpm'
    - run: pnpm install
    - run: pnpm run --if-present test --configuration=ci
    - run: pnpm run --if-present build
----
====

Another Example::
+
[source, java]
----
----

======

== Setting up CI/CD pipelines for automating the deployment process with GitHub Actions
CD is the practice of automatically deploying changes to production as soon as they have passed through 
the production pipeline. This includes the automated processes of testing, building, and deploying. 
GitHub Actions supports CD, enabling you to automate these processes efficiently.

To begin with, this will be a test and build stage like the other two. Deployment takes place naturally at the end of development, so the 
same will apply to the workflow. At the end of our current workflow, just after the build, we’ll add the 
following to deploy on a remote server:
[source,yml,attributes]
----
 - name: Upload build files to remote server
 uses: appleboy/scp-action@master
 with:
 host: ${{ secrets.SI_HOST }}
 username: ${{ secrets.SI_USERNAME }}
 password: ${{ secrets.SI_PASSWORD }}
 port: ${{ secrets.SI_PORT }}
 source: "[SORUCE_FOLDER]"
 target: "[DESTINATION_TARGET_ON_YOUR_SERVER]"
----

The provided GitHub Actions workflow snippet is designed to automate the process of uploading 
build files to a remote server. This is a common step in a CI/CD pipeline for deploying applications. 
Here’s a breakdown of the workflow step:

* appleboy/scp-action@master: This specifies that this step uses the scp-action
action from the appleboy GitHub repository. This action is designed to securely copy files 
from your GitHub Actions runner to a remote server using the secure copy protocol (SCP). 
The @master tag indicates that the action should use the code from the master branch of 
the repository.
* host: The address of the remote server where the files will be uploaded. This value is retrieved 
from a GitHub secret named SI_HOST.
* username: The username for authenticating with the remote server. This value is retrieved 
from a GitHub secret named SI_USERNAME.
* password: The password for authenticating with the remote server. This value is retrieved 
from a GitHub secret named SI_PASSWORD.
* port: The port number for connecting to the remote server. This value is retrieved from a 
GitHub secret named SI_PORT.
* source: The path to the files that will be uploaded. In this case, it’s set to upload all files in 
the source build directory.
* target: The destination path on the remote server where the files will be uploaded.

For other cloud-oriented platforms, GitHub Actions makes the task easier by offering deployment 
templates for most of these platforms.
== Create Workflows

Workflows should be defined in a .github/workflows folder in a GitHub repository root, and they should be described following the YAML format provided by GitHub Actions.

== Deployment pipeline(commit-stage)

=== Deployment pipeline(commit-stage): Versioning release candidates for continuous delivery
[source,yml,attributes]
----
    env:
      REGISTRY: ghcr.io 
      IMAGE_NAME: ${{ github.repository_owner }}/${{ matrix.project }} 
      # Publishes a release candidate with a version equal to the Git commit hash
      VERSION: ${{ github.sha }} <1>
  package: 
    name: Package and Publish
    # This job runs only on the main branch
    if: ${{ github.ref == 'refs/heads/main' }} 
    # Runs the job only if the “build” job completes successfully
    needs: [ build ] 
    steps:
      - name: Publish container image >2>
        run: docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
        # Adds the “latest” tag to the newest release candidate
      - name: Publish container image (latest) <3>
        run: |
          docker tag ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }} \
            ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
          docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
----
After updating the workflow, commit your changes and push them to GitHub. That
will trigger the execution of the commit stage workflow. The outcome
will be a container image published to GitHub Container Registry, versioned with the
current Git commit hash and the additional latest tag.

=== Deployment pipeline(commit-stage): static code analysis, compilation, unit tests, and integration tests
create a commit-stage.yml
file under a new .github/workflows folder. This workflow will be triggered whenever
new code is pushed to the repository.

[,yml]
----
name: Commit Stage
# The workflow is triggered when new code is pushed to the repository.
on:
  push:

jobs:
  build:
    name: Build and Test
    runs-on: ubuntu-22.04
    permissions:
      contents: read
      security-events: write
    steps:
        # Checks out the currentGit repository
      - name: Checkout source code
        uses: actions/checkout@v4
        # Installs and configures a Java runtime
      - name: Set up JDK
        uses: actions/setup-java@v4
        # Defines which version, distribution, and cache type to use
        with:
          distribution: temurin
          java-version: 17
          cache: maven
      - name: Build, unit tests and integration tests
        # Ensures the Gradle wrapper is executable, solving Windows incompatibilities
        # Runs the Maven build task, which incompatibilities compiles the codebase and runs unit and integration tests
        run: |
          chmod +x mvnw
          ./mvnw install
      - name: Code vulnerability scanning
        uses: anchore/scan-action@v3
        # Assigns an identifier to the current step so that it can be referenced from subsequent steps
        id: scan
        with:
          # The path to the checked-out repository
          path: "${{ github.workspace }}"
          # Whether to fail the build in the event of security vulnerabilities
          fail-build: false
          #The minimum security category to be considered as an error (low, medium, high, critical)
          severity-cutoff: high
      - name: Upload vulnerability report
        # Uploads the security vulnerability report to itHub (SARIF format)
        uses: github/codeql-action/upload-sarif@v3
        # Uploads the report even if the previous step fails
        if: success() || failure()
        with:
          # Fetches the report from the output of the previous step
          sarif_file: ${{ steps.scan.outputs.sarif }}
      # - name: Setup tools
      #   uses: alexellis/setup-arkade@v3
      # - name: Install tools
      #   uses: alexellis/arkade-get@master
      #   with:
      #     kustomize: latest
      #     kubeconform: latest
      # - name: Validate Kubernetes manifests
      #   run: |
      #     kustomize build k8s | kubeconform --strict -
----

After completing the declaration of the initial commit stage for the deployment pipe-
line, commit your changes and push them to the remote GitHub repository. The
newly created workflow will be immediately triggered. You can see the execution
results on your GitHub repository page on the Actions tab.
image::{figures}/image.png[The commit stage workflow is executed after you push new changes to the remote repository.]
By keeping the result of
the commit stage green, you can be quite sure that you haven't broken anything or
introduced new regressions (assuming that you have proper tests in place).

After scanning the Java project for vulnerabilities, we also included a step to fetch
the security report generated by grype and upload it to GitHub, independently of
whether the build succeeds or not. If any security vulnerability is found, you can see
the results in the Security tab of your GitHub repository page

=== Deployment pipeline(commit-stage): Package and publish

define a few environment variables to store some essential facts you’ll need when building a container image for the application. By using environment variables, you can easily change
which container registry you use or the version for the release artifact. 
[source,yml,attributes]
----
env:
 REGISTRY: ghcr.io  
 IMAGE_NAME: <your_github_username>/catalog-service
 VERSION: latest 
----

add a new “Package and Publish” job to the workflow
[source,yml,attributes]
----
  package: 
    name: Package and Publish
    # This job runs only on the main branch
    if: ${{ github.ref == 'refs/heads/main' }} 
    # Runs the job only if the “build” job completes successfully
    needs: [ build ] 
    runs-on: ubuntu-22.04 
    permissions:
      # Grants the job permissions to read the repository contents and write to the package registry
      # This is necessary for the job to be able to push the Docker image to the registry
      contents: read 
      packages: write 
      # Grants the job permissions to write security events, such as security vulnerability reports
      security-events: write 
    steps:
      - name: Checkout source code
        uses: actions/checkout@v3 
      - name: Set up JDK
        uses: actions/setup-java@v3 
        with:
          distribution: temurin
          java-version: 17
          cache: maven
      - name: Build container image
        run: |
          chmod +x mvnw
          ./mvnw spring-boot:build-image --imageName ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
----
After packaging the application as a container image, update the commit-
stage.yml file to use grype to scan the image for vulnerabilities and publish a report to
GitHub

add a new “Package and Publish” job to the workflow
[source,yml,attributes]
----
      - name: OCI image vulnerability scanning
        uses: anchore/scan-action@v3 
        id: scan
        with: 
          image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
          fail-build: false 
          severity-cutoff: high 
      #Scans the release candidate image for vulnerabilities using grype
      - name: Upload vulnerability report
        uses: github/codeql-action/upload-sarif@v2 
        if: success() || failure()
        with:
          sarif_file: ${{ steps.scan.outputs.sarif }}
----
Finally, we can authenticate with the con-
tainer registry and push the image representing our release candidate.
[source,yml,attributes]
----
      - name: Log into container registry
        uses: docker/login-action@v2 
        with:
          registry: ${{ env.REGISTRY }} 
          username: ${{ github.actor }} 
          password: ${{ secrets.GITHUB_TOKEN }} 
      - name: Publish container image 
        run: docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
----
=== Deployment pipeline(commit-stage): Validate Kubernetes manifests
Since a manifest specifies the desired state of an object, we should ensure that
our specification complies with the API exposed by Kubernetes. It’s a good idea to
automate this validation in the commit stage of a deployment pipeline to get fast
feedback in case of errors (rather than waiting until the acceptance stage, where we
need to use those manifests to deploy the application in a Kubernetes cluster)

Go to your Catalog Service project (catalog-service), and open the commit-stage.yml
file within the .github/workflows folder. 

[source,yml,attributes]
----
- name: Setup tools
  uses: alexellis/setup-arkade@v3
- name: Install tools
  uses: alexellis/arkade-get@master
  with:
    kustomize: latest
    kubeconform: latest
- name: Validate Kubernetes manifests
  run: |
    kustomize build k8s | kubeconform --strict -
----
=== Deployment pipeline(commit-stage): Build native images
When working locally, it’s convenient to run and test serverless applications on the
JVM rather than using GraalVM due to the shorter build time and the less resource-
demanding process. However, to achieve better quality and catch errors earlier, we
should run and verify the applications in native mode as early in the delivery process
as possible. The commit stage is where we compile and test our applications, so it
might be a good place to add those additional steps.

the commit stage execution after adding this steps takes quite a bit
longer than without building native image. As the commit stage is supposed to be fast, possibly under five minutes, to provide developers with fast feedback about their changes and allow them to move on to the next
task, in the spirit of continuous integration. The additional steps using GraalVM that
we have just added might slow down the workflow too much. In that case, you might
consider moving this check to the acceptance stage, where we allow the overall process to take longer.

[source,yml,attributes]
----
name: Commit Stage
on: push

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: polarbookshop/quote-function
  VERSION: ${{ github.sha }}

jobs:
  build:
    name: Build and Test
    runs-on: ubuntu-22.04
    permissions:
      contents: read
      security-events: write
    steps:
      - name: Checkout source code
        uses: actions/checkout@v4
      - name: Set up JDK
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: 17
          cache: gradle
      - name: Build, unit tests and integration tests
        run: |
          chmod +x gradlew
          ./gradlew build
      - name: Code vulnerability scanning
        uses: anchore/scan-action@v3
        id: scan
        with:
          path: "${{ github.workspace }}"
          fail-build: false
          severity-cutoff: high
      - name: Upload vulnerability report
        uses: github/codeql-action/upload-sarif@v3
        if: success() || failure()
        with:
          sarif_file: ${{ steps.scan.outputs.sarif }}
  native: 
    name: Build and Test (Native)
    runs-on: ubuntu-22.04
    permissions:
      contents: read
    steps:
      - name: Checkout source code
        uses: actions/checkout@v4
      - name: Set up GraalVM <1>
        uses: graalvm/setup-graalvm@v1
        with:
          java-version: '17'
          distribution: 'graalvm'
          github-token: ${{ secrets.GITHUB_TOKEN }}
      - name: Build, unit tests and integration tests (native) <2>
        run: |
          chmod +x gradlew
          ./gradlew nativeBuild
  package:
    name: Package and Publish
    if: ${{ github.ref == 'refs/heads/main' }}
    needs: [ build, native ] <3>
    runs-on: ubuntu-22.04
    permissions:
      contents: read
      packages: write
      security-events: write
    steps:
      - name: Checkout source code
        uses: actions/checkout@v4
      - name: Set up JDK
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: 17
          cache: gradle
      - name: Build container image
        run: |
          chmod +x gradlew
          ./gradlew bootBuildImage \
            --imageName ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
      - name: OCI image vulnerability scanning
        uses: anchore/scan-action@v3
        id: scan
        with:
          image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
          fail-build: false
          severity-cutoff: high
      - name: Upload vulnerability report
        uses: github/codeql-action/upload-sarif@v3
        if: success() || failure()
        with:
          sarif_file: ${{ steps.scan.outputs.sarif }}
      - name: Log into container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Publish container image
        run: docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }}
      - name: Publish container image (latest)
        run: |
          docker tag ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ env.VERSION }} \
            ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
          docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
----
<1> Installs and configures GraalVM with Java 17 and the native image component
<2> Compiles the application as a native executable and runs unit and integration tests
<3> The “Package and Publish” job runs only if both of the previous jobs complete successfully.

== Deployment pipeline(acceptance-stage)
The acceptance stage is triggered
whenever a new release candidate is published to the artifact repository. One option
for defining such a trigger is listening for the events published by GitHub whenever
the commit stage workflow has completed a run.

create a new acceptance-stage.yml file within the .github/workflows folder

Following the continuous integration principles, developers commit often during the day and repeatedly trigger the commit stage. Since the
commit stage is much faster than the acceptance stage, we risk creating a bottleneck.
When an acceptance stage run has completed, we are not interested in verifying all he release candidates that have queued up in the meantime. We are only interested
in the newest one, so the others can be discarded. GitHub Actions provides a mechanism for handling this scenario via concurrency controls.

Next, you would define several jobs to run in parallel against a production-like envi-
ronment, accomplishing functional and non-functional acceptance tests. For our
example, we’ll simply print a message, since we haven’t implemented the autotests for
this stage.

[[production-stage]]
== Deployment pipeline(production-stage)
Compared to the previous stages, implementing the production stage of a deployment pipeline can differ a lot depending on several factors. Let’s start by focusing on
the first step of the production stage.

At the end of the acceptance stage, we have a release candidate that’s proven to be
ready for production. After that, we need to update the Kubernetes manifests in our
production overlay with the new release version. When we’re keeping both the application source code and deployment scripts in the same repository, the production
stage could be listening to a specific event published by GitHub whenever the acceptance stage completes successfully, much like how we configured the flow between the
commit and acceptance stages.

If we are keeping the deployment scripts in a separate repository,
which means that whenever the acceptance stage workflow completes its execution
in the application repository, we need to notify the production stage workflow in the deployment repository. GitHub Actions provides the option of implementing this
notification process via a custom event.

Open your Catalog Service project (catalog-service), and go to the acceptance-
stage.yml file within the .github/workflows folder. After all the acceptance tests have
run successfully, we have to define a final step that will send a notification to the polar-
deployment repository and ask it to update the Catalog Service production manifests
with the new release version. That will be the trigger for the production stage.

``.github/workflows/acceptance-stage.yml``
[source,yml,attributes]
----
  deliver:
    name: Deliver release candidate to production
    needs: [ functional, performance, security ]
    runs-on: ubuntu-22.04
    steps:
      - name: Log into container registry
        uses: docker/login-action@v2 
        with:
          registry: ${{ env.REGISTRY }} 
          username: ${{ github.repository_owner }} 
          password: ${{ secrets.GITHUB_TOKEN }} 
      - name: Deliver application to production
        # An action to send an event to another repository and trigger a workflow
        uses: peter-evans/repository-dispatch@v3
        with:
          token: ${{ secrets.DISPATCH_TOKEN }}
          repository: ${{ env.OWNER }}/${{ env.DEPLOY_REPO }}
          # A name to identify the event (this is up to you)
          event-type: app_delivery
          # The payload of the message sent to the other repository. Add any information that the other repository might need to perform its operations.
          client-payload: '{
            "app_image": "${{ env.REGISTRY }}/${{ env.OWNER }}/${{ matrix.project }}",
            "app_name": "${{ matrix.project }}",
            "app_version": "${{ env.VERSION }}"}'
----
With this new step, if no error is found during the execution of the acceptance
tests, a notification is sent to the repository to trigger an update for this service.

By default, GitHub Actions doesn’t allow you to trigger workflows located in other
repositories, even if they both belong to you or your organization. Therefore, we need
to provide the repository-dispatch action with an access token that grants it such
permissions. The token can be a personal access token (PAT), a GitHub tool>

Go to your GitHub account, navigate to Settings > Developer Settings > Personal
Access Token, and choose Generate New Token. Input a meaningful name, and assign
it the workflow scope to give the token permissions to trigger workflows in other
repositories. Finally, generate the token and copy its value. GitHub will
show you the token value only once. Make sure you save it since you’ll need it soon.

image::{figures}/personal-access-token-granting-permissions-to-trigger-workflows-in-other-repositories.png[A personal access token (PAT) granting permissions to trigger workflows in other repositories]

Next, go to your Catalog Service repository on GitHub, navigate to the Settings tab,
and then select Secrets > Actions. On that page, choose New Repository Secret, name it DISPATCH_TOKEN (the same name we used in listing above), and input the value of
the PAT you generated earlier. Using the Secrets feature provided by GitHub, we can
provide the PAT securely to the acceptance stage workflow.

create a production-stage.yml file within a new .github/workflows folder

The production stage is triggered
whenever the acceptance stage from an application repository dispatches an app_delivery event. The event itself contains contextual information about the application name, image, and version for the newest release candidate. Since the application specific information is parameterized, we can use this workflow for all the applications

The first job of the production stage is updating the production Kubernetes manifests with the new release version. This job will consist of three steps:

1. Check out the  source code.
2. Update the production Kustomization with the new version for the given application.
3. Commit the changes to the  repository.

``.github/workflows/production-stage.yml``
[source,yml,attributes]
----
name: Production Stage

on:
  # Executes the workflow only when a new app_delivery event is received, dispatched from another repository
  repository_dispatch: <1>
    types: [app_delivery]

jobs:
  update:
    name: Update application version
    runs-on: ubuntu-22.04
    permissions:
      contents: write
    env:
      # Saves the event payload data as environment variables for convenience
      APP_IMAGE: ${{ github.event.client_payload.app_image }} <2>
      APP_NAME: ${{ github.event.client_payload.app_name }}
      APP_VERSION: ${{ github.event.client_payload.app_version }}
    steps:
        # Checks out the repository
      - name: Checkout source code <3>
        uses: actions/checkout@v4
      - name: Update image version <4>
        # Navigates to the production overlay for the given application
        # Updates the image name and version via Kustomize for the given application
        # Updates the tag used by Kustomize to access the correct base manifests stored in the application repository
        run: |
          cd polar-deployment/kubernetes/applications/${{ env.APP_NAME }}/production
          echo "image: ${{ env.APP_NAME }}=${{ env.APP_IMAGE }}:${{ env.APP_VERSION }}"
          kustomize edit set image ${{ env.APP_NAME }}=${{ env.APP_IMAGE }}:${{ env.APP_VERSION }}
          sed -i 's/ref=[\w+]/${{ env.APP_VERSION }}/' kustomization.yml
      # An action to commit and push the changes applied to the current repository from the previous step
      - name: Commit updated manifests <5>
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
            # Details about the commit operation
          commit_message: "Update ${{ env.APP_NAME }} to version ${{ env.APP_VERSION }}"
          branch: main
----
The new commit to the repository will trigger the deployment
pipeline. First, the commit stage will produce a container image (our release candidate) and publish it to GitHub Container Registry. Then the acceptance stage will fictitiously run further tests on the application and finally send a notification (a custom
app_delivery event) to the repository. The event triggers the production stage, which will update the production Kubernetes manifests for project services and commit the changes to the repository

image::{figures}/Deployment-pipeline-from-code-commit-to-ready-for-production-deployment.png[The commit stage goes from code commit to a release candidate, which goes through the acceptance stage. If it passes all the tests, the production stage updates the deployment manifests.]

