= Linux processes
:figures: 19-OS/linux/process

The most central concept in any operating system is the
process: an abstraction of a running program. Everything else hinges on this con-
cept, and the operating system designer (and student) should have a thorough
understanding of what a process is as early as possible.

Processes are one of the oldest and most important abstractions that operating
systems provide. They support the ability to have (pseudo) concurrent operation
even when there is only one CPU available. They turn a single CPU into multiple
virtual CPUs. When there are four or eight or more CPUs (cores) available, there
could be dozens or hundreds of processes running. Without the process abstrac-
tion, modern computing could not exist.

All modern computers often do several things at the same time. In any multiprogramming system, each CPU switches from process to process
quickly, running each for tens or maybe hundreds of milliseconds. While strictly
speaking, at any one instant each CPU is running only one process, in the course of
1 second it may work on several of them, giving the illusion of parallelism. Some-
times people speak of pseudoparallelism in this context, to contrast it with the
true hardware parallelism of multiprocessor systems (which have two or more
CPUs sharing the same physical memory). Keeping track of multiple, parallel
activities is hard for people to do. Therefore, operating system designers over the
years have evolved a conceptual model (sequential processes) that makes paral-
lelism easier to deal with.

All the runnable software on the computer, sometimes including
the operating system, is organized into a number of sequential processes, or just
processes for short. A process is just an instance of an executing program, includ-
ing the current values of the program counter, registers, and variables. Con-
ceptually, each process has its own virtual CPU. In reality, of course, each real
CPU switches back and forth from process to process, but to understand the sys-
tem, it is much easier to think about a collection of processes running in (pseudo)
parallel than to try to keep track of how each CPU switches from program to pro-
gram. Switching rapidly back and forth like this is known as multiprogramming

A process is an executing (i.e., running) instance of a program. Processes are also frequently referred to as tasks. A program is an executable file that is held in storage.

An executable file is a binary file (i.e., a file at least part of which is not plain text) that has been compiled (i.e., converted using a special type of program called a compiler) from source code into machine machine code, which is a pattern of bytes that can be read directly by a central processing unit (CPU). Source code is the version of software as it is originally written (i.e., typed into a computer) by a human in plain text (i.e., human readable alphanumeric characters). 

A program is a passive entity until it is launched, and a process can be thought of as a program in action. Processes are dynamic entities in that they are constantly changing as their machine code instructions are executed by the CPU. Each process consists of (1) system resources that are allocated to it, (2) a section of memory, (3) security attributes (such as its owner and its set of permissions) and (4) the processor state.

A process is basically a
program in execution. Associated with each process is its address space, a list of
memory locations from 0 to some maximum, which the process can read and write.
The address space contains the executable program, the program‚Äôs data, and its
stack. Also associated with each process is a set of resources, commonly including
registers (including the program counter and stack pointer), a list of open files,
outstanding alarms, lists of related processes, and all the other information needed
to run the program. A process is fundamentally a container that holds all the infor-
mation needed to run a program.

The processor state includes the contents of its registers and physical memory addresses. Registers are a very small amount of very fast memory that is built into a processor in order to speed up its operations by providing quick access to commonly used values. A memory address is a location in memory. 

An alternative definition of a process is the execution context of a running program, i.e., all of the activity in the current time slot in the CPU. A time slot, also called a time slice or a quantum, is the length of time that each process is permitted to run in the CPU until it is preempted (i.e., replaced) by another process in a time sharing operating system.

Linux and other Unix-like operating systems have been designed from the ground up as complete time sharing systems, that is, as both multitasking and multi-user systems. A multitasking system is one that allows multiple processes to operate seemingly simultaneously without interfering with each other, and a multi-user system allows multiple users to use the system simultaneously, with each having the illusion of being the sole user.

This intricate but robust time sharing capability is made possible by the ability of the system to both retain many processes in memory at the same time and switch between them fast enough to make it appear as though they are all running simultaneously. If one process crashes (i.e., stops functioning), it will usually not cause other processes to crash because each process runs in its own protected memory space (i.e., area of memory) and is not capable of interacting with other processes except through secure mechanisms managed by the kernel (i.e., the core of the operating system).

Programs and processes are distinct entities. Thus, in a multitasking operating system, multiple instances of a single program can be executing simultaneously, and each instance is a separate process (or processes). For example, if seven users, each with their own keyboard and display device, decide to run the vi text editor at the same time, there will be seven separate instances of vi, each a separate process, although they will all share the same executable file. A single user can likewise simultaneously run seven instances of vi, or some other program.

a process is an instance of an executing program. When a program
is executed, the kernel loads the code of the program into virtual memory, allo-
cates space for program variables, and sets up kernel bookkeeping data structures
to record various information (such as process ID, termination status, user IDs, and
group IDs) about the process.

image::{figures}/process-multiprogramming-example.png[]

In Fig. 2-1(a), we see a computer multiprogramming four programs in memo-
ry. In Fig. 2-1(b) we see four processes, each with its own flow of control (i.e., its
own logical program counter), and each one running independently of the other
ones. Of course, there is only one physical program counter, so when each process
runs, its logical program counter is loaded into the real program counter. When it is
finished (for the time being), the physical program counter is saved in the process‚Äô
stored logical program counter in memory. In Fig. 2-1(c) we see that, viewed over
a long enough time interval, all the processes have made progress, but at any given
instant only one process is actually running.

Another, compatible, definition of a process, for those familiar with the C programming language (in which the kernels and numerous other programs in Unix-like operating systems are written), is the collection of data structures that completely describe how far the execution of the program has progressed. A data structure is a way of storing data in a computer so that it can be used efficiently. 

if a program is running twice, it counts as two processes.
For example, it is often possible to start a word processor twice or print two files at
the same time if two printers are available. The fact that two processes happen to
be running the same program does not matter; they are distinct processes. The
operating system may be able to share the code between them so only one copy is
in memory, but that is a technical detail that does not change the conceptual situa-
tion of two processes running.

From a kernel point of view, processes are the entities among which the kernel
must share the various resources of the computer. For resources that are limited,
such as memory, the kernel initially allocates some amount of the resource to the
process, and adjusts this allocation over the lifetime of the process in response to
the demands of the process and the overall system demand for that resource.
When the process terminates, all such resources are released for reuse by other
processes. Other resources, such as the CPU and network bandwidth, are renew-
able, but must be shared equitably among all processes.

== Process Analogy
The difference between a process and a program is subtle, but absolutely cru-
cial. An analogy may help you here. Consider a culinary-minded computer scien-
tist who is baking a birthday cake for his young daughter. He has a birthday cake
recipe and a kitchen well stocked with all the input: flour, eggs, sugar, extract of
vanilla, and so on. In this analogy, the recipe is the program, that is, an algorithm
expressed in some suitable notation, the computer scientist is the processor (CPU),
and the cake ingredients are the input data. The process is the activity consisting of
our baker reading the recipe, fetching the ingredients, and baking the cake. Now imagine that the computer scientist‚Äôs son comes running in screaming his
head off, saying that he has been stung by a bee. The computer scientist records
where he was in the recipe (the state of the current process is saved), gets out a
first-aid book, and begins following the directions in it. Here we see the processor
being switched from one process (baking) to a higher-priority process (administer-
ing medical care), each having a different program (recipe versus first aid book).
When the bee sting has been taken care of, the computer scientist goes back to his
cake, continuing at the point where he left off.

The key idea here is that a process is an activity of some kind. It has a pro-
gram, input, output, and a state. A single processor may be shared among several
processes, with some scheduling algorithm being accustomed to determine when to
stop work on one process and service a different one. In contrast, a program is
something that may be stored on disk, not doing anything.

== process model
The process model is how the OS:

* Creates processes
* Runs them
* Pauses them
* Switches between them
* Destroys them

In this model, all the runnable software on the computer, sometimes including
the operating system, is organized into a number of sequential processes, or just
processes for short. 

The process model is based on two independent concepts: resource grouping and execution.

*Resource grouping* means everything a program has access to. Think of it as a container of resources:

* Memory (heap, global variables)
* Open files
* Network sockets
* Permissions (user ID, access rights)
* Current working directory

Imagine a web browser process, It owns:

* Loaded web pages in memory
* Open files (cache, downloads)
* Network connections

All of this belongs to one resource group. This is the resource side of a process. 

*Execution* means doing work:

* A sequence of CPU instructions
* A program counter (where you are in the code)
* Registers
* A stack (local variables, function calls)

for example Inside that browser:

* One execution is Running JavaScript
* Another execution is Rendering the page
* Another execution is Handling user input

This is the execution side.

== Process versus kernel views of the system
A running system typically has numerous processes. For a process, many things
happen asynchronously. 

* An executing process doesn‚Äôt know when it will next time
out, which other processes will then be scheduled for the CPU (and in what order),
or when it will next be scheduled. The delivery of signals and the occurrence of
interprocess communication events are mediated by the kernel, and can occur at
any time for a process. Many things happen transparently for a process. 
* A process
doesn‚Äôt know where it is located in RAM or, in general, whether a particular part of
its memory space is currently resident in memory or held in the swap area (a
reserved area of disk space used to supplement the computer‚Äôs RAM). 
* Similarly, a
process doesn‚Äôt know where on the disk drive the files it accesses are being held; it
simply refers to the files by name. 
* A process operates in isolation; it can‚Äôt directly
communicate with another process. 
* A process can‚Äôt itself create a new process or
even end its own existence. 
* a process can‚Äôt communicate directly with the
input and output devices attached to the computer.

By contrast, a running system has one kernel that knows and controls every-
thing. 

* The kernel facilitates the running of all processes on the system. 
* The kernel
decides which process will next obtain access to the CPU, when it will do so, and for
how long. 
* The kernel maintains data structures containing information about all
running processes and updates these structures as processes are created, change
state, and terminate. 
* The kernel maintains all of the low-level data structures that
enable the filenames used by programs to be translated into physical locations on
the disk. 
* The kernel also maintains data structures that map the virtual memory of
each process into the physical memory of the computer and the swap area(s) on
disk. 
* All communication between processes is done via mechanisms provided by
the kernel. In response to requests from processes, the kernel creates new processes and terminates existing processes. 
* the kernel (in particular, device
drivers) performs all direct communication with input and output devices, transferring information to and from user processes as required.

When we say things such as ‚Äúa process can create another pro-
cess,‚Äù ‚Äúa process can create a pipe,‚Äù ‚Äúa process can write data to a file,‚Äù and ‚Äúa pro-
cess can terminate by calling exit().‚Äù Remember, however, that the kernel mediates
all such actions, and these statements are just shorthand for ‚Äúa process can request
that the kernel create another process,‚Äù and so on.

== Process memory layout
A process is logically divided into the following parts, known as segments:

* Text segment: the instructions of the program(i.e., the program code).
* Data segment: the static variables used by the program.
* Heap segment: an area from which programs can dynamically allocate extra memory.
* Stack segment: a piece of memory that grows and shrinks as functions are called and
return and that is used to allocate storage for local variables and function call
linkage information.

the data segment grows upward and the stack grows downward,
as shown in next Fig. Between them is a gap of unused address space. The stack
grows into the gap automatically, as needed, but expansion of the data segment is
done explicitly by using a system call, brk, which specifies the new address where
the data segment is to end. This call, however, is not defined by the POSIX stan-
dard, since programmers are encouraged to use the malloc library procedure for
dynamically allocating storage, and the underlying implementation of malloc was
not thought to be a suitable subject for standardization since few programmers use
it directly and it is doubtful that anyone even notices that brk is not in POSIX. (In
most systems, there are other memory areas also, for instance those create with the
mmap system call, which creates a new virtual memory areas)

image::{figures}/process-memory-layout.png[Processes have three segments: text, data, and stack]

== Viewing Processes
The ps command is used to list the currently running processes and their PIDs. At a bare minimum, two processes will be shown, the shell (usually bash on Linux) and ps, which itself is a process and which dies as soon as its output is displayed. Usually, there will be many more. The following will provide a full listing of the current processes:

    ps -aux | less

The -a option tells ps to list the processes of all users on the system rather than just those of the current user. The -u option tells ps to provide detailed information about each process. The -x option adds to the list processes that have no controlling terminal, such as daemon that are started during booting. In contrast to most commands, the hyphen preceding the option(s) with ps is optional.

As the number of processes can be quite long and occupy more than a single screen, the output of ps aux can be piped (i.e., transferred) to the less command, which lets it be viewed one screenful at a time. The output can be advanced one screen forward by pressing the SPACE bar and moved one screen backward by pressing the b key.

Among the information that ps aux provides about each process is the user of the process, the PID, the percentage of CPU used by the process, the percentage of memory used by the process, VSZ (virtual size in kilobytes), RSS (real memory size or resident set size in 1024 byte units), STAT (the process state code), the starting time of the process, the length of time the process has been active and the command that initiated the process. The process state codes include D, uninterruptable sleep; N, low priority; R, runnable (on run queue); S, sleeping; T, traced or stopped; and Z, defunct (zombie).

The processes can also be viewed with the pstree command, which can be used as follows to list all of the processes currently on the system in the form of a tree diagram:

    pstree

The addition of the -p option will also show the PIDs:

    pstree -p

The processes that are directly connected to the main stem (i.e., a vertical line extending downward from init along the left hand edge of the screen) of the tree are listed by default in alphabetic order. This is in contrast to ps, which by default lists the processes in the order in which they were created. It can be seen that pstree itself is also listed as a process. 

== Controlling Processes
There are a number of reasons that a user would want to control processes, possibly the most common of which is to close a program that has frozen or crashed. This can be accomplished by using the lethal-sounding kill command.

For example, if the Mozilla web browser freezes and it cannot be closed by using ordinary keyboard commands or mouse clicks, it can in many cases be closed by first using ps aux | less or pstree -p to obtain Mozilla's PID and then using that PID as an argument (i.e., input) with kill. Thus, if the PID were found to be 1102, then the Mozilla process could be killed with the following:

    kill 1102

Another reason that a user might want to control processes is to make use of job control, a feature of the shell that facilitates the handling of multiple processes. Job control can be used to switch processes between the foreground and the background, and it allows programs to be started initially in the background.

Running a job in the background is typically done when its execution is expected to take a long time and in order to free the issuing terminal after entering the command. (Some processes are not suitable for running in the background, such as text editors, which occupy the full console or terminal window screen.) Starting a program in the background is accomplished by typing its name followed by an ampersand. For example, typing

    gftp &

at the command line starts gftp, an open source FTP (file transfer protocol) program that can be used to send files between computers, in the background. This frees the console or terminal window for use by other commands while gftp is downloading or uploading programs.

A process that is running in the foreground can be suspended by simultaneously pressing the CTRL and z keys and can be terminated by simultaneously pressing the CTRL and c keys. The command bg reactivates a suspended program in the background, and the command fg puts a suspended program or a program that is running in the background into the foreground. 

== Daemons
Daemons are a class of processes that run continuously in the background, rather than under the direct control of a user. The term is derived from the ancient Greek word daimon, which refers to a supernatural being that is intermediate between a human and a god, or similar to a guiding spirit. Daemons are generally easy to recognize because their names end with the letter d.

Daemons are usually launched automatically while a computer is booting up and then wait in the background until their services are required. They typically respond to hardware activity, to network requests or to other programs by performing specified tasks. They can also configure hardware (such as the daemon devfsd, which can provide intelligent management of device entries in the device filesystem on some Linux systems), run scheduled tasks (e.g., crond) and perform a variety of other functions.

Another example is the secure networking daemon, xinetd (eXtended InterNET services Daemon), which is usually launched during booting and listens passively until a program, such as FTP or telnet, requests a connection

== Process Life Cycle
When a computer is booted up (i.e., started), the operating system is loaded into memory. The first part of the operating system that is loaded is vmlinuz, which is the compressed kernel executable.

This results in the creation of init, which is the first process of the session and which becomes the ancestor of all other processes created during that session. The role of init is to read the entries in the file /etc/inittab and execute various programs according to that file. This includes starting the getty process on each of the login terminals, which eventually provides the designated shell for each user. 

A program can be started automatically or by a user typing in the name (and correct path if necessary) of the program at the command line (i.e., all-text display mode) and then pressing the ENTER key. This causes the program to be read into memory and executed by the kernel. Some programs create a single process when launched, such as ls (which is used to show the contents of a directory), whereas others, such as OpenOffice (an increasingly popular and open source office suite), initiate a series of processes. 

In Unix-like operating systems, each process is given a unique number, referred to as a process identification (PID), when it is created, and this number is used by the system to reference the process. Each process is guaranteed to have a unique PID, which is always a non-negative integer. init always has a PID of 1 because it is always the first process on the system. A very large PID does not necessarily mean that there are anywhere near that many processes on a system, because such numbers are often a result of the fact that PIDs are not immediately reused in order to prevent possible errors. 

While a process is running, it can spawn (i.e., give birth to) other processes. Spawning is accomplished through the use of a system call termed a fork (because it splits in two). System calls are clearly defined, direct entry points into the kernel through which processes request services from the kernel. 

The first step in spawning a new process is for an existing process to create an identical copy of itself. This copy is then transformed into the new process, and it, in turn, can create additional processes, thereby resulting in multiple generations of processes (i.e., parents spawn children which spawn grandchildren). Analogies can be made with the filesystem hierarchy of Unix-like systems and also with the object hierarchy in an object-oriented programming language (such as Java, in which all classes are descendants of the class named Object). 

As is virtually everything else running in a Unix-like operating system, the shell is also a process. (The big exception is the kernel, which is a set of routines that resides continuously in memory and to which all processes have access.) When a user types in a command, the shell spawns a process that executes that command. Unless the user specifies otherwise, the shell typically waits for this child process to be completed before it displays the prompt again to indicate that it is ready for a new command. A prompt, also referred to as a command prompt, is a short text message at the start of each line on a console or terminal window. 

If a process is suspended (i.e., temporarily not in use), it becomes eligible for swapping (i.e., transferring) to the swap partition in order to free up space in the main memory for other processes.

During its lifetime, a process will utilize a variety of system resources. They include (1) the processor to run its instructions, (2) the memory to hold it and its data, (3) files within the filesystem and (4) physical devices on the system. The operating system must keep track of each process and the resources it uses in order to manage it and the other processes efficiently, i.e., so that no one process monopolizes the processor or memory. 

the easiest way to get a good intuitive feel for a process is to think
about a multiprogramming system. The user may have started a video editing pro-
gram and instructed it to convert a 2-hour video to a certain format (something that
can take hours) and then gone off to surf the Web. Meanwhile, a background proc-
ess that wakes up periodically to check for incoming email may have started run-
ning. Thus, we have (at least) three active processes: the video editor, the Web
browser, and the email receiver. Periodically, the operating system decides to stop
running one process and start running another, perhaps because the first one has
used up more than its share of CPU time in the past second or two.

When a process is suspended temporarily like this, it must later be restarted in
exactly the same state it had when it was stopped. This means that all information
about the process must be explicitly saved somewhere during the suspension. For
example, the process may have several files open for reading at once. Associated
with each of these files is a pointer giving the current position (i.e., the number of
the byte or record to be read next). When a process is temporarily suspended, all
these pointers must be saved so that a read call executed after the process is restart-
ed will read the proper data. In many operating systems, all the information about
each process, other than the contents of its own address space, is stored in an oper-
ating system table called the process table, which is an array of structures, one for
each process currently in existence.

Thus, a (suspended) process consists of its address space, usually called the
core image (in honor of the magnetic core memories used in days of yore), and its
process table entry, which contains the contents of its registers and many other
items needed to restart the process later.

=== Process States
Although each process is an independent entity, with its own program counter
and internal state, processes often need to interact with other processes. One proc-
ess may generate some output that another process uses as input. In the command
----
cat chapter1 chapter2 chapter3 | grep tree
----
the first process, running cat, concatenates and outputs three files. The second
process, running grep, selects all lines containing the word ‚Äò‚Äòtree.‚Äô‚Äô Depending on
the relative speeds of the two processes (which depends on both the relative com-
plexity of the programs and how much CPU time each one has had), it may happen
that grep is ready to run, but there is no input waiting for it. It must then block
until some input is available.

When a process blocks, it does so because logically it cannot continue, typi-
cally because it is waiting for input that is not yet available. It is also possible for a
process that is conceptually ready and able to run to be stopped because the operat-
ing system has decided to allocate the CPU to another process for a while. These
two conditions are completely different. In the first case, the suspension is inherent
in the problem (you cannot process the user‚Äôs command line until it has been
typed). In the second case, it is a technicality of the system (not enough CPUs to
give each process its own private processor)

During its lifecycle, a process moves through several states:
[cols="1,4", options="header"]
|===
| State | Description
| Running (R) | Actively using the CPU or waiting in the run-queue.
| Ready | runnable; temporarily stopped to let another process run.
| Sleeping (S/D) | (Blocked)Waiting for an event (Interruptible) or disk I/O (Uninterruptible).
| Stopped (T) | Suspended by a signal (e.g., SIGSTOP or Ctrl+Z).
| Zombie (Z) | Terminated, but waiting for the parent to read its exit status.
|===
image::{figures}/process-states.png[A process can be in running, blocked, or ready state. Transitions between these states are as shown.]

Logically, the first two states are similar. In both cases the process is willing to run,
only in the second one, there is temporarily no CPU available for it. 

The third state is fundamentally different from the first two in that the process cannot run, even if the CPU is idle and has nothing else to do

Four transitions are possible among these three states, as shown. 

. Transition 1
occurs when the operating system discovers that a process cannot continue right
now. In some systems the process can execute a system call, such as pause, to get
into blocked state. In other systems, including UNIX, when a process reads from a
pipe or special file (e.g., a terminal) and there is no input available, the process is
automatically blocked.
. Transitions 2 and 3 are caused by the process scheduler, a part of the operating
system, without the process even knowing about them. Transition 2 occurs when
the scheduler decides that the running process has run long enough, and it is time
to let another process have some CPU time. 
. Transition 3 occurs when all the other
processes have had their fair share and it is time for the first process to get the CPU
to run again. The subject of scheduling, that is, deciding which process should run
when and for how long, is an important one. Many algorithms have been devised to try to balance the competing demands of
efficiency for the system as a whole and fairness to individual processes. 
. Transition 4 occurs when the external event for which a process was waiting
(such as the arrival of some input) happens. If no other process is running at that
instant, transition 3 will be triggered and the process will start running. Otherwise
it may have to wait in ready state for a little while until the CPU is available and its
turn comes.

== Process creation and program execution
The key process-management system calls are those dealing with the creation
and termination of processes. Consider a typical example. A process called the
command interpreter or (i.e., shell) reads commands from a terminal. The user
has just typed a command requesting that a program be compiled. The shell must
now create a new process that will run the compiler. When that process has fin-
ished the compilation, it executes a system call to terminate itself.

Four principal events cause processes to be created:

. *System initialization*
+
When an operating system is booted, typically numerous processes are created.
Some of these processes are foreground processes, that is, processes that interact
with (human) users and perform work for them. Others run in the background and
are not associated with particular users, but instead have some specific function.
For example, one background process may be designed to accept incoming email,
sleeping most of the day but suddenly springing to life when email arrives. Another
background process may be designed to accept incoming requests for Web pages
hosted on that machine, waking up when a request arrives to service the request.
Processes that stay in the background to handle some activity such as email, Web
pages, news, printing, and so on are called daemons. Large systems commonly
have dozens of them. In UNIX‚Ä†, the ps program can be used to list the running
processes. 
. *Execution of a process-creation system call by a running process*
+
Often a running process will issue system calls to create one or
more new processes to help it do its job. Creating new processes is particularly use-
ful when the work to be done can easily be formulated in terms of several related,
but otherwise independent interacting processes. For example, if a large amount of
data are being fetched over a network for subsequent processing, it may be con-
venient to create one process to fetch the data and put them in a shared buffer while
a second process removes the data items and processes them. On a multiprocessor,
allowing each process to run on a different CPU may also make the job go faster.
. *A user request to create a new process*
+
In interactive systems, users can start a program by typing a command or (dou-
ble) clicking on an icon. Taking either of these actions starts a new process and
runs the selected program in it. In command-based UNIX systems running the X
Window System, the new process takes over the window in which it was started. In
Windows, when a process is started it does not have a window, but it can create one
(or more) and most do. In both systems, users may have multiple windows open at
once, each running some process. Using the mouse, the user can select a window
and interact with the process, for example, providing input when needed
. *Initiation of a batch job*
+
This applies only to batch systems
found on large mainframes. Think of inventory management at the end of a day at
a chain of stores‚Äîcalculating what to order, analyzing per-store product popular-
ity, etc. Here users can submit batch jobs to the system (possibly remotely). When
the operating system decides it has the resources to run another job, it creates a
new process and runs the next job from the input queue in it.

in all these cases, a new process is created by having an existing
process execute a process creation system call. That process may be a running user
process, a system process invoked from the keyboard or mouse, or a batch-man-
ager process. What that process does is execute a system call to create the new
process. This system call tells the operating system to create a new process and
indicates, directly or indirectly, which program to run in it

In UNIX, there is only one system call to create a new process: fork. This call
creates an exact clone of the calling process. After the fork, the two processes, the
parent and the child, have the same memory image, the same environment strings,
and the same open files. That is all there is. Usually, the child process then
executes execve or a similar system call to change its memory image and run a
new program. For example, when a user types a command, say, sort, to the shell,
the shell forks off a child process and the child executes sort. The reason for this
two-step process is to allow the child to manipulate its file descriptors after the fork
but before the execve in order to accomplish redirection of standard input, standard
output, and standard error.

.ChatGPT
[%collapsible]
====
Why does the child usually call execve()?
Problem

If fork() only makes a copy, how do we run a different program (like sort, ls, etc.)?

Solution: execve()

execve():

Throws away the child‚Äôs memory

Loads a new program into the same process

So the process stays the same, but the program changes.

5Ô∏è‚É£ The classic two-step process: fork() ‚Üí execve()

This is the UNIX process creation model:

fork() ‚Üí make a child

execve() ‚Üí replace child‚Äôs program

Example: running sort

When you type:

sort file.txt


What actually happens:

The shell (bash) is running

The shell calls fork()

Now there is a child shell

The child calls execve("sort")

The child becomes the sort program

The parent shell waits

6Ô∏è‚É£ Why not just ‚Äúcreate a process and run a program‚Äù in one step?

This is the key idea in the paragraph.

Because UNIX wants flexibility

Between fork() and execve(), the child can prepare its environment.

Most importantly:
üëâ File descriptor manipulation

7Ô∏è‚É£ File descriptors (FDs) in simple terms

File descriptors are numbers that represent open files:

FD number	Meaning
0	Standard Input (stdin)
1	Standard Output (stdout)
2	Standard Error (stderr)

By default:

Input ‚Üí keyboard

Output ‚Üí terminal

Error ‚Üí terminal

8Ô∏è‚É£ Why fork() first helps with redirection
Example: output redirection

Command:

sort file.txt > out.txt


What happens:

Shell calls fork()

Child process:

Opens out.txt

Replaces FD 1 (stdout) with out.txt

Child calls execve("sort")

sort writes to stdout

But stdout now points to out.txt

üí° sort doesn‚Äôt know about files or redirection at all
It just writes to FD 1.

9Ô∏è‚É£ Example: piping

Command:

ls | sort


Steps:

Shell creates a pipe

Shell forks twice

Child 1:

stdout ‚Üí pipe write end

exec ls

Child 2:

stdin ‚Üí pipe read end

exec sort

Again:

All the setup happens after fork() and before execve().
====

After a process is created, the parent and
child have their own distinct address spaces. If either process changes a word in its
address space, the change is not visible to the other process. In traditional UNIX,
the child‚Äôs initial address space is a copy of the parent‚Äôs, but there are definitely
two distinct address spaces involved; no writable memory is shared. Some UNIX
implementations share the program text between the two since that cannot be mod-
ified. Alternatively, the child may share all of the parent‚Äôs memory, but in that case
the memory is shared copy-on-write, which means that whenever either of the two
wants to modify part of the memory, that chunk of memory is explicitly copied
first to make sure the modification occurs in a private memory area. Again, no
writable memory is shared. It is, however, possible for a newly created process to
share some of its creator‚Äôs other resources, such as open files. 

.From AI
[%collapsible]
======
[tabs]
=====
ChatGPT::
+
[tabs]
====
Concept::
+
1. What is an ‚Äúaddress space‚Äù?
+
An address space is the memory that a process thinks it owns.
+
Each process sees memory starting from address 0
+
It cannot see or touch another process‚Äôs memory
+
The OS + MMU enforce this separation
+
üëâ Think of it as each process living in its own private apartment.
+
2. Parent and child processes have separate memory
+
When a process creates another process (for example using fork() in UNIX):
+
The parent keeps its memory
+
The child gets its own memory
+
Changes in one do not affect the other
+
Example
+
----
int x = 5;

fork();   // creates child

x = 10;
----
+
Parent sets x = 10
+
Child still sees x = 5
+
They do not interfere with each other
+
üëâ Even though the child started from the parent, they are now independent.
+
3. Traditional UNIX: ‚Äúcopy of the parent‚Äôs memory‚Äù
+
In classic UNIX:
+
The child‚Äôs memory is initially a copy of the parent‚Äôs memory
+
After creation:
+
Two distinct address spaces
+
No writable memory is shared
+
Analogy
+
You photocopy a notebook
+
Each person writes in their own copy
+
Writing in one does not change the other
+
4. Sharing program text (code) is safe
+
Some UNIX systems share the program text (machine code):
+
Code is read-only
+
No one can modify it
+
Safe to share
+
Example
+
Parent and child both run /bin/ls
+
The machine instructions for ls are stored once in RAM
+
Both processes use the same copy
+
üëâ This saves memory and is safe because code does not change.
+
5. Copy-on-Write (COW): smart optimization
+
Modern UNIX systems use copy-on-write.
+
What it means
+
Parent and child initially share the same physical memory
+
Memory is marked read-only
+
If one tries to modify it:
+
Only that part is copied
+
The writer gets a private copy
+
Example
+
----
int x = 5;
fork();      // memory shared (read-only)

x = 6;       // triggers copy-on-write
----
+
OS copies the memory page containing x
+
Parent and child now have separate pages
+
Still no writable memory is shared
+
Analogy
+
Two people share a Google Doc in view-only
+
When one clicks ‚ÄúEdit‚Äù:
+
They get their own copy
+
Changes don‚Äôt affect the other
+
6. ‚ÄúNo writable memory is shared‚Äù (IMPORTANT)
+
No matter how it‚Äôs implemented:
+
Direct copy
+
Shared code
+
Copy-on-write
+
‚û°Ô∏è Writable memory is never shared by default
+
This guarantees:
+
Safety
+
Security
+
No accidental corruption
+
7. But some resources can be shared
+
Memory is private, but other resources may be shared.
+
Example: open files
+
----
int fd = open("data.txt", O_WRONLY);
fork();
----
+
Parent and child share:
+
The same file descriptor
+
The same file offset
+
If parent writes first:
+
Hello
+
Child writes next:
+
HelloWorld
+
üëâ This sharing is intentional and controlled by the OS.
+
8. Summary in one table
Concept	Simple meaning
Address space	Process‚Äôs private memory
Parent vs child	Separate memory
Traditional UNIX	Full memory copy
Shared program text	Code is shared (read-only)
Copy-on-write	Copy only when writing
Writable memory	Never shared by default
Shared resources	Files, sockets, etc.
One-sentence takeaway
+
Processes are isolated in memory for safety, but the OS uses clever tricks like copy-on-write to make this isolation fast and memory-efficient.

memory layout diagram::
+
----
Before fork()
-------------

Process P (Parent)
+-------------------------+
| Stack                   |
|-------------------------|
| Heap                    |
|-------------------------|
| Data (globals)          |
|-------------------------|
| Code (text, read-only)  |
+-------------------------+


After fork() ‚Äî logical view
---------------------------

Parent Process                 Child Process
+---------------------+        +---------------------+
| Stack (private)     |        | Stack (private)     |
|---------------------|        |---------------------|
| Heap (COW)          |<-----> | Heap (COW)          |
|---------------------|        |---------------------|
| Data (COW)          |<-----> | Data (COW)          |
|---------------------|        |---------------------|
| Code (shared RO)    |<-----> | Code (shared RO)    |
+---------------------+        +---------------------+
----
+
Legend:
RO  = Read-only
COW = Copy-on-write
<--> = Initially shared physical memory
+
What happens on write (Copy-on-Write)
+
----
Child writes to Heap
--------------------

Parent Process                 Child Process
+---------------------+        +---------------------+
| Stack (private)     |        | Stack (private)     |
|---------------------|        |---------------------|
| Heap (original)     |        | Heap (copied page)  |
|---------------------|        |---------------------|
| Data (shared COW)   |<-----> | Data (shared COW)   |
|---------------------|        |---------------------|
| Code (shared RO)    |<-----> | Code (shared RO)    |
+---------------------+        +---------------------+
----
+
Only the modified memory page is copied
+
The rest remains shared
+
No writable memory is shared after the write
+
File sharing (not memory)
+
----
Open File Table
----------------
FD 3 --> data.txt (offset = 128)

Parent FD 3  --------+
                      +--> Same open file entry
Child  FD 3  --------+
----
+
Memory is isolated
+
Files can be shared intentionally
+
One-line intuition
+
Processes look duplicated, memory starts shared, but the first write splits them apart.
+
If you want next, I can:
+
Redraw this for Windows process creation
+
Show fork + exec transition
+
Add shared memory (shm) case where memory is writable and shared
====

Gemini:: Gemini

=====
======

A process can create a new process using the fork() system call. The process that
calls fork() is referred to as the parent process, and the new process is referred to as
the child process. The kernel creates the child process by making a duplicate of the 32 Chapter 2
parent process. The child inherits copies of the parent‚Äôs data, stack, and heap seg-
ments, which it may then modify independently of the parent‚Äôs copies. (The pro-
gram text, which is placed in memory marked as read-only, is shared by the two
processes.)

The child process goes on either to execute a different set of functions in the
same code as the parent, or, frequently, to use the execve() system call to load and
execute an entirely new program. An execve() call destroys the existing text, data,
stack, and heap segments, replacing them with new segments based on the code of
the new program.

Several related C library functions are layered on top of execve(), each provid-
ing a slightly different interface to the same functionality. All of these functions
have names starting with the string exec.

In Linux, process creation and program execution are distinct operations governed by a "copy-and-replace" model. A process is an active instance of a program, identified by a unique Process ID (PID).

=== Process Creation via fork()
Most processes in Linux are created by an existing parent process using the fork() or clone() system calls.

* The Clone Mechanism: fork() creates a child process that is an exact duplicate of the parent. The child inherits memory, environment variables, and file descriptors.
* Copy-on-Write (COW): To save resources, the kernel does not copy physical memory immediately. Both processes share the same memory pages until one attempts a "write" operation, at which point a private copy is made.
* The Init Process: All processes originate from init (typically systemd in 2026), which holds PID 1.

[tabs]
====
Kernel::
+
In the Linux kernel, process creation is primarily managed in
kernel/fork.c. The modern entry point for creating processes‚Äîwhether through fork(), vfork(), or pthread_create()‚Äîis the kernel_clone() function (formerly known as _do_fork()).
+
. The Entry Point: kernel_clone()
+
All process creation system calls eventually converge here. This function orchestrates the cloning process.
+
/* Simplified representation of kernel_clone in kernel/fork.c */
pid_t kernel_clone(struct kernel_clone_args *args)
{
    struct task_struct *p;
    pid_t nr;

    // 1. Perform the actual work of copying the process
    p = copy_process(NULL, trace, NUMA_NO_NODE, args);

    if (!IS_ERR(p)) {
        struct pid *pid;

        // 2. Get the PID of the newly created task
        pid = get_task_pid(p, PIDTYPE_PID);
        nr = pid_vnr(pid);

        // 3. Wake up the new process and add it to the scheduler
        wake_up_new_task(p);

        put_pid(pid);
        return nr;
    }

    return PTR_ERR(p);
}
+
. The Core Logic: copy_process()
+
The copy_process() function is where the "heavy lifting" occurs. It clones the parent's task_struct and handles the resource sharing defined by the clone flags.
+
* dup_task_struct(): Allocates a new task_struct (the process descriptor) and kernel stack for the child.
* Resource Copying: Depending on flags (e.g., CLONE_VM, CLONE_FILES), it calls various sub-functions:
** copy_namespaces(): Clones or shares namespaces (network, mount, etc.).
** copy_files(): Copies or shares open file descriptors.
** copy_mm(): Handles memory management; if CLONE_VM is NOT set, it sets up Copy-on-Write (COW).
** copy_thread(): Initializes the CPU-specific registers for the child, ensuring it begins execution at the correct instruction. 
+
. System Call Definitions
+
The kernel uses macros to define the actual system calls that userspace invokes. These calls wrap the generic clone logic with specific flags. 
+
/* Implementation of the fork() system call */
SYSCALL_DEFINE0(fork)
{
    struct kernel_clone_args args = {
        .exit_signal = SIGCHLD,
    };
    return kernel_clone(&args);
}
+
/* Implementation of the vfork() system call */
SYSCALL_DEFINE0(vfork)
{
    struct kernel_clone_args args = {
        .flags = CLONE_VFORK | CLONE_VM,
        .exit_signal = SIGCHLD,
    };
    return kernel_clone(&args);
}
+
https://github.com/torvalds/linux/blob/master/kernel/fork.c
kernel_clone::
+
[source, C]
----
/*
 *  Ok, this is the main fork-routine.
 *
 * It copies the process, and if successful kick-starts
 * it and waits for it to finish using the VM if required.
 *
 * args->exit_signal is expected to be checked for sanity by the caller.
 */
pid_t kernel_clone(struct kernel_clone_args *args)
{
	u64 clone_flags = args->flags;
	struct completion vfork;
	struct pid *pid;
	struct task_struct *p;
	int trace = 0;
	pid_t nr;

	/*
	 * For legacy clone() calls, CLONE_PIDFD uses the parent_tid argument
	 * to return the pidfd. Hence, CLONE_PIDFD and CLONE_PARENT_SETTID are
	 * mutually exclusive. With clone3() CLONE_PIDFD has grown a separate
	 * field in struct clone_args and it still doesn't make sense to have
	 * them both point at the same memory location. Performing this check
	 * here has the advantage that we don't need to have a separate helper
	 * to check for legacy clone().
	 */
	if ((clone_flags & CLONE_PIDFD) &&
	    (clone_flags & CLONE_PARENT_SETTID) &&
	    (args->pidfd == args->parent_tid))
		return -EINVAL;

	/*
	 * Determine whether and which event to report to ptracer.  When
	 * called from kernel_thread or CLONE_UNTRACED is explicitly
	 * requested, no event is reported; otherwise, report if the event
	 * for the type of forking is enabled.
	 */
	if (!(clone_flags & CLONE_UNTRACED)) {
		if (clone_flags & CLONE_VFORK)
			trace = PTRACE_EVENT_VFORK;
		else if (args->exit_signal != SIGCHLD)
			trace = PTRACE_EVENT_CLONE;
		else
			trace = PTRACE_EVENT_FORK;

		if (likely(!ptrace_event_enabled(current, trace)))
			trace = 0;
	}

	p = copy_process(NULL, trace, NUMA_NO_NODE, args);
	add_latent_entropy();

	if (IS_ERR(p))
		return PTR_ERR(p);

	/*
	 * Do this prior waking up the new thread - the thread pointer
	 * might get invalid after that point, if the thread exits quickly.
	 */
	trace_sched_process_fork(current, p);

	pid = get_task_pid(p, PIDTYPE_PID);
	nr = pid_vnr(pid);

	if (clone_flags & CLONE_PARENT_SETTID)
		put_user(nr, args->parent_tid);

	if (clone_flags & CLONE_VFORK) {
		p->vfork_done = &vfork;
		init_completion(&vfork);
		get_task_struct(p);
	}

	if (IS_ENABLED(CONFIG_LRU_GEN_WALKS_MMU) && !(clone_flags & CLONE_VM)) {
		/* lock the task to synchronize with memcg migration */
		task_lock(p);
		lru_gen_add_mm(p->mm);
		task_unlock(p);
	}

	wake_up_new_task(p);

	/* forking complete and child started to run, tell ptracer */
	if (unlikely(trace))
		ptrace_event_pid(trace, pid);

	if (clone_flags & CLONE_VFORK) {
		if (!wait_for_vfork_done(p, &vfork))
			ptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);
	}

	put_pid(pid);
	return nr;
}
----

copy_process::
+
[source, C]
----
/*
 * This creates a new process as a copy of the old one,
 * but does not actually start it yet.
 *
 * It copies the registers, and all the appropriate
 * parts of the process environment (as per the clone
 * flags). The actual kick-off is left to the caller.
 */
__latent_entropy struct task_struct *copy_process(
					struct pid *pid,
					int trace,
					int node,
					struct kernel_clone_args *args)
{
	int pidfd = -1, retval;
	struct task_struct *p;
	struct multiprocess_signals delayed;
	struct file *pidfile = NULL;
	const u64 clone_flags = args->flags;
	struct nsproxy *nsp = current->nsproxy;

	/*
	 * Don't allow sharing the root directory with processes in a different
	 * namespace
	 */
	if ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))
		return ERR_PTR(-EINVAL);

	if ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))
		return ERR_PTR(-EINVAL);

	/*
	 * Thread groups must share signals as well, and detached threads
	 * can only be started up within the thread group.
	 */
	if ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))
		return ERR_PTR(-EINVAL);

	/*
	 * Shared signal handlers imply shared VM. By way of the above,
	 * thread groups also imply shared VM. Blocking this case allows
	 * for various simplifications in other code.
	 */
	if ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))
		return ERR_PTR(-EINVAL);

	/*
	 * Siblings of global init remain as zombies on exit since they are
	 * not reaped by their parent (swapper). To solve this and to avoid
	 * multi-rooted process trees, prevent global and container-inits
	 * from creating siblings.
	 */
	if ((clone_flags & CLONE_PARENT) &&
				current->signal->flags & SIGNAL_UNKILLABLE)
		return ERR_PTR(-EINVAL);

	/*
	 * If the new process will be in a different pid or user namespace
	 * do not allow it to share a thread group with the forking task.
	 */
	if (clone_flags & CLONE_THREAD) {
		if ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||
		    (task_active_pid_ns(current) != nsp->pid_ns_for_children))
			return ERR_PTR(-EINVAL);
	}

	if (clone_flags & CLONE_PIDFD) {
		/*
		 * - CLONE_DETACHED is blocked so that we can potentially
		 *   reuse it later for CLONE_PIDFD.
		 */
		if (clone_flags & CLONE_DETACHED)
			return ERR_PTR(-EINVAL);
	}

	/*
	 * Force any signals received before this point to be delivered
	 * before the fork happens.  Collect up signals sent to multiple
	 * processes that happen during the fork and delay them so that
	 * they appear to happen after the fork.
	 */
	sigemptyset(&delayed.signal);
	INIT_HLIST_NODE(&delayed.node);

	spin_lock_irq(&current->sighand->siglock);
	if (!(clone_flags & CLONE_THREAD))
		hlist_add_head(&delayed.node, &current->signal->multiprocess);
	recalc_sigpending();
	spin_unlock_irq(&current->sighand->siglock);
	retval = -ERESTARTNOINTR;
	if (task_sigpending(current))
		goto fork_out;

	retval = -ENOMEM;
	p = dup_task_struct(current, node);
	if (!p)
		goto fork_out;
	p->flags &= ~PF_KTHREAD;
	if (args->kthread)
		p->flags |= PF_KTHREAD;
	if (args->user_worker) {
		/*
		 * Mark us a user worker, and block any signal that isn't
		 * fatal or STOP
		 */
		p->flags |= PF_USER_WORKER;
		siginitsetinv(&p->blocked, sigmask(SIGKILL)|sigmask(SIGSTOP));
	}
	if (args->io_thread)
		p->flags |= PF_IO_WORKER;

	if (args->name)
		strscpy_pad(p->comm, args->name, sizeof(p->comm));

	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? args->child_tid : NULL;
	/*
	 * Clear TID on mm_release()?
	 */
	p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? args->child_tid : NULL;

	ftrace_graph_init_task(p);

	rt_mutex_init_task(p);

	lockdep_assert_irqs_enabled();
#ifdef CONFIG_PROVE_LOCKING
	DEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);
#endif
	retval = copy_creds(p, clone_flags);
	if (retval < 0)
		goto bad_fork_free;

	retval = -EAGAIN;
	if (is_rlimit_overlimit(task_ucounts(p), UCOUNT_RLIMIT_NPROC, rlimit(RLIMIT_NPROC))) {
		if (p->real_cred->user != INIT_USER &&
		    !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))
			goto bad_fork_cleanup_count;
	}
	current->flags &= ~PF_NPROC_EXCEEDED;

	/*
	 * If multiple threads are within copy_process(), then this check
	 * triggers too late. This doesn't hurt, the check is only there
	 * to stop root fork bombs.
	 */
	retval = -EAGAIN;
	if (data_race(nr_threads >= max_threads))
		goto bad_fork_cleanup_count;

	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */
	p->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE | PF_NO_SETAFFINITY);
	p->flags |= PF_FORKNOEXEC;
	INIT_LIST_HEAD(&p->children);
	INIT_LIST_HEAD(&p->sibling);
	rcu_copy_process(p);
	p->vfork_done = NULL;
	spin_lock_init(&p->alloc_lock);

	init_sigpending(&p->pending);

	p->utime = p->stime = p->gtime = 0;
#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
	p->utimescaled = p->stimescaled = 0;
#endif
	prev_cputime_init(&p->prev_cputime);

#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
	seqcount_init(&p->vtime.seqcount);
	p->vtime.starttime = 0;
	p->vtime.state = VTIME_INACTIVE;
#endif

#ifdef CONFIG_IO_URING
	p->io_uring = NULL;
#endif

	p->default_timer_slack_ns = current->timer_slack_ns;

#ifdef CONFIG_PSI
	p->psi_flags = 0;
#endif

	task_io_accounting_init(&p->ioac);
	acct_clear_integrals(p);

	posix_cputimers_init(&p->posix_cputimers);
	tick_dep_init_task(p);

	p->io_context = NULL;
	audit_set_context(p, NULL);
	cgroup_fork(p);
	if (args->kthread) {
		if (!set_kthread_struct(p))
			goto bad_fork_cleanup_delayacct;
	}
#ifdef CONFIG_NUMA
	p->mempolicy = mpol_dup(p->mempolicy);
	if (IS_ERR(p->mempolicy)) {
		retval = PTR_ERR(p->mempolicy);
		p->mempolicy = NULL;
		goto bad_fork_cleanup_delayacct;
	}
#endif
#ifdef CONFIG_CPUSETS
	p->cpuset_mem_spread_rotor = NUMA_NO_NODE;
	seqcount_spinlock_init(&p->mems_allowed_seq, &p->alloc_lock);
#endif
#ifdef CONFIG_TRACE_IRQFLAGS
	memset(&p->irqtrace, 0, sizeof(p->irqtrace));
	p->irqtrace.hardirq_disable_ip	= _THIS_IP_;
	p->irqtrace.softirq_enable_ip	= _THIS_IP_;
	p->softirqs_enabled		= 1;
	p->softirq_context		= 0;
#endif

	p->pagefault_disabled = 0;

	lockdep_init_task(p);

	p->blocked_on = NULL; /* not blocked yet */

#ifdef CONFIG_BCACHE
	p->sequential_io	= 0;
	p->sequential_io_avg	= 0;
#endif
#ifdef CONFIG_BPF_SYSCALL
	RCU_INIT_POINTER(p->bpf_storage, NULL);
	p->bpf_ctx = NULL;
#endif

	unwind_task_init(p);

	/* Perform scheduler related setup. Assign this task to a CPU. */
	retval = sched_fork(clone_flags, p);
	if (retval)
		goto bad_fork_cleanup_policy;

	retval = perf_event_init_task(p, clone_flags);
	if (retval)
		goto bad_fork_sched_cancel_fork;
	retval = audit_alloc(p);
	if (retval)
		goto bad_fork_cleanup_perf;
	/* copy all the process information */
	shm_init_task(p);
	retval = security_task_alloc(p, clone_flags);
	if (retval)
		goto bad_fork_cleanup_audit;
	retval = copy_semundo(clone_flags, p);
	if (retval)
		goto bad_fork_cleanup_security;
	retval = copy_files(clone_flags, p, args->no_files);
	if (retval)
		goto bad_fork_cleanup_semundo;
	retval = copy_fs(clone_flags, p);
	if (retval)
		goto bad_fork_cleanup_files;
	retval = copy_sighand(clone_flags, p);
	if (retval)
		goto bad_fork_cleanup_fs;
	retval = copy_signal(clone_flags, p);
	if (retval)
		goto bad_fork_cleanup_sighand;
	retval = copy_mm(clone_flags, p);
	if (retval)
		goto bad_fork_cleanup_signal;
	retval = copy_namespaces(clone_flags, p);
	if (retval)
		goto bad_fork_cleanup_mm;
	retval = copy_io(clone_flags, p);
	if (retval)
		goto bad_fork_cleanup_namespaces;
	retval = copy_thread(p, args);
	if (retval)
		goto bad_fork_cleanup_io;

	stackleak_task_init(p);

	if (pid != &init_struct_pid) {
		pid = alloc_pid(p->nsproxy->pid_ns_for_children, args->set_tid,
				args->set_tid_size);
		if (IS_ERR(pid)) {
			retval = PTR_ERR(pid);
			goto bad_fork_cleanup_thread;
		}
	}

	/*
	 * This has to happen after we've potentially unshared the file
	 * descriptor table (so that the pidfd doesn't leak into the child
	 * if the fd table isn't shared).
	 */
	if (clone_flags & CLONE_PIDFD) {
		int flags = (clone_flags & CLONE_THREAD) ? PIDFD_THREAD : 0;

		/*
		 * Note that no task has been attached to @pid yet indicate
		 * that via CLONE_PIDFD.
		 */
		retval = pidfd_prepare(pid, flags | PIDFD_STALE, &pidfile);
		if (retval < 0)
			goto bad_fork_free_pid;
		pidfd = retval;

		retval = put_user(pidfd, args->pidfd);
		if (retval)
			goto bad_fork_put_pidfd;
	}

#ifdef CONFIG_BLOCK
	p->plug = NULL;
#endif
	futex_init_task(p);

	/*
	 * sigaltstack should be cleared when sharing the same VM
	 */
	if ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)
		sas_ss_reset(p);

	/*
	 * Syscall tracing and stepping should be turned off in the
	 * child regardless of CLONE_PTRACE.
	 */
	user_disable_single_step(p);
	clear_task_syscall_work(p, SYSCALL_TRACE);
#if defined(CONFIG_GENERIC_ENTRY) || defined(TIF_SYSCALL_EMU)
	clear_task_syscall_work(p, SYSCALL_EMU);
#endif
	clear_tsk_latency_tracing(p);

	/* ok, now we should be set up.. */
	p->pid = pid_nr(pid);
	if (clone_flags & CLONE_THREAD) {
		p->group_leader = current->group_leader;
		p->tgid = current->tgid;
	} else {
		p->group_leader = p;
		p->tgid = p->pid;
	}

	p->nr_dirtied = 0;
	p->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);
	p->dirty_paused_when = 0;

	p->pdeath_signal = 0;
	p->task_works = NULL;
	clear_posix_cputimers_work(p);

#ifdef CONFIG_KRETPROBES
	p->kretprobe_instances.first = NULL;
#endif
#ifdef CONFIG_RETHOOK
	p->rethooks.first = NULL;
#endif

	/*
	 * Ensure that the cgroup subsystem policies allow the new process to be
	 * forked. It should be noted that the new process's css_set can be changed
	 * between here and cgroup_post_fork() if an organisation operation is in
	 * progress.
	 */
	retval = cgroup_can_fork(p, args);
	if (retval)
		goto bad_fork_put_pidfd;

	/*
	 * Now that the cgroups are pinned, re-clone the parent cgroup and put
	 * the new task on the correct runqueue. All this *before* the task
	 * becomes visible.
	 *
	 * This isn't part of ->can_fork() because while the re-cloning is
	 * cgroup specific, it unconditionally needs to place the task on a
	 * runqueue.
	 */
	retval = sched_cgroup_fork(p, args);
	if (retval)
		goto bad_fork_cancel_cgroup;

	/*
	 * Allocate a default futex hash for the user process once the first
	 * thread spawns.
	 */
	if (need_futex_hash_allocate_default(clone_flags)) {
		retval = futex_hash_allocate_default();
		if (retval)
			goto bad_fork_cancel_cgroup;
		/*
		 * If we fail beyond this point we don't free the allocated
		 * futex hash map. We assume that another thread will be created
		 * and makes use of it. The hash map will be freed once the main
		 * thread terminates.
		 */
	}
	/*
	 * From this point on we must avoid any synchronous user-space
	 * communication until we take the tasklist-lock. In particular, we do
	 * not want user-space to be able to predict the process start-time by
	 * stalling fork(2) after we recorded the start_time but before it is
	 * visible to the system.
	 */

	p->start_time = ktime_get_ns();
	p->start_boottime = ktime_get_boottime_ns();

	/*
	 * Make it visible to the rest of the system, but dont wake it up yet.
	 * Need tasklist lock for parent etc handling!
	 */
	write_lock_irq(&tasklist_lock);

	/* CLONE_PARENT re-uses the old parent */
	if (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {
		p->real_parent = current->real_parent;
		p->parent_exec_id = current->parent_exec_id;
		if (clone_flags & CLONE_THREAD)
			p->exit_signal = -1;
		else
			p->exit_signal = current->group_leader->exit_signal;
	} else {
		p->real_parent = current;
		p->parent_exec_id = current->self_exec_id;
		p->exit_signal = args->exit_signal;
	}

	klp_copy_process(p);

	sched_core_fork(p);

	spin_lock(&current->sighand->siglock);

	rv_task_fork(p);

	rseq_fork(p, clone_flags);

	/* Don't start children in a dying pid namespace */
	if (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {
		retval = -ENOMEM;
		goto bad_fork_core_free;
	}

	/* Let kill terminate clone/fork in the middle */
	if (fatal_signal_pending(current)) {
		retval = -EINTR;
		goto bad_fork_core_free;
	}

	/* No more failure paths after this point. */

	/*
	 * Copy seccomp details explicitly here, in case they were changed
	 * before holding sighand lock.
	 */
	copy_seccomp(p);

	init_task_pid_links(p);
	if (likely(p->pid)) {
		ptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);

		init_task_pid(p, PIDTYPE_PID, pid);
		if (thread_group_leader(p)) {
			init_task_pid(p, PIDTYPE_TGID, pid);
			init_task_pid(p, PIDTYPE_PGID, task_pgrp(current));
			init_task_pid(p, PIDTYPE_SID, task_session(current));

			if (is_child_reaper(pid)) {
				ns_of_pid(pid)->child_reaper = p;
				p->signal->flags |= SIGNAL_UNKILLABLE;
			}
			p->signal->shared_pending.signal = delayed.signal;
			p->signal->tty = tty_kref_get(current->signal->tty);
			/*
			 * Inherit has_child_subreaper flag under the same
			 * tasklist_lock with adding child to the process tree
			 * for propagate_has_child_subreaper optimization.
			 */
			p->signal->has_child_subreaper = p->real_parent->signal->has_child_subreaper ||
							 p->real_parent->signal->is_child_subreaper;
			list_add_tail(&p->sibling, &p->real_parent->children);
			list_add_tail_rcu(&p->tasks, &init_task.tasks);
			attach_pid(p, PIDTYPE_TGID);
			attach_pid(p, PIDTYPE_PGID);
			attach_pid(p, PIDTYPE_SID);
			__this_cpu_inc(process_counts);
		} else {
			current->signal->nr_threads++;
			current->signal->quick_threads++;
			atomic_inc(&current->signal->live);
			refcount_inc(&current->signal->sigcnt);
			task_join_group_stop(p);
			list_add_tail_rcu(&p->thread_node,
					  &p->signal->thread_head);
		}
		attach_pid(p, PIDTYPE_PID);
		nr_threads++;
	}
	total_forks++;
	hlist_del_init(&delayed.node);
	spin_unlock(&current->sighand->siglock);
	syscall_tracepoint_update(p);
	write_unlock_irq(&tasklist_lock);

	if (pidfile)
		fd_install(pidfd, pidfile);

	proc_fork_connector(p);
	sched_post_fork(p);
	cgroup_post_fork(p, args);
	perf_event_fork(p);

	trace_task_newtask(p, clone_flags);
	uprobe_copy_process(p, clone_flags);
	user_events_fork(p, clone_flags);

	copy_oom_score_adj(clone_flags, p);

	return p;

bad_fork_core_free:
	sched_core_free(p);
	spin_unlock(&current->sighand->siglock);
	write_unlock_irq(&tasklist_lock);
bad_fork_cancel_cgroup:
	cgroup_cancel_fork(p, args);
bad_fork_put_pidfd:
	if (clone_flags & CLONE_PIDFD) {
		fput(pidfile);
		put_unused_fd(pidfd);
	}
bad_fork_free_pid:
	if (pid != &init_struct_pid)
		free_pid(pid);
bad_fork_cleanup_thread:
	exit_thread(p);
bad_fork_cleanup_io:
	if (p->io_context)
		exit_io_context(p);
bad_fork_cleanup_namespaces:
	exit_nsproxy_namespaces(p);
bad_fork_cleanup_mm:
	if (p->mm) {
		sched_mm_cid_exit(p);
		mm_clear_owner(p->mm, p);
		mmput(p->mm);
	}
bad_fork_cleanup_signal:
	if (!(clone_flags & CLONE_THREAD))
		free_signal_struct(p->signal);
bad_fork_cleanup_sighand:
	__cleanup_sighand(p->sighand);
bad_fork_cleanup_fs:
	exit_fs(p); /* blocking */
bad_fork_cleanup_files:
	exit_files(p); /* blocking */
bad_fork_cleanup_semundo:
	exit_sem(p);
bad_fork_cleanup_security:
	security_task_free(p);
bad_fork_cleanup_audit:
	audit_free(p);
bad_fork_cleanup_perf:
	perf_event_free_task(p);
bad_fork_sched_cancel_fork:
	sched_cancel_fork(p);
bad_fork_cleanup_policy:
	lockdep_free_task(p);
#ifdef CONFIG_NUMA
	mpol_put(p->mempolicy);
#endif
bad_fork_cleanup_delayacct:
	delayacct_tsk_free(p);
bad_fork_cleanup_count:
	dec_rlimit_ucounts(task_ucounts(p), UCOUNT_RLIMIT_NPROC, 1);
	exit_cred_namespaces(p);
	exit_creds(p);
bad_fork_free:
	WRITE_ONCE(p->__state, TASK_DEAD);
	exit_task_stack_account(p);
	put_task_stack(p);
	delayed_free_task(p);
fork_out:
	spin_lock_irq(&current->sighand->siglock);
	hlist_del_init(&delayed.node);
	spin_unlock_irq(&current->sighand->siglock);
	return ERR_PTR(retval);
}
----
====

=== Program Execution via exec()
The exec() family (e.g., execve()) is used to transform a process into a different program.

* Transformation: exec() replaces the current process image (code, data, heap, and stack) with a new binary, such as an ELF (Executable and Linkable Format) file.
* PID Retention: Unlike fork(), exec() does not create a new PID. The process remains the same entity but executes new code.
* Shell Example: When you run ls in a terminal:
1. The shell calls fork() to create a child.
2. The child calls execve("/bin/ls") to load the ls program.
3. The parent (shell) typically calls wait() until the child finishes.

== Process Termination and Cleanup
process will terminate, usually due to one of the following conditions:

. Normal exit (voluntary)
+
Most processes terminate because they have done their work. When a compiler
has compiled the program given to it, the compiler executes a system call to tell the
operating system that it is finished. This call is exit in UNIX and ExitProcess in
Windows. Screen-oriented programs also support voluntary termination. Word
processors, Internet browsers, and similar programs always have an icon or menu
item that the user can click to tell the process to remove any temporary files it has
open and then terminate
. Error exit (voluntary)
The third reason for termination is an error caused by the process, often due to
a program bug. Examples include executing an illegal instruction, referencing non-
existent memory, or dividing by zero. In some systems (e.g., UNIX), a process can
tell the operating system that it wishes to handle certain errors itself, in which case
the process is signaled (interrupted) instead of terminated when one of the errors
occurs.
. Fatal error (involuntary)
+
The second reason for termination is that the process discovers a fatal error.
For example, if a user types the command
cc foo.c
to compile the program foo.c and no such file exists, the compiler simply announ-
ces this fact and exits. Screen-oriented interactive processes generally do not exit
when given bad parameters. Instead they pop up a dialog box and ask the user to
try again.
. Killed by another process (involuntary)
+
The fourth reason a process might terminate is that the process executes a sys-
tem call telling the operating system to kill some other process. In UNIX this call
is kill.

* exit(): A process terminates by calling exit(), releasing most resources but leaving a return code.
* Reaping: The parent must call wait() to "reap" the child, removing its entry from the process table.
* Orphaned Processes: If a parent dies first, the child is adopted by init (PID 1), which handles the eventual reaping.

== Process ID and parent process ID
Each process has a unique integer process identifier (PID). Each process also has a
parent process identifier (PPID) attribute, which identifies the process that requested
the kernel to create this process.

== Process termination and termination status
A process can terminate in one of two ways: 

* by requesting its own termination
using the _exit() system call (or the related exit() library function)
* or by being killed by the delivery of a signal. 

In either case, the process yields a termination status, a
small nonnegative integer value that is available for inspection by the parent process using the wait() system call. In the case of a call to _exit(), the process explicitly
specifies its own termination status. If a process is killed by a signal, the termination
status is set according to the type of signal that caused the death of the process.

By convention, a termination status of 0 indicates that the process succeeded,
and a nonzero status indicates that some error occurred. Most shells make the termination status of the last executed program available via a shell variable named $?.

== Implementation of Processes
To implement the process model, the operating system maintains a table (an
array of structures), called the process table, with one entry per process. (Some books call each row a Process Control Block (PCB).) This entry contains important
information about the process‚Äô state, including its program counter, stack pointer,
memory allocation, the status of its open files, its accounting and scheduling infor-
mation, and everything else about the process that must be saved when the process
is switched from running to ready or blocked state so that it can be restarted later
as if it had never been stopped.

fields of a typical process-table entry

//TODO convert to table add fields like field name and type in kernel code
*Process management* 

Registers 
Program counter 
Program status word 
Stack pointer 
Process state 
Priority
Scheduling parameters
Process ID
Parent process
Process group
Signals
Time when process started
CPU time used
Children‚Äôs CPU time
Time of next alarm

*Memory management*

Pointer to text segment info
Pointer to data segment info 
Pointer to stack segment info 

*File management*
Root directory
Working directory
File descriptors
User ID
Group ID

.From AI
[%collapsible]
======
[tabs]
=====
ChatGPT::
+
[tabs]
====
Concept::
1Ô∏è‚É£ What is the process model?
+
A process is a program while it is running.
+
The executable file on disk ‚Üí program
+
The same program loaded into memory and executing ‚Üí process
+
üëâ Example:
+
/bin/bash is a program
+
When you run it, the OS creates a process for it
+
The process model is how the OS:
+
Creates processes
+
Runs them
+
Pauses them
+
Switches between them
+
Destroys them
+
2Ô∏è‚É£ What is the process table?
+
To manage many processes at once, the OS keeps a big table in memory called the process table.
+
Think of it like an Excel sheet
+
One row = one process
+
Each row stores everything the OS needs to know about that process
+
Some books call each row a Process Control Block (PCB).
+
3Ô∏è‚É£ Why does the OS need this table?
+
Because the CPU can run only one process at a time, but the system has many processes.
+
The OS must be able to:
+
Stop one process
+
Remember exactly where it was
+
Resume it later as if nothing happened
+
To do that, the OS saves the process‚Äôs state in its process table entry.
+
4Ô∏è‚É£ What kind of information is stored? (Key fields explained)
üîπ 1. Program Counter (PC)
+
What it is:
The address of the next instruction the process should execute.
+
Why it matters:
When a process is paused, the OS must know where to continue.
+
Example:
If a process was executing line 120 of a program, the PC remembers:
+
‚ÄúResume from instruction at address 0x80483af‚Äù
+
üîπ 2. Stack Pointer (SP)
+
What it is:
Points to the top of the process‚Äôs stack.
+
Why it matters:
The stack holds:
+
Function calls
+
Local variables
+
Return addresses
+
Example:
If the process is inside foo() called by main(), the stack pointer ensures it returns correctly when resumed.
+
üîπ 3. Process State
+
What it is:
Tells what the process is currently doing.
+
Common states:
+
Running ‚Äì currently using the CPU
+
Ready ‚Äì waiting for CPU
+
Blocked ‚Äì waiting for something (I/O, file, network)
+
Example:
A process waiting for disk data ‚Üí Blocked
+
üîπ 4. Memory Management Information
+
What it includes:
+
Where the process‚Äôs memory starts and ends
+
Page tables or segment info
+
Why it matters:
Each process must be isolated from others.
+
Example:
Process A cannot read Process B‚Äôs memory because the OS uses this info to enforce protection.
+
üîπ 5. Open Files Information
+
What it includes:
+
Which files the process has opened
+
File descriptors
+
Current file offsets
+
Example:
If a process opened:
+
fd = open("data.txt", O_RDONLY);
+
The process table keeps:
+
That the file is open
+
Where the next read() should continue from
+
So after a context switch, read() continues correctly.
+
üîπ 6. Scheduling Information
+
What it includes:
+
Priority
+
Time slice used
+
CPU usage statistics
+
Why it matters:
The scheduler uses this to decide:
+
‚ÄúWho runs next?‚Äù
+
Example:
A real-time process may get higher priority than a background one.
+
üîπ 7. Accounting Information
+
What it includes:
+
How much CPU time the process has used
+
User ID (who owns the process)
+
Start time
+
Why it matters:
Used for:
+
Billing
+
Resource limits
+
Monitoring tools like top
+
5Ô∏è‚É£ Context Switching (the core idea)
+
When the OS switches from one process to another:
+
Save all important CPU state into Process A‚Äôs table entry
+
Load Process B‚Äôs state from its table entry
+
Resume Process B
+
This makes it look like:
+
‚ÄúEach process runs continuously‚Äù
+
Even though they‚Äôre actually taking turns.
+
6Ô∏è‚É£ Why the book says ‚Äúsystem dependent‚Äù
+
Different OSes store different details.
+
Linux has task_struct
+
Windows has EPROCESS
+
Minix has its own PCB format
+
But the idea is always the same:
+
Store everything needed to pause and resume a process safely.
+
7Ô∏è‚É£ Big picture summary (in one paragraph)
+
The operating system keeps a process table, with one entry per process, to remember everything about each running program. This information lets the OS stop a process, run another one, and later restart the first one exactly where it left off. Each entry includes CPU state (like program counter and stack pointer), memory info, open files, scheduling data, and accounting details. The exact contents vary by OS, but the goal is always the same: make multitasking possible and safe.
+
map to Linux‚Äôs struct task_struct::
1Ô∏è‚É£ Big picture first
+
In Linux:
+
There is no single ‚Äúprocess table array‚Äù
+
Instead, each process is represented by one struct task_struct
+
All task_structs are linked together (lists, trees, hash tables)
+
üìå task_struct is Linux‚Äôs Process Control Block (PCB)
+
Location (conceptually):
+
include/linux/sched.h
+
2Ô∏è‚É£ Mapping the textbook fields ‚Üí Linux task_struct
+
I‚Äôll follow the same categories Tanenbaum uses.
+
üü¶ Process Management Fields
üìç Program Counter (PC)
+
Textbook:
+
program counter
+
Linux:
Stored in the CPU register state, saved during context switch.
+
Key structures:
+
struct task_struct {
    struct thread_struct thread;
}
+
Inside thread_struct (architecture-specific):
+
Saved instruction pointer (IP / RIP / PC)
+
Saved stack pointer (SP)
+
üí° Linux does not store PC directly in task_struct; it‚Äôs saved in thread_struct or on the kernel stack.
+
üìç Stack Pointer (SP)
+
Textbook:
+
stack pointer
+
Linux:
Two stacks:
+
User stack ‚Üí managed via memory structures
+
Kernel stack ‚Üí referenced directly
+
task_struct->stack   // pointer to kernel stack
+
The exact SP value is saved during a context switch.
+
üìç Process State
+
Textbook:
+
running, ready, blocked
+
Linux:
+
task_struct->state
+
Examples:
+
TASK_RUNNING
TASK_INTERRUPTIBLE
TASK_UNINTERRUPTIBLE
TASK_STOPPED
TASK_ZOMBIE
+
üìå Linux splits ‚Äúblocked‚Äù into interruptible and uninterruptible sleep.
+
üìç Process ID (PID)
+
Textbook:
+
process identifier
+
Linux:
+
task_struct->pid
task_struct->tgid   // thread group ID (process ID)
+
pid ‚Üí unique per thread
+
tgid ‚Üí shared by all threads in a process
+
üìç Parent / Child Relationship
+
Linux:
+
task_struct->parent
task_struct->children
task_struct->sibling
+
This builds the process tree you see with pstree.
+
üü© Scheduling Information
+
Textbook:
+
scheduling information
+
Linux:
+
task_struct->prio
task_struct->static_prio
task_struct->normal_prio
task_struct->policy
task_struct->se
+
Key ideas:
+
Priority values
+
Scheduling policy (SCHED_NORMAL, SCHED_FIFO, etc.)
+
Virtual runtime (CFS scheduler)
+
üìå The scheduler uses these fields to decide who runs next.
+
üü® Memory Management Information
+
Textbook:
+
memory allocation
+
Linux:
+
task_struct->mm
task_struct->active_mm
+
Points to:
+
struct mm_struct
+
Which contains:
+
Page tables
+
Virtual memory areas (VMAs)
+
Code, heap, stack regions
+
üìå Kernel threads have mm == NULL.
+
üü• File Management Information
+
Textbook:
+
status of its open files
+
Linux:
+
task_struct->files
+
Points to:
+
struct files_struct
+
Which contains:
+
File descriptor table
+
Open file objects (struct file)
+
File offsets and flags
+
üìå This is why fork() shares files and dup() works.
+
üü™ Accounting Information
+
Textbook:
+
accounting information
+
Linux:
+
task_struct->utime   // user CPU time
task_struct->stime   // kernel CPU time
task_struct->start_time
task_struct->cred    // credentials (UID, GID)
+
Used by:
+
top
+
ps
+
Resource limits
+
üüß Signal Handling
+
(Not explicitly mentioned in Tanenbaum‚Äôs figure, but crucial in Linux)
+
task_struct->signal
task_struct->sighand
task_struct->pending
+
Controls:
+
Signal delivery
+
Signal masks
+
Default actions
+
üü´ Context Switching Support
+
Textbook:
+
everything that must be saved and restored
+
Linux:
Saved across:
+
task_struct
+
thread_struct
+
Kernel stack
+
Key function:
+
context_switch(prev, next)
+
Steps:
+
Save CPU registers into prev
+
Load registers from next
+
Switch stacks
+
Resume execution
+
3Ô∏è‚É£ One-to-one summary table
Textbook PCB Field	Linux task_struct
Program counter	thread_struct / kernel stack
Stack pointer	task_struct->stack
Process state	task_struct->state
PID	pid, tgid
Scheduling info	prio, policy, se
Memory info	mm, mm_struct
Open files	files, files_struct
Accounting	utime, stime, start_time
4Ô∏è‚É£ Why Linux looks ‚Äúmore complex‚Äù than the book
+
Tanenbaum shows a clean conceptual model.
+
Linux:
+
Is highly optimized
+
Supports threads, NUMA, cgroups, containers
+
Splits PCB data across multiple structs
+
üìå But conceptually, it‚Äôs the same PCB idea‚Äîjust industrial-strength.
+
5Ô∏è‚É£ Mental model to keep
+
task_struct = the OS‚Äôs memory of a process
+
If it‚Äôs in task_struct, Linux can:
+
Stop the process
+
Resume it
+
Schedule it
+
Account for it
+
Kill it

====

Gemini::
[tabs]
====
Concept:: Concept

Concept:: Concept
====
=====
======


A process may be interrupted thousands of times during its execution, but the
key idea is that after each interrupt the interrupted process returns to precisely the
same state it was in before the interrupt occurred.

Associated with each I/O class is a location (typically at a fixed location
near the bottom of memory) called the interrupt vector. It contains the address of
the ISR (Interrupt Service Routine). Suppose that user process 3 is running
when a disk interrupt happens. User process 3‚Äôs program counter, program status
word, and sometimes one or more registers are pushed onto the (current) stack by
the interrupt hardware. The computer then jumps to the address in the interrupt
vector. That is all the hardware does. From here on, it is up to the ISR in software.

All interrupts start by saving the registers, often in the process table entry for
the current process. Then the information pushed onto the stack by the interrupt is
removed and the stack pointer is set to point to a temporary stack used by the proc-
ess handler. Actions such as saving the registers and setting the stack pointer can-
not even be expressed in high-level languages such as C, so they are performed by
a small assembly-language routine, usually the same one for all interrupts since the
work of saving the registers is identical, no matter the cause of the interrupt.

When this routine is finished, it calls a C procedure to do the rest of the work
for this specific interrupt type. (We assume the operating system is written in C,
the usual choice for all operating systems used in production.) When it has done
its job, possibly making some process now ready, the scheduler is called to see
what to run next. After that, control is passed back to the assembly-language code
to load up the registers and memory map for the now-current process and start it
running.

. Hardware stacks program counter, etc.
. Hardware loads new program counter from interrupt vector.
. Assembly-language procedure saves registers.
. Assembly-language procedure sets up new stack.
. C interrupt service runs (typically reads and buffers input).
. Scheduler decides which process is to run next.
. C procedure returns to the assembly code.
. Assembly-language procedure starts up new current process.

.From AI
[%collapsible]
======
[tabs]
=====
ChatGPT::
+
[tabs]
====
Concept:: 
Great question ‚Äî this paragraph is dense, but it‚Äôs describing the exact machinery that makes multitasking work. I‚Äôll explain it in plain language, step by step, with examples and a mental picture.
+
Think of this as:
+
What really happens inside the OS when an interrupt occurs and the CPU might switch processes.
+
1. Why this discussion exists at all
Key idea
+
The OS wants to create the illusion that:
+
Many programs are running one after another
+
Even though the CPU can run only one instruction at a time
+
This illusion is maintained by:
+
Interrupts
+
Saving process state
+
Scheduling another process
+
2. Interrupt vector: ‚ÄúWhere should I jump?‚Äù
What the book says
+
‚ÄúAssociated with each I/O class is a location ‚Ä¶ called the interrupt vector.‚Äù
+
Simple meaning
+
An interrupt vector is a table in memory that says:
+
Device	Address to jump to
Disk	Disk ISR address
Timer	Timer ISR address
Keyboard	Keyboard ISR address
+
üìå Think of it like:
+
‚ÄúIf this device interrupts, start executing that function.‚Äù
+
3. An interrupt happens while a process is running
Example
+
Process 3 is running in user mode
+
It‚Äôs doing calculations
+
Suddenly, the disk finishes reading data
+
The disk sends an interrupt to the CPU.
+
4. What the hardware does (very little!)
The book says
+
‚ÄúProgram counter, program status word, and sometimes registers are pushed onto the stack by the interrupt hardware.‚Äù
+
In simple words
+
The CPU hardware automatically:
+
Pauses process 3
+
Saves:
+
Program Counter (PC) ‚Üí where execution stopped
+
Program Status Word (PSW) ‚Üí mode, flags
+
Some registers
+
Pushes this info onto the current stack
+
Jumps to the ISR address from the interrupt vector
+
‚ö†Ô∏è Important
+
The hardware does nothing else.
+
No scheduling.
No waking processes.
No decisions.
+
That‚Äôs why the book says:
+
‚ÄúThat is all the hardware does.‚Äù
+
5. Now software (the OS) takes over
+
From this point on:
+
Everything is OS code.
+
This code is the ISR (Interrupt Service Routine).
+
6. First job of every interrupt: save registers
Why?
+
Because registers contain temporary values the process was using.
+
If we don‚Äôt save them:
+
The process would be corrupted when it resumes.
+
Where are they saved?
+
‚ÄúOften in the process table entry for the current process.‚Äù
+
So process 3‚Äôs entry now contains:
+
PC
+
PSW
+
Registers
+
Other CPU state
+
7. Stack switching: why change stacks?
What the book says
+
‚ÄúThe information pushed onto the stack ‚Ä¶ is removed and the stack pointer is set to a temporary stack used by the process handler.‚Äù
+
Simple explanation
+
The interrupt started on the user process‚Äôs stack, but the kernel:
+
Does NOT want to run on a user stack
+
Needs a safe kernel stack
+
So the OS:
+
Copies the saved info elsewhere
+
Switches to a kernel (temporary) stack
+
üìå Think of it as:
+
‚ÄúWe‚Äôre entering the kernel now ‚Äî use kernel tools, not user tools.‚Äù
+
8. Why assembly language is needed
The book says
+
‚ÄúActions such as saving the registers and setting the stack pointer cannot even be expressed in C.‚Äù
+
Why?
+
C cannot:
+
Directly control CPU registers
+
Change stack pointers
+
Control return-from-interrupt instructions
+
So:
+
A small assembly routine runs first
+
It does the low-level CPU work
+
This routine is:
+
Generic
+
Used for all interrupts
+
Same steps every time
+
9. C code handles the specific interrupt
+
Once the assembly setup is done:
+
‚ÄúIt calls a C procedure to do the rest of the work for this specific interrupt type.‚Äù
+
Example
+
Disk interrupt ‚Üí disk ISR in C
+
Timer interrupt ‚Üí scheduler-related ISR
+
Keyboard interrupt ‚Üí input handling ISR
+
This C code can:
+
Mark I/O as complete
+
Wake sleeping processes
+
Update kernel data structures
+
10. Scheduler: ‚ÄúWho runs next?‚Äù
What the book says
+
‚ÄúPossibly making some process now ready, the scheduler is called to see what to run next.‚Äù
+
Example
+
Process 5 was waiting for disk I/O
+
Disk interrupt completes
+
Process 5 becomes READY
+
Scheduler now decides:
+
Continue process 3?
+
Or switch to process 5?
+
11. Restoring a process and running it
+
Once a decision is made:
+
Control returns to assembly code
+
It:
+
Loads registers
+
Loads memory map (page tables)
+
Restores stack pointer
+
Executes a return-from-interrupt
+
The CPU now runs:
+
Either process 3 again
+
Or a different process
+
From the process‚Äôs perspective:
+
‚ÄúI just continued execution normally.‚Äù
+
12. Big picture timeline (simple)
Process 3 running
        ‚Üì
Disk interrupt
        ‚Üì
Hardware saves PC/PSW ‚Üí jumps to ISR
        ‚Üì
Assembly: save registers, switch stack
        ‚Üì
C ISR: handle disk, wake process 5
        ‚Üì
Scheduler chooses next process
        ‚Üì
Assembly restores chosen process
        ‚Üì
Process runs
+
One-sentence summary
+
When an interrupt occurs, the hardware briefly saves the running process‚Äôs state and jumps into the kernel, where assembly code preserves CPU state, C code handles the specific event, the scheduler may choose another process, and finally the CPU resumes execution‚Äîcreating the illusion that many programs run at once.

context-switch diagram::
+
----
USER MODE
=========
Process A running
PC = 0x8040123
Registers = A-state
Stack = A user stack
        |
        |  (timer / disk / I/O interrupt)
        v
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
HARDWARE (CPU)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ Save PC, PSW, (some regs) on current stack
‚Ä¢ Switch to kernel mode
‚Ä¢ Jump to interrupt vector
        |
        v
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
KERNEL MODE ‚Äì Assembly Stub
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ Save remaining CPU registers
‚Ä¢ Store them in Process A's process table entry
‚Ä¢ Switch stack pointer ‚Üí kernel stack
        |
        v
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
KERNEL MODE ‚Äì C ISR
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ Identify interrupt source
‚Ä¢ Acknowledge device
‚Ä¢ Wake blocked processes (if any)
‚Ä¢ Possibly mark Process B READY
        |
        v
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
SCHEDULER
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ Decide next process to run
‚Ä¢ Example decision:
    Process A ‚Üí READY
    Process B ‚Üí RUNNING
        |
        v
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
KERNEL MODE ‚Äì Restore Context
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ Load Process B page tables
‚Ä¢ Load Process B registers
‚Ä¢ Load Process B kernel stack
        |
        v
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
RETURN FROM INTERRUPT
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ Restore PC, PSW
‚Ä¢ Switch to user mode
        |
        v
USER MODE
=========
Process B running
PC = where it left off
Registers = B-state
Stack = B user stack
----
*Mental Model (Very Important)*
+
Think of a context switch as three phases:
+
1Ô∏è‚É£ Freeze the old process
+
‚ÄúPause Process A exactly as it is.‚Äù
+
Save registers
+
Save PC
+
Save memory map
+
2Ô∏è‚É£ Decide who runs next
+
‚ÄúWho deserves the CPU now?‚Äù
+
Scheduler logic
+
Priority, time slice, I/O completion
+
3Ô∏è‚É£ Resume the new process
+
‚ÄúUnpause Process B exactly where it stopped.‚Äù
+
Restore registers
+
Restore PC
+
Restore address space
+
Why this creates the multitasking illusion
+
Each process sees:
+
run ‚Üí pause ‚Üí resume
+
But never sees:
+
Interrupts
+
Other processes
+
Kernel code
+
So it feels like:
+
‚ÄúI ran continuously.‚Äù
+
Tiny real-world analogy
+
üìò Bookmark analogy:
+
Process A: bookmark placed, book closed
+
Process B: bookmark opened, reading resumes
+
CPU: only reading one book at a time

Full Mental Mode::
Context switch mental model showing CPU state, RAM, stacks, and process table.
Read top to bottom as time progresses.
+
Before any interrupt, a process is running normally in user mode.
+
----
CPU
--------------------------------
PC   -> 0x4012A3        (next instruction)
SP   -> 0x7ffdf000      (user stack pointer)
MODE -> USER
REGS -> r1 r2 r3 ...
+
RAM
--------------------------------
Process Table Entry (Process A)
- saved PC     (old)
- saved SP
- saved PSW
- saved registers
+
User Stack (Process A)
0x7ffdf000 -> local variables
              return address
              function arguments
----
+
At this point, the CPU is executing instructions from Process A and all live execution
state resides in CPU registers.
+
An interrupt arrives (timer, disk, or I/O). The CPU hardware reacts automatically.
+
----
CPU (hardware actions)
--------------------------------
- Push PC onto current stack
- Push PSW onto current stack
- Possibly push some registers
- Switch MODE -> KERNEL
- Jump to interrupt vector
----
+
----
User Stack (Process A)
--------------------------------
0x7ffdf000 -> PC        (pushed by hardware)
              PSW
              local variables
----
+
This is all the hardware does. No scheduling or OS logic occurs yet.
+
The kernel interrupt entry code written in assembly executes next.
+
----
CPU (assembly ISR entry)
--------------------------------
- Save remaining registers
- Copy PC, PSW, SP, registers into process table
- Switch stack pointer -> kernel stack
----
+
----
RAM
--------------------------------
Process Table Entry (Process A)
- saved PC
- saved PSW
- saved SP
- saved registers
+
Kernel Stack (Process A)
0xffff8000 -> interrupt frame
              kernel local variables
----
+
The kernel is now running safely on a kernel stack, not the user stack.
+
The C interrupt service routine performs device-specific work.
+
----
CPU
--------------------------------
MODE -> KERNEL
SP   -> kernel stack
PC   -> interrupt_handler()
----
+
----
RAM
--------------------------------
- Device status updated
- I/O completion recorded
- Waiting processes possibly marked READY
----
+
The scheduler is invoked to choose the next process.
+
----
Scheduler decision
--------------------------------
Current process: Process A
Candidate process: Process B
Decision: Switch to Process B
----
+
The kernel restores the chosen process context.
+
----
CPU (assembly restore)
--------------------------------
- Load Process B page tables
- Load Process B kernel stack
- Restore Process B registers
- Restore Process B PC and PSW
----
+
----
RAM
--------------------------------
Process Table Entry (Process B)
- saved PC
- saved PSW
- saved SP
- saved registers
+
User Stack (Process B)
0x7ffde000 -> local variables
              return address
----
+
A return-from-interrupt instruction completes the switch.
+
----
CPU
--------------------------------
MODE -> USER
PC   -> Process B saved PC
SP   -> Process B user stack
REGS -> Process B registers
----
+
Process B resumes execution exactly where it previously stopped.
+
Summary of saved state and locations:
+
* Program Counter (PC)
** Saved automatically by hardware
** Temporarily pushed onto user stack
** Copied into process table
+
* Program Status Word (PSW)
** Saved automatically by hardware
** Temporarily pushed onto user stack
** Copied into process table
+
* General-purpose registers
** Saved by assembly interrupt entry
** Stored in process table
+
* User stack pointer
** Saved by assembly code
** Stored in process table
+
* Kernel stack pointer
** Managed by the operating system
** Stored in kernel task structure
+
* Memory map / address space
** Managed by the OS and MMU
** Represented by page tables
+
Key rule:
+
A context switch freezes one process‚Äôs CPU state into RAM and restores another
process‚Äôs CPU state from RAM back into the CPU.

direct Linux mapping + architecture-specific stack frames::
Mapping of saved execution state to Linux kernel data structures.
+
Saved Program Counter (PC / RIP)
+
* Architecture meaning
** x86-64: RIP
** ARM64: PC
* Where Linux saves it
** struct task_struct
*** task_struct.thread.ip        (older kernels / arch-dependent)
*** task_struct.thread.cpu_context.pc   (ARM64)
** Also temporarily saved on kernel stack during interrupt entry
* Final authoritative copy
** Stored in task_struct.thread or pt_regs snapshot
+
Saved Program Status Word (PSW / RFLAGS / PSTATE)
+
* Architecture meaning
** x86-64: RFLAGS
** ARM64: PSTATE
* Where Linux saves it
** struct pt_regs
*** x86-64: pt_regs->flags
*** ARM64: pt_regs->pstate
* pt_regs is stored on
** Kernel stack of the interrupted task
+
Saved general-purpose registers
+
* Architecture meaning
** x86-64: RAX, RBX, RCX, RDX, RSI, RDI, RBP, R8‚ÄìR15
** ARM64: X0‚ÄìX30
* Where Linux saves them
** struct pt_regs (interrupt/system call entry)
** struct thread_struct / cpu_context (during context switch)
* Structures
** x86-64:
*** task_struct.thread.sp
*** task_struct.thread.fsbase
*** task_struct.thread.gsbase
** ARM64:
*** task_struct.thread.cpu_context.x19‚Äìx28
*** task_struct.thread.cpu_context.fp
*** task_struct.thread.cpu_context.lr
+
Saved user stack pointer
+
* Architecture meaning
** x86-64: RSP
** ARM64: SP_EL0
* Where Linux saves it
** struct pt_regs
*** x86-64: pt_regs->sp
*** ARM64: pt_regs->sp
* Persistent copy
** task_struct.thread.sp (kernel stack pointer)
** User SP restored from pt_regs on return to user mode
+
Saved kernel stack pointer
+
* Meaning
** Stack used while executing kernel code for this task
* Where Linux stores it
** task_struct.stack
** task_struct.thread.sp
* Each task has
** Exactly one kernel stack (THREAD_SIZE)
+
Saved memory map (address space)
+
* Meaning
** Virtual memory layout of the process
* Where Linux stores it
** task_struct.mm        (user processes)
** task_struct.active_mm (kernel threads)
* Used by
** Context switch to load CR3 (x86-64)
** TTBR0_EL1 (ARM64)
+
Saved scheduling state
+
* Meaning
** RUNNING, READY, BLOCKED
* Where Linux stores it
** task_struct.state
** task_struct.on_rq
** task_struct.prio
+
Temporary interrupt frame
+
* Meaning
** Snapshot created at interrupt entry
* Where Linux stores it
** On kernel stack
** Represented by struct pt_regs
* Lifetime
** Exists only during kernel execution
** Used to restore user mode state
+
------------------------------------------------------------------
+
x86-64 interrupt and context-switch stack layout.
+
Kernel stack (top grows downward):
+
----
|                |
|  kernel data   |
|----------------|
| struct pt_regs |
|----------------|
| RIP            | <- pushed by CPU
| CS             |
| RFLAGS         |
| RSP            |
| SS             |
|----------------|
| user stack     |
----
+
Interrupt entry sequence (x86-64):
+
* CPU pushes RIP, CS, RFLAGS, RSP, SS
* Entry assembly saves remaining registers into pt_regs
* pt_regs lives on kernel stack
* task_struct.thread.sp updated
* C interrupt handler runs
* __schedule() may call context_switch()
* __switch_to() swaps thread_struct fields
* iretq restores pt_regs and returns to user mode
+
------------------------------------------------------------------
+
ARM64 interrupt and context-switch stack layout.
+
Kernel stack (top grows downward):
+
----
|                |
|  kernel data   |
|----------------|
| struct pt_regs |
|----------------|
| X0‚ÄìX30         |
| SP_EL0         |
| ELR_EL1 (PC)   |
| SPSR_EL1       |
|----------------|
| user stack     |
----
+
Interrupt entry sequence (ARM64):
+
* Hardware switches to EL1
* PC saved into ELR_EL1
* PSTATE saved into SPSR_EL1
* Entry assembly builds pt_regs
* pt_regs stored on kernel stack
* thread.cpu_context used for non-volatile registers
* __switch_to() swaps cpu_context
* eret restores EL0 execution
+
------------------------------------------------------------------
+
Linux context switch path (simplified):
+
* Interrupt or syscall enters kernel
* pt_regs created on kernel stack
* Scheduler invoked
* context_switch()
** switch_mm()
** switch_to(prev, next)
*** __switch_to(prev, next)
**** Save prev.thread.*
**** Load next.thread.*
* Return path restores pt_regs
* Execution resumes in user mode
+
------------------------------------------------------------------
+
Key invariant:
+
CPU registers are never the source of truth.
The source of truth is always task_struct + kernel stack.
+
The CPU is just a cache for task state.

====
=====

Gemini::
[tabs]
=====
ChatGPT::
+
[tabs]
====
Concept:: Concept

Concept:: Concept
====

=====
======

* https://github.com/torvalds/linux/blob/master/include/linux/sched.h#L819
* https://github.com/torvalds/linux/blob/master/kernel/sched/core.c#L5198

.Code
[%collapsible]
======
[tabs]
=====
context_switch::
0Ô∏è‚É£ Where context_switch() sits in the kernel
+
High-level call chain (simplified):
+
schedule()
 ‚îî‚îÄ‚îÄ pick_next_task()
 ‚îî‚îÄ‚îÄ context_switch(prev, next)
+
üìå context_switch() does not decide WHO runs
üìå It only switches FROM prev TO next
+
1Ô∏è‚É£ Function signature (conceptual)
context_switch(prev, next)
+
Where:
+
prev ‚Üí currently running task_struct
+
next ‚Üí task chosen by scheduler
+
Both are fully valid processes.
+
2Ô∏è‚É£ First: memory context switch (mm_struct)
switch_mm(prev->mm, next->mm, next);
+
What this does
+
Switches virtual address space
+
Updates page tables
+
Handles TLB implications
+
Why it matters
+
Each process has its own virtual memory.
+
Process A: 0x400000 ‚Üí libc
Process B: 0x400000 ‚Üí totally different data
+
üìå If prev->mm == next->mm
‚Üí no memory switch (threads!)
+
Special case: kernel threads
if (next->mm == NULL)
    use active_mm
+
Kernel threads:
+
Don‚Äôt have user memory
+
Borrow previous mm
+
3Ô∏è‚É£ Save CPU state of prev
+
This is mostly architecture-specific, but conceptually:
+
save_registers(prev->thread);
+
What gets saved
+
Instruction pointer (PC / RIP)
+
Stack pointer (SP)
+
Callee-saved registers
+
CPU flags
+
üìå This is usually done by assembly, not C.
+
Where it‚Äôs saved
prev->thread_struct
prev->kernel stack
+
4Ô∏è‚É£ Switch kernel stack
switch_stack(prev, next);
+
Why kernel stacks matter
+
Each process has its own kernel stack:
+
Process A kernel stack
Process B kernel stack
+
When switching process:
+
Kernel must switch stacks
+
Otherwise return addresses break
+
üìå This is critical and non-negotiable
+
5Ô∏è‚É£ Update current pointer
current = next;
+
Linux uses a per-CPU variable:
+
current ‚Üí task_struct of running process
+
After this line:
+
The kernel believes next is running
+
6Ô∏è‚É£ Load CPU state of next
load_registers(next->thread);
+
This restores:
+
PC
+
SP
+
Registers
+
Flags
+
üìå After this step, the CPU is ready to:
+
resume next exactly where it stopped
+
7Ô∏è‚É£ Architecture hook: switch_to()
+
In real Linux, most of this is wrapped in:
+
switch_to(prev, next, last);
+
This macro:
+
Is architecture-specific
+
Written mostly in assembly
+
Saves prev
+
Restores next
+
Returns as last
+
Mental trick:
+
switch_to() returns twice
+
Once for:
+
Old process (before switch)
+
New process (after switch)
+
8Ô∏è‚É£ After the switch: return path
+
Here‚Äôs the mind-bending part ü§Ø
+
When next resumes:
+
It returns from switch_to()
+
As if it was never stopped
+
From next‚Äôs perspective:
+
‚ÄúI called schedule(), and now I‚Äôm back.‚Äù
+
9Ô∏è‚É£ Full flow in compact pseudo-code
context_switch(prev, next)
{
    switch_mm(prev->mm, next->mm);
    save_cpu_state(prev);
    switch_kernel_stack(prev, next);
    current = next;
    load_cpu_state(next);
}
+
üîü What is NOT done here (important!)
+
‚ùå No memory copying
‚ùå No file descriptor copying
‚ùå No process creation
‚ùå No scheduling decisions
+
Those happen elsewhere.
+
1Ô∏è‚É£1Ô∏è‚É£ Timeline view (very important)
Time ‚Üí
Process A running
  ‚Üì
timer interrupt
  ‚Üì
schedule()
  ‚Üì
context_switch(A, B)
  ‚Üì
Process B resumes execution
+
CPU illusion:
+
[A][B][A][C][A][B]
+
1Ô∏è‚É£2Ô∏è‚É£ Cost of a context switch
+
A context switch costs:
+
Register save/restore
+
Possible TLB flush
+
Cache effects
+
That‚Äôs why:
+
Threads are cheaper than processes
+
Excessive switching kills performance
+
1Ô∏è‚É£3Ô∏è‚É£ One-sentence mental model (golden)
+
context_switch() freezes one CPU reality and replaces it with another.


=====
======

== Process Groups and Shell Job Control
In some systems, when a process creates another process, the parent process
and child process continue to be associated in certain ways. The child process can
itself create more processes, forming a process hierarchy. Note that unlike plants
and animals that use sexual reproduction, a process has only one parent (but zero,
one, two, or more children). So a process is more like a hydra than like, say, a cow.

In UNIX, a process and all of its children and further descendants together
form a process group. When a user sends a signal from the keyboard (e.g., by
pressing CTRL-C), the signal is delivered to all members of the process group cur-
rently associated with the keyboard (usually all active processes that were created
in the current window). Individually, each process can catch the signal, ignore the
signal, or take the default action, which is to be killed by the signal.

As another example of where the process hierarchy plays a key role, let us look
at how UNIX initializes itself when it is started, just after the computer is booted.
A special process, called init, is present in the boot image. When it starts running,
it reads a file telling how many terminals there are. Then it forks off a new process
per terminal. These processes wait for someone to log in. If a login is successful,
the login process executes a shell to accept commands. These commands may start
up more processes, and so forth. Thus, all the processes in the whole system
belong to a single tree, with init at the root.

Processes in UNIX cannot disinherit their children

Each program executed by the shell is started in a new process. For example, the
shell creates three processes to execute the following pipeline of commands (which
displays a list of files in the current working directory sorted by file size):

$ ls -l | sort -k5n | less

All major shells, except the Bourne shell, provide an interactive feature called job
control, which allows the user to simultaneously execute and manipulate multiple
commands or pipelines. 

In job-control shells, all of the processes in a pipeline are
placed in a new process group or job. (In the simple case of a shell command line con-
taining a single command, a new process group containing just a single process is
created.) 

Each process in a process group has the same integer process group
identifier, which is the same as the process ID of one of the processes in the group,
termed the process group leader.

The kernel allows for various actions, notably the delivery of signals, to be per-
formed on all members of a process group. Job-control shells use this feature to
allow the user to suspend or resume all of the processes in a pipeline

== Sessions, Controlling Terminals, and Controlling Processes
A session is a collection of process groups ( jobs). All of the processes in a session
have the same session identifier. A session leader is the process that created the ses-
sion, and its process ID becomes the session ID.

Sessions are used mainly by job-control shells. All of the process groups cre-
ated by a job-control shell belong to the same session as the shell, which is the ses-
sion leader.

Sessions usually have an associated controlling terminal. The controlling termi-
nal is established when the session leader process first opens a terminal device. For
a session created by an interactive shell, this is the terminal at which the user
logged in. A terminal may be the controlling terminal of at most one session.

As a consequence of opening the controlling terminal, the session leader
becomes the controlling process for the terminal. The controlling process receives a
SIGHUP signal if a terminal disconnect occurs (e.g., if the terminal window is closed).

At any point in time, one process group in a session is the foreground process
group ( foreground job), which may read input from the terminal and send output to
it. If the user types the interrupt character (usually Control-C) or the suspend character
(usually Control-Z) on the controlling terminal, then the terminal driver sends a signal
that kills or suspends (i.e., stops) the foreground process group. A session can have
any number of background process groups (background jobs), which are created by ter-
minating a command with the ampersand (&) character.

Job-control shells provide commands for listing all jobs, sending signals to jobs,
and moving jobs between the foreground and background

== Process user and group identifiers (credentials)
The key process-management system calls are those dealing with the creation
and termination of processes. Consider a typical example. A process called the
command interpreter or (i.e., shell) reads commands from a terminal. The user
has just typed a command requesting that a program be compiled. The shell must
now create a new process that will run the compiler. When that process has fin-
ished the compilation, it executes a system call to terminate itself.

Each process has a number of associated user IDs (UIDs) and group IDs (GIDs).
These include:

* Real user ID and real group ID: These identify the user and group to which the
process belongs. A new process inherits these IDs from its parent. A login shell
gets its real user ID and real group ID from the corresponding fields in the sys-
tem password file.
* Effective user ID and effective group ID: These two IDs (in conjunction with the
supplementary group IDs) are used in determining the
permissions that the process has when accessing protected resources such as
files and interprocess communication objects. Typically, the process‚Äôs effective
IDs have the same values as the corresponding real IDs. Changing the effective IDs
is a mechanism that allows a process to assume the privileges of another user or group.
* Supplementary group IDs: These IDs identify additional groups to which a pro-
cess belongs. A new process inherits its supplementary group IDs from its par-
ent. A login shell gets its supplementary group IDs from the system group file.

== Privileged processes
Traditionally, on UNIX systems, a privileged process is one whose effective user ID is 0
(superuser). Such a process bypasses the permission restrictions normally applied
by the kernel. By contrast, the term unprivileged (or nonprivileged) is applied to pro-
cesses run by other users. Such processes have a nonzero effective user ID and
must abide by the permission rules enforced by the kernel.

A process may be privileged because it was created by another privileged pro-
cess‚Äîfor example, by a login shell started by root (superuser). Another way a process
may become privileged is via the set-user-ID mechanism, which allows a process to
assume an effective user ID that is the same as the user ID of the program file that
it is executing.

== Capabilities
Since kernel 2.2, Linux divides the privileges traditionally accorded to the super-
user into a set of distinct units called capabilities. Each privileged operation is asso-
ciated with a particular capability, and a process can perform an operation only if it
has the corresponding capability. A traditional superuser process (effective user ID
of 0) corresponds to a process with all capabilities enabled.

Granting a subset of capabilities to a process allows it to perform some of the
operations normally permitted to the superuser, while preventing it from perform-
ing others.

Capability names begin with the prefix CAP_, as in CAP_KILL

https://github.com/torvalds/linux/blob/master/include/uapi/linux/capability.h
[tabs]
====
/include/uapi/linux/capability.h::
+
[source, C]
----
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 * This is <linux/capability.h>
 *
 * Andrew G. Morgan <morgan@kernel.org>
 * Alexander Kjeldaas <astor@guardian.no>
 * with help from Aleph1, Roland Buresund and Andrew Main.
 *
 * See here for the libcap2 library (compliant with Section 25 of
 * the withdrawn POSIX 1003.1e Draft 17):
 *
 * https://www.kernel.org/pub/linux/libs/security/linux-privs/libcap2/
 */

#ifndef _UAPI_LINUX_CAPABILITY_H
#define _UAPI_LINUX_CAPABILITY_H

#include <linux/types.h>

/* User-level do most of the mapping between kernel and user
   capabilities based on the version tag given by the kernel. The
   kernel might be somewhat backwards compatible, but don't bet on
   it. */

/* Note, cap_t, is defined by POSIX (draft) to be an "opaque" pointer to
   a set of three capability sets.  The transposition of 3*the
   following structure to such a composite is better handled in a user
   library since the draft standard requires the use of malloc/free
   etc.. */

#define _LINUX_CAPABILITY_VERSION_1  0x19980330
#define _LINUX_CAPABILITY_U32S_1     1

#define _LINUX_CAPABILITY_VERSION_2  0x20071026  /* deprecated - use v3 */
#define _LINUX_CAPABILITY_U32S_2     2

#define _LINUX_CAPABILITY_VERSION_3  0x20080522
#define _LINUX_CAPABILITY_U32S_3     2

typedef struct __user_cap_header_struct {
	__u32 version;
	int pid;
} __user *cap_user_header_t;

struct __user_cap_data_struct {
        __u32 effective;
        __u32 permitted;
        __u32 inheritable;
};
typedef struct __user_cap_data_struct __user *cap_user_data_t;


#define VFS_CAP_REVISION_MASK	0xFF000000
#define VFS_CAP_REVISION_SHIFT	24
#define VFS_CAP_FLAGS_MASK	~VFS_CAP_REVISION_MASK
#define VFS_CAP_FLAGS_EFFECTIVE	0x000001

#define VFS_CAP_REVISION_1	0x01000000
#define VFS_CAP_U32_1           1
#define XATTR_CAPS_SZ_1         (sizeof(__le32)*(1 + 2*VFS_CAP_U32_1))

#define VFS_CAP_REVISION_2	0x02000000
#define VFS_CAP_U32_2           2
#define XATTR_CAPS_SZ_2         (sizeof(__le32)*(1 + 2*VFS_CAP_U32_2))

#define VFS_CAP_REVISION_3	0x03000000
#define VFS_CAP_U32_3           2
#define XATTR_CAPS_SZ_3         (sizeof(__le32)*(2 + 2*VFS_CAP_U32_3))

#define XATTR_CAPS_S*           XATTR_CAPS_SZ_3
#define VFS_CAP_U32             VFS_CAP_U32_3
#define VFS_CAP_REVISION	VFS_CAP_REVISION_3

struct vfs_cap_data {
	__le32 magic_etc;            /* Little endian */
	struct {
		__le32 permitted;    /* Little endian */
		__le32 inheritable;  /* Little endian */
	} data[VFS_CAP_U32];
};

/*
 * same as vfs_cap_data but with a rootid at the end
 */
struct vfs_ns_cap_data {
	__le32 magic_etc;
	struct {
		__le32 permitted;    /* Little endian */
		__le32 inheritable;  /* Little endian */
	} data[VFS_CAP_U32];
	__le32 rootid;
};

#ifndef __KERNEL__

/*
 * Backwardly compatible definition for source code - trapped in a
 * 32-bit world. If you find you need this, please consider using
 * libcap to untrap yourself...
 */
#define _LINUX_CAPABILITY_VERSION  _LINUX_CAPABILITY_VERSION_1
#define _LINUX_CAPABILITY_U32S     _LINUX_CAPABILITY_U32S_1

#endif


/**
 ** POSIX-draft defined capabilities.
 **/

/* In a system with the [_POSIX_CHOWN_RESTRICTED] option defined, this
   overrides the restriction of changing file ownership and group
   ownership. */

#define CAP_CHOWN            0

/* Override all DAC access, including ACL execute access if
   [_POSIX_ACL] is defined. Excluding DAC access covered by
   CAP_LINUX_IMMUTABLE. */

#define CAP_DAC_OVERRIDE     1

/* Overrides all DAC restrictions regarding read and search on files
   and directories, including ACL restrictions if [_POSIX_ACL] is
   defined. Excluding DAC access covered by CAP_LINUX_IMMUTABLE. */

#define CAP_DAC_READ_SEARCH  2

/* Overrides all restrictions about allowed operations on files, where
   file owner ID must be equal to the user ID, except where CAP_FSETID
   is applicable. It doesn't override MAC and DAC restrictions. */

#define CAP_FOWNER           3

/* Overrides the following restrictions that the effective user ID
   shall match the file owner ID when setting the S_ISUID and S_ISGID
   bits on that file; that the effective group ID (or one of the
   supplementary group IDs) shall match the file owner ID when setting
   the S_ISGID bit on that file; that the S_ISUID and S_ISGID bits are
   cleared on successful return from chown(2) (not implemented). */

#define CAP_FSETID           4

/* Overrides the restriction that the real or effective user ID of a
   process sending a signal must match the real or effective user ID
   of the process receiving the signal. */

#define CAP_KILL             5

/* Allows setgid(2) manipulation */
/* Allows setgroups(2) */
/* Allows forged gids on socket credentials passing. */

#define CAP_SETGID           6

/* Allows set*uid(2) manipulation (including fsuid). */
/* Allows forged pids on socket credentials passing. */

#define CAP_SETUID           7


/**
 ** Linux-specific capabilities
 **/

/* Without VFS support for capabilities:
 *   Transfer any capability in your permitted set to any pid,
 *   remove any capability in your permitted set from any pid
 * With VFS support for capabilities (neither of above, but)
 *   Add any capability from current's capability bounding set
 *       to the current process' inheritable set
 *   Allow taking bits out of capability bounding set
 *   Allow modification of the securebits for a process
 */

#define CAP_SETPCAP          8

/* Allow modification of S_IMMUTABLE and S_APPEND file attributes */

#define CAP_LINUX_IMMUTABLE  9

/* Allows binding to TCP/UDP sockets below 1024 */
/* Allows binding to ATM VCIs below 32 */

#define CAP_NET_BIND_SERVICE 10

/* Allow broadcasting, listen to multicast */

#define CAP_NET_BROADCAST    11

/* Allow interface configuration */
/* Allow administration of IP firewall, masquerading and accounting */
/* Allow setting debug option on sockets */
/* Allow modification of routing tables */
/* Allow setting arbitrary process / process group ownership on
   sockets */
/* Allow binding to any address for transparent proxying (also via NET_RAW) */
/* Allow setting TOS (type of service) */
/* Allow setting promiscuous mode */
/* Allow clearing driver statistics */
/* Allow multicasting */
/* Allow read/write of device-specific registers */
/* Allow activation of ATM control sockets */

#define CAP_NET_ADMIN        12

/* Allow use of RAW sockets */
/* Allow use of PACKET sockets */
/* Allow binding to any address for transparent proxying (also via NET_ADMIN) */

#define CAP_NET_RAW          13

/* Allow locking of shared memory segments */
/* Allow mlock and mlockall (which doesn't really have anything to do
   with IPC) */

#define CAP_IPC_LOCK         14

/* Override IPC ownership checks */

#define CAP_IPC_OWNER        15

/* Insert and remove kernel modules - modify kernel without limit */
#define CAP_SYS_MODULE       16

/* Allow ioperm/iopl access */
/* Allow sending USB messages to any device via /dev/bus/usb */

#define CAP_SYS_RAWIO        17

/* Allow use of chroot() */

#define CAP_SYS_CHROOT       18

/* Allow ptrace() of any process */

#define CAP_SYS_PTRACE       19

/* Allow configuration of process accounting */

#define CAP_SYS_PACCT        20

/* Allow configuration of the secure attention key */
/* Allow administration of the random device */
/* Allow examination and configuration of disk quotas */
/* Allow setting the domainname */
/* Allow setting the hostname */
/* Allow mount() and umount(), setting up new smb connection */
/* Allow some autofs root ioctls */
/* Allow nfsservctl */
/* Allow VM86_REQUEST_IRQ */
/* Allow to read/write pci config on alpha */
/* Allow irix_prctl on mips (setstacksize) */
/* Allow flushing all cache on m68k (sys_cacheflush) */
/* Allow removing semaphores */
/* Used instead of CAP_CHOWN to "chown" IPC message queues, semaphores
   and shared memory */
/* Allow locking/unlocking of shared memory segment */
/* Allow turning swap on/off */
/* Allow forged pids on socket credentials passing */
/* Allow setting readahead and flushing buffers on block devices */
/* Allow setting geometry in floppy driver */
/* Allow turning DMA on/off in xd driver */
/* Allow administration of md devices (mostly the above, but some
   extra ioctls) */
/* Allow tuning the ide driver */
/* Allow access to the nvram device */
/* Allow administration of apm_bios, serial and bttv (TV) device */
/* Allow manufacturer commands in isdn CAPI support driver */
/* Allow reading non-standardized portions of pci configuration space */
/* Allow DDI debug ioctl on sbpcd driver */
/* Allow setting up serial ports */
/* Allow sending raw qic-117 commands */
/* Allow enabling/disabling tagged queuing on SCSI controllers and sending
   arbitrary SCSI commands */
/* Allow setting encryption key on loopback filesystem */
/* Allow setting zone reclaim policy */
/* Allow everything under CAP_BPF and CAP_PERFMON for backward compatibility */
/* Allow setting hardware protection emergency action */

#define CAP_SYS_ADMIN        21

/* Allow use of reboot() */

#define CAP_SYS_BOOT         22

/* Allow raising priority and setting priority on other (different
   UID) processes */
/* Allow use of FIFO and round-robin (realtime) scheduling on own
   processes and setting the scheduling algorithm used by another
   process. */
/* Allow setting cpu affinity on other processes */
/* Allow setting realtime ioprio class */
/* Allow setting ioprio class on other processes */

#define CAP_SYS_NICE         23

/* Override resource limits. Set resource limits. */
/* Override quota limits. */
/* Override reserved space on ext2 filesystem */
/* Modify data journaling mode on ext3 filesystem (uses journaling
   resources) */
/* NOTE: ext2 honors fsuid when checking for resource overrides, so
   you can override using fsuid too */
/* Override size restrictions on IPC message queues */
/* Allow more than 64h* interrupts from the real-time clock */
/* Override max number of consoles on console allocation */
/* Override max number of keymaps */
/* Control memory reclaim behavior */

#define CAP_SYS_RESOURCE     24

/* Allow manipulation of system clock */
/* Allow irix_stime on mips */
/* Allow setting the real-time clock */

#define CAP_SYS_TIME         25

/* Allow configuration of tty devices */
/* Allow vhangup() of tty */

#define CAP_SYS_TTY_CONFIG   26

/* Allow the privileged aspects of mknod() */

#define CAP_MKNOD            27

/* Allow taking of leases on files */

#define CAP_LEASE            28

/* Allow writing the audit log via unicast netlink socket */

#define CAP_AUDIT_WRITE      29

/* Allow configuration of audit via unicast netlink socket */

#define CAP_AUDIT_CONTROL    30

/* Set or remove capabilities on files.
   Map uid=0 into a child user namespace. */

#define CAP_SETFCAP	     31

/* Override MAC access.
   The base kernel enforces no MAC policy.
   An LSM may enforce a MAC policy, and if it does and it chooses
   to implement capability based overrides of that policy, this is
   the capability it should use to do so. */

#define CAP_MAC_OVERRIDE     32

/* Allow MAC configuration or state changes.
   The base kernel requires no MAC configuration.
   An LSM may enforce a MAC policy, and if it does and it chooses
   to implement capability based checks on modifications to that
   policy or the data required to maintain it, this is the
   capability it should use to do so. */

#define CAP_MAC_ADMIN        33

/* Allow configuring the kernel's syslog (printk behaviour) */

#define CAP_SYSLOG           34

/* Allow triggering something that will wake the system */

#define CAP_WAKE_ALARM            35

/* Allow preventing system suspends */

#define CAP_BLOCK_SUSPEND    36

/* Allow reading the audit log via multicast netlink socket */

#define CAP_AUDIT_READ		37

/*
 * Allow system performance and observability privileged operations
 * using perf_events, i915_perf and other kernel subsystems
 */

#define CAP_PERFMON		38

/*
 * CAP_BPF allows the following BPF operations:
 * - Creating all types of BPF maps
 * - Advanced verifier features
 *   - Indirect variable access
 *   - Bounded loops
 *   - BPF to BPF function calls
 *   - Scalar precision tracking
 *   - Larger complexity limits
 *   - Dead code elimination
 *   - And potentially other features
 * - Loading BPF Type Format (BTF) data
 * - Retrieve xlated and JITed code of BPF programs
 * - Use bpf_spin_lock() helper
 *
 * CAP_PERFMON relaxes the verifier checks further:
 * - BPF progs can use of pointer-to-integer conversions
 * - speculation attack hardening measures are bypassed
 * - bpf_probe_read to read arbitrary kernel memory is allowed
 * - bpf_trace_printk to print kernel memory is allowed
 *
 * CAP_SYS_ADMIN is required to use bpf_probe_write_user.
 *
 * CAP_SYS_ADMIN is required to iterate system wide loaded
 * programs, maps, links, BTFs and convert their IDs to file descriptors.
 *
 * CAP_PERFMON and CAP_BPF are required to load tracing programs.
 * CAP_NET_ADMIN and CAP_BPF are required to load networking programs.
 */
#define CAP_BPF			39


/* Allow checkpoint/restore related operations */
/* Allow PID selection during clone3() */
/* Allow writing to ns_last_pid */

#define CAP_CHECKPOINT_RESTORE	40

#define CAP_LAST_CAP         CAP_CHECKPOINT_RESTORE

#define cap_valid(x) ((x) >= 0 && (x) <= CAP_LAST_CAP)

/*
 * Bit location of each capability (used by user-space library and kernel)
 */

#define CAP_TO_INDEX(x)     ((x) >> 5)        /* 1 << 5 == bits in __u32 */
#define CAP_TO_MASK(x)      (1U << ((x) & 31)) /* mask for indexed __u32 */


#endif /* _UAPI_LINUX_CAPABILITY_H */
----
====

== The init process
When booting the system, the kernel creates a special process called init, the ‚Äúparent
of all processes,‚Äù which is derived from the program file /sbin/init. All processes
on the system are created (using fork()) either by init or by one of its descendants.
The init process always has the process ID 1 and runs with superuser privileges. The
init process can‚Äôt be killed (not even by the superuser), and it terminates only when
the system is shut down. The main task of init is to create and monitor a range of
processes required by a running system.

[tabs]
====
Summary::
+
In the Linux kernel, the creation of the
init process (PID 1) occurs during the final phase of kernel boot. This transition from "kernel space" to the first "user space" process is handled primarily in https://github.com/torvalds/linux/blob/master/init/main.c[init/main.c].
+
1. The Spawning Point: rest_init()
+
After the kernel has initialized its internal subsystems (memory, scheduler, interrupts) via start_kernel(), it calls rest_init(). This function is responsible for spawning the first two threads.
+
[source,C,attributes]
----
/* Simplified representation from init/main.c */
static noinline void __init_refok rest_init(void)
{
    // 1. Create the kernel thread that will become PID 1
    kernel_thread(kernel_init, NULL, CLONE_FS);
    
    // 2. Create the kernel thread manager (PID 2)
    pid = kernel_thread(kthreadd, NULL, CLONE_FS | CLONE_FILES);
    
    // 3. Current execution becomes the 'idle' task (PID 0)
    cpu_startup_entry(CPUHP_ONLINE);
}
----
2. The Bridge to Userspace: kernel_init()
The kernel_init function runs as a kernel thread but eventually transforms into the user space init process. It performs final hardware setups and then calls kernel_init_freeable().
[source,C,attributes]
----
static int __ref kernel_init(void *unused)
{
    kernel_init_freeable();

    // Mark the process as unkillable (Standard for PID 1)
    current->signal->flags |= SIGNAL_UNKILLABLE;

    // Search for the init binary in standard locations
    if (!try_to_run_init_process("/sbin/init") ||
        !try_to_run_init_process("/etc/init") ||
        !try_to_run_init_process("/bin/init") ||
        !try_to_run_init_process("/bin/sh"))
        return 0;

    panic("No working init found. Try passing init= option to kernel.");
}
----
3. Executing the Binary: run_init_process()
+
The transformation from a kernel thread to a user process happens through a wrapper around the execve system call. This replaces the kernel-mode code with the user-mode code of the specified binary (e.g., systemd or busybox).
static int run_init_process(const char *init_filename)
[source,C,attributes]
----
{
    const char *argv[] = { init_filename, NULL };
    // This call essentially performs an 'exec' to start the user program
    return kernel_execve(init_filename, argv, envp_init);
}

----
Summary of the Flow
+
* start_kernel(): Core initialization.
* rest_init(): Calls kernel_thread() to spawn the process that will be PID 1.
* kernel_init(): Performs final setups and searches for a valid init binary.
* kernel_execve(): Replaces the kernel thread's code with the binary (e.g., /sbin/init), officially starting the User Space. 
+
on most modern Linux systems, /sbin/init is not a standalone executable file but rather a symbolic link (alias) to a more complex system manager, typically systemd. 
+
Here is how it is structured across different environments:
+
1. Modern Distributions (systemd)
+
On mainstream systems like Debian, Ubuntu, and Arch Linux, /sbin/init points to the systemd binary. 
+
* Path: /sbin/init ‚Üí /lib/systemd/systemd (or /usr/lib/systemd/systemd).
* Reason: This link is maintained for backward compatibility. The Linux kernel hardcodes /sbin/init as the default location to find the first process (PID 1). By linking it to systemd, the kernel can start the modern init system without needing modified boot parameters. 
+
2. Embedded and Minimal Systems (BusyBox)
+
On lightweight systems, such as those used in routers or initial ramdisks (initramfs), /sbin/init is often a link to BusyBox. 
+
* BusyBox is a "ready-made" multicall binary that provides the functionality of many standard utilities (including init) in a single executable file.
+
3. Legacy and Alternative Systems (SysVinit)
+
In older systems or distributions that explicitly avoid systemd (like Devuan or Slackware), /sbin/init is a direct binary belonging to the SysVinit package. In this case, it is a compiled executable that specifically reads /etc/inittab to start system services. 
+
4. Windows Subsystem for Linux (WSL)
+
In WSL environments, the init process is unique. Instead of a standard Linux init binary, Microsoft uses a proprietary binary usually located at /init to manage the bridge between Windows and the Linux instance. 
+
*How to verify on your system*
+
You can check the nature of /sbin/init on your own machine using these commands:
+
* Check if it's a link: ls -l /sbin/init
* Check the file type: file /sbin/init
* Identify the running PID 1: ps --pid 1 -f 

====

== Daemon processes
A daemon is a special-purpose process that is created and handled by the system
in the same way as other processes, but which is distinguished by the following
characteristics:

* It is long-lived. A daemon process is often started at system boot and remains
in existence until the system is shut down.
* It runs in the background, and has no controlling terminal from which it can
read input or to which it can write output.

Examples of daemon processes include syslogd, which records messages in the system log, and httpd, which serves web pages via the Hypertext Transfer Protocol
(HTTP).

== Environment list
Each process has an environment list, which is a set of environment variables that are
maintained within the user-space memory of the process. 

Each element of this list
consists of a name and an associated value.

When a new process is created via
fork(), it inherits a copy of its parent‚Äôs environment. Thus, the environment pro-
vides a mechanism for a parent process to communicate information to a child pro-
cess. 

When a process replaces the program that it is running using exec(), the new
program either inherits the environment used by the old program or receives a
new environment specified as part of the exec() call.

Environment variables are created with the export command in most shells (or
the setenv command in the C shell), as in the following example:

 $ export MYVAR='Hello world'

The variable MYVAR is then available to any child processes created by the shell.

C programs can access the environment using an external variable (char **environ),
and various library functions allow a process to retrieve and modify values in its
environment.

Environment variables are used for a variety of purposes. 

* For example, the
shell defines and uses a range of variables that can be accessed by scripts and pro-
grams executed from the shell. 
** These include the variable HOME, which specifies the
pathname of the user‚Äôs login directory, 
** and the variable PATH, which specifies a list
of directories that the shell should search when looking for programs correspond-
ing to commands entered by the user.

== Resource limits
Each process consumes resources, such as open files, memory, and CPU time.

Using the setrlimit() system call, a process can establish upper limits on its consump-
tion of various resources. Each such resource limit has two associated values: 

* a soft
limit, which limits the amount of the resource that the process may consume; 

* and a
hard limit, which is a ceiling on the value to which the soft limit may be adjusted. 

An
unprivileged process may change its soft limit for a particular resource to any value
in the range from zero up to the corresponding hard limit, but can only lower its
hard limit.

When a new process is created with fork(), it inherits copies of its parent‚Äôs
resource limit settings.
The resource limits of the shell can be adjusted using the ulimit command
(limit in the C shell). These limit settings are inherited by the child processes that
the shell creates to execute commands.

[tabs]
====
ChatGPT::
+
1. What are Linux process resource limits?
+
Process resource limits (aka rlimits) restrict how much of a given resource a process can consume.
+
They are part of Linux's protection and accounting mechanisms, preventing:
+
* Runaway processes
* Accidental resource exhaustion
* Denial-of-service scenarios
+
Each limit has two values:
+
* Soft limit ‚Üí enforced by the kernel
* Hard limit ‚Üí maximum value the soft limit can be raised to
+
A process can:
+
* Lower its own soft or hard limits
* Raise soft limit up to hard limit
* Only a privileged process (CAP_SYS_RESOURCE) can raise the hard limit
+
2. Common Linux resource limits
+
Defined in POSIX + Linux-specific extensions
+
[cols="1,4", options="header"]
|===
| Limit | Meaning
| RLIMIT_CPU | Max CPU time (seconds)
| RLIMIT_FSIZE | Max file size
| RLIMIT_DATA | Max heap size
| RLIMIT_STACK | Max stack size
| RLIMIT_NOFILE | Max open file descriptors
| RLIMIT_AS | Max virtual memory
| RLIMIT_NPROC | Max number of processes
| RLIMIT_MEMLOCK | Max locked memory
| RLIMIT_CORE | Max core dump size
|===
+
3. Kernel-level view: where resource limits live
+
3.1 Core kernel data structures
+
File: include/uapi/linux/resource.h
+
[source, C]
----
struct rlimit {
	__kernel_ulong_t rlim_cur;   // soft limit
	__kernel_ulong_t rlim_max;   // hard limit
};
----
+
Each process has an array of rlimits.
+
3.2 Where rlimits are stored per process
+
File: include/linux/sched.h
+
Inside task_struct:
+
[source, C]
----
struct task_struct {
	...
	struct rlimit rlim[RLIM_NLIMITS];
	...
};
----
+
So:
+
* Every process has its own rlim[]
* Inherited on fork()
* Copied on clone()
+
4. How the kernel enforces resource limits
+
4.1 CPU time (RLIMIT_CPU)
+
File: kernel/sched/core.c
+
When CPU time exceeds limit:
+
* Kernel sends SIGXCPU
* If ignored ‚Üí process killed (SIGKILL)
+
4.2 File size (RLIMIT_FSIZE)
+
File: fs/open.c
+
When writing past limit:
+
* Write fails
* Kernel sends SIGXFSZ
+
4.3 Open files (RLIMIT_NOFILE)
+
File: fs/file.c
+
Checked during: get_unused_fd_flags()
+
If limit exceeded: EMFILE error returned
+
4.4 Virtual memory (RLIMIT_AS)
+
File: mm/mmap.c
+
Checked during:
+
* mmap
* brk
* malloc (indirectly)
+
Failure: ENOMEM
+
4.5 Stack size (RLIMIT_STACK)
+
File: arch/*/mm/fault.c
+
Enforced on:
+
* Stack growth
* Page fault handling
+
5. System calls used to manage resource limits
+
Kernel syscalls
+
File: kernel/sys.c
+
[source, C]
----
SYSCALL_DEFINE2(getrlimit, ...)
SYSCALL_DEFINE2(setrlimit, ...)
----
+
6. Using resource limits in C (user space)
+
6.1 Required headers
+
[source, C]
----
#include <sys/resource.h>
#include <stdio.h>
#include <unistd.h>
----
+
6.2 Reading resource limits (getrlimit)
+
[source, C]
----
int main() {
	struct rlimit lim;

	getrlimit(RLIMIT_NOFILE, &lim);

	printf("Soft limit: %lu\n", lim.rlim_cur);
	printf("Hard limit: %lu\n", lim.rlim_max);

	return 0;
}
----
+
6.3 Setting resource limits (setrlimit)
+
‚ö†Ô∏è You cannot exceed the hard limit unless privileged.
+
[source, C]
----
int main() {
	struct rlimit lim;

	lim.rlim_cur = 1024;   // soft
	lim.rlim_max = 2048;   // hard

	if (setrlimit(RLIMIT_NOFILE, &lim) == -1) {
		perror("setrlimit failed");
		return 1;
	}

	return 0;
}
----
+
6.4 Common example: limit memory usage
+
[source, C]
----
struct rlimit lim;

lim.rlim_cur = 100 * 1024 * 1024; // 100 MB
lim.rlim_max = 100 * 1024 * 1024;

setrlimit(RLIMIT_AS, &lim);
----
+
This prevents:
+
* malloc
* mmap
+
from exceeding 100 MB virtual memory.
+
6.5 Limit CPU time
+
[source, C]
----
struct rlimit lim;

lim.rlim_cur = 5;  // 5 seconds
lim.rlim_max = 10;

setrlimit(RLIMIT_CPU, &lim);
----
+
7. Inheritance behavior
+
[cols="1,4", options="header"]
|===
| Action | Behavior
| fork() | Child inherits limits
| execve() | Limits preserved
| setuid | Limits remain
| Container / namespace | Limits still enforced
|===
+
8. Relation to modern Linux (cgroups)
+
[cols="1,4", options="header"]
|===
| rlimits | cgroups
| Per-process | Per-group
| Simple | Fine-grained
| POSIX | Linux-specific
| Legacy | Modern container control
|===
+
üí° Docker / Kubernetes primarily use cgroups, but rlimits still apply inside containers.
+
9. When to use rlimits vs cgroups
+
Use rlimits when:
+
* Writing native C programs
* Want per-process protection
* Simple safety limits
+
Use cgroups when:
+
* Managing multiple processes
* Containers
* System-wide policies
+
10. Summary
+
Linux process resource limits:
+
* Stored in task_struct ‚Üí rlim[]
* Enforced throughout scheduler, memory, FS
* Configured via getrlimit() / setrlimit()
* Inherited across fork() and exec()
* Still relevant even with cgroups
+
Below is a real, concrete kernel call-path walkthrough showing how malloc() ends up hitting RLIMIT_AS, step by step, from user space ‚Üí glibc ‚Üí kernel ‚Üí rlimit check.
+
I'll keep it code-level accurate, but readable.
+
1. High-level picture
+
[source]
----
malloc()
  ‚Üì
glibc allocator (ptmalloc)
  ‚Üì
syscall: mmap() or brk()
  ‚Üì
kernel: do_mmap()
  ‚Üì
security + rlimit checks
  ‚Üì
RLIMIT_AS enforced
----
+
RLIMIT_AS limits the total virtual address space of the process.
+
2. User-space: malloc() decision logic
+
2.1 Where malloc() lives
+
glibc source: malloc/malloc.c
+
glibc uses ptmalloc.
+
2.2 malloc chooses between brk() and mmap()
+
Simplified logic:
+
[source, C]
----
if (size < MMAP_THRESHOLD)
	use brk()
else
	use mmap()
----
+
* Small allocations ‚Üí heap (brk)
* Large allocations ‚Üí anonymous mmap
* Default MMAP_THRESHOLD ‚âà 128 KB (tunable via mallopt)
+
3. Path A: malloc ‚Üí mmap ‚Üí kernel
+
Let's follow the mmap path (most direct for RLIMIT_AS).
+
3.1 glibc issues the syscall
+
glibc wrapper:
+
[source, C]
----
void *p = mmap(NULL, size,
			   PROT_READ | PROT_WRITE,
			   MAP_PRIVATE | MAP_ANONYMOUS,
			   -1, 0);
----
+
This becomes: `syscall(SYS_mmap, ...)`
+
3.2 Entry into kernel
+
Architecture entry point
+
Example (x86-64): arch/x86/entry/syscall_64.c
+
Control flows to: `__x64_sys_mmap()`
+
3.3 Kernel syscall handler
+
File: mm/mmap.c
+
[source, C]
----
SYSCALL_DEFINE6(mmap, ...)
{
	return ksys_mmap_pgoff(...);
}
----
+
‚Üì ksys_mmap_pgoff()
+
‚Üì vm_mmap()
+
‚Üì do_mmap()
+
4. Critical point: RLIMIT_AS enforcement
+
4.1 do_mmap()
+
File: mm/mmap.c
+
Inside do_mmap():
+
[source, C]
----
addr = get_unmapped_area(...)

if (!may_expand_vm(mm, flags, len >> PAGE_SHIFT))
	return -ENOMEM;
----
+
4.2 may_expand_vm() ‚Äî this is the key
+
[source, C]
----
bool may_expand_vm(struct mm_struct *mm,
				   vm_flags_t flags,
				   unsigned long npages)
----
+
This function checks:
+
* RLIMIT_AS
* overcommit policy
* accounting rules
+
4.3 RLIMIT_AS check
+
Inside may_expand_vm():
+
[source, C]
----
if (mm->total_vm + npages > rlimit(RLIMIT_AS) >> PAGE_SHIFT)
	return false;
----
+
Where: `rlimit(RLIMIT_AS) = current->signal->rlim[RLIMIT_AS].rlim_cur`
+
üî¥ This is the exact enforcement point
+
4.4 If limit exceeded
+
* may_expand_vm() returns false
* do_mmap() returns -ENOMEM
* syscall returns MAP_FAILED
* errno = ENOMEM
* malloc() returns NULL
+
5. Path B: malloc ‚Üí brk ‚Üí RLIMIT_AS
+
Now the heap growth path.
+
5.1 malloc calls brk()
+
glibc uses: `sbrk(increment);`
+
‚Üì `syscall(SYS_brk)`
+
5.2 Kernel: sys_brk()
+
File: mm/mmap.c
+
[source, C]
----
SYSCALL_DEFINE1(brk, unsigned long, brk)
----
+
‚Üì do_brk()
+
5.3 do_brk() ‚Üí same check
+
Inside do_brk():
+
[source, C]
----
if (!may_expand_vm(mm, flags, npages))
	return -ENOMEM;
----
+
* Same function
* Same RLIMIT_AS logic
* Same failure mode
+
6. Where the limit value comes from
+
6.1 Process structure
+
[source]
----
task_struct
 ‚îî‚îÄ‚îÄ signal_struct
	 ‚îî‚îÄ‚îÄ rlim[RLIM_NLIMITS]

current->signal->rlim[RLIMIT_AS].rlim_cur
----
+
6.2 Units
+
* RLIMIT_AS stored in bytes
* Kernel converts to pages for comparison
+
7. What happens in user space when it fails
+
malloc failure path:
+
[source, C]
----
void *p = malloc(1UL << 30);
if (!p) {
	perror("malloc");
}
----
+
Output: `malloc: Cannot allocate memory`
+
errno == ENOMEM
+
8. Why overcommit doesn't bypass RLIMIT_AS
+
Even with: `/proc/sys/vm/overcommit_memory = 1`
+
RLIMIT_AS is:
+
* Hard per-process limit
* Checked before overcommit heuristics
+
Order: RLIMIT_AS ‚Üí overcommit ‚Üí OOM killer
+
9. Observing this live (strace)
+
[source, bash]
----
ulimit -v 100000   # ~100MB
strace ./a.out
----
+
You'll see: `mmap(NULL, 104857600, ...) = -1 ENOMEM`
+
10. Summary (call path)
+
[source]
----
malloc()
  ‚îî‚îÄ glibc ptmalloc
	  ‚îú‚îÄ small ‚Üí brk()
	  ‚îÇ         ‚îî‚îÄ do_brk()
	  ‚îÇ             ‚îî‚îÄ may_expand_vm()
	  ‚îÇ                 ‚îî‚îÄ RLIMIT_AS check
	  ‚îî‚îÄ large ‚Üí mmap()
				‚îî‚îÄ do_mmap()
					‚îî‚îÄ may_expand_vm()
						‚îî‚îÄ RLIMIT_AS check
----

XML::
+
.XML Configuration
[source, xml]
----
----
====

== Interprocess Communication and Synchronization
A running Linux system consists of numerous processes, many of which operate
independently of each other. Some processes, however, cooperate to achieve their
intended purposes, and these processes need methods of communicating with one
another and synchronizing their actions.

One way for processes to communicate is by reading and writing informa-
tion in disk files. However, for many applications, this is too slow and inflexible.
Therefore, Linux, like all modern UNIX implementations, provides a rich set of mech-
anisms for interprocess communication (IPC), including the following:

* **signals**, which are used to indicate that an event has occurred;
* pipes (familiar to shell users as the | operator) and FIFOs, which can be used to
transfer data between processes;
* sockets, which can be used to transfer data from one process to another, either
on the same host computer or on different hosts connected by a network;
* file locking, which allows a process to lock regions of a file in order to prevent
other processes from reading or updating the file contents;
* message queues, which are used to exchange messages (packets of data) between
processes;
* semaphores, which are used to synchronize the actions of processes; and
* shared memory, which allows two or more processes to share a piece of memory.
When one process changes the contents of the shared memory, all of the other
processes can immediately see the changes.

The wide variety of IPC mechanisms on UNIX systems, with sometimes overlapping
functionality, is in part due to their evolution under different variants of the UNIX
system and the requirements of various standards. For example, FIFOs and UNIX
domain sockets essentially perform the same function of allowing unrelated pro-
cesses on the same system to exchange data. Both exist in modern UNIX systems
because FIFOs came from System V, while sockets came from BSD.

=== Signals
signals are
more usually employed in a wide range of other contexts, 

Signals are often described as ‚Äúsoftware interrupts.‚Äù The arrival of a signal
informs a process that some event or exceptional condition has occurred. There
are various types of signals, each of which identifies a different event or condition.
Each signal type is identified by a different integer, defined with symbolic names of
the form SIGxxxx.

Signals are sent to a process by the kernel, by another process (with suitable
permissions), or by the process itself. For example, the kernel may send a signal to
a process when one of the following occurs:

* the user typed the interrupt character (usually Control-C) on the keyboard;
* one of the process‚Äôs children has terminated;
* a timer (alarm clock) set by the process has expired; or
* the process attempted to access an invalid memory address.

Within the shell, the kill command can be used to send a signal to a process. The
kill() system call provides the same facility within programs.

When a process receives a signal, it takes one of the following actions, depend-
ing on the signal:

* it ignores the signal;
* it is killed by the signal; or
* it is suspended until later being resumed by receipt of a special-purpose signal.

For most signal types, 

* instead of accepting the default signal action, a program can
choose to ignore the signal (useful if the default action for the signal is something
other than being ignored), 
* or to establish a signal handler. A signal handler is a
programmer-defined function that is automatically invoked when the signal is
delivered to the process. This function performs some action appropriate to the
condition that generated the signal.

In the interval between the time it is generated and the time it is delivered, a
signal is said to be pending for a process. Normally, a pending signal is delivered as
soon as the receiving process is next scheduled to run, or immediately if the pro-
cess is already running. However, it is also possible to block a signal by adding it to
the process‚Äôs signal mask. If a signal is generated while it is blocked, it remains
pending until it is later unblocked (i.e., removed from the signal mask).

=== Pipes
pipes relates to both processes and files. 

A pipe is a sort of pseudofile that can be used to connect two processes. If processes A and B wish to talk using a pipe, they
must set it up in advance. When process A wants to send data to process B, it writes
on the pipe as though it were an output file. In fact, the implementation of a pipe is
very much like that of a file. Process B can read the data by reading from the pipe
as though it were an input file. Thus, communication between processes in UNIX
looks very much like ordinary file reads and writes. Stronger yet, the only way a
process can discover that the output file it is writing on is not really a file, but a
pipe, is by making a special system call.

== Threads
In traditional operating systems, each process has an address space and a single
thread of control. In fact, that is almost the definition of a process. Nevertheless, in
many situations, it is useful to have multiple threads of control in the same address
space running in quasi-parallel, as though they were (almost) separate processes
(except for the shared address space).

In modern UNIX implementations, each process can have multiple threads of execution. 

The main reason for having threads is that in many
applications, multiple activities are going on at once. Some of these may block
from time to time. By decomposing such an application into multiple sequential
threads that run in quasi-parallel, the programming model becomes simpler. Only now with threads we add a new ele-
ment: the ability for the parallel entities to share an address space and all of its data
among themselves. This ability is essential for certain applications, which is why
having multiple processes (with their separate address spaces) will not work

A second argument for having threads is that since they are lighter weight than
processes, they are easier (i.e., faster) to create and destroy. In many systems,
creating a thread goes 10‚Äì100 times faster than creating a process. When the num-
ber of threads needed changes dynamically and rapidly, this property is useful to
have.

A third reason for having threads is also a performance argument. Threads
yield no performance gain when all of them are CPU bound, but when there is sub-
stantial computing and also substantial I/O, having threads allows these activities
to overlap, thus speeding up the application.

=== Classical Thread Model
The process model is based on two
independent concepts: resource grouping and execution. Sometimes it is useful to
separate them; this is where threads come in.

One way of looking at a process is that it is a convenient way to group together
related resources. A process has an address space that contains program text and
data, as well as other resources. These resources may include open files, child
processes, pending alarms, signal handlers, accounting information, and more. By
putting them together in the form of a process, they can be managed more easily.

So In the classical model, One process = one resource group + one execution. So If you want two things running at once, You create two processes Each process gets:

* Its own memory
* Its own files
* Its own execution

but This can be wasteful and slow: Duplicated memory,Expensive context switches, Hard to share data.

A thread on the other hand is Execution only, It does not own resources, its

* Share the same resource group
* Have their own: Program counter, Registers, 

Simple analogy

* Process = an office
* Threads = workers in that office
* All workers: Share files, desks, printers (resources)
* Each worker: Does their own task independently (execution)

Example: Web server

* Without threads:
** One process per request
** Each process:
*** Loads its own copy of data
*** Opens its own resources
* With threads:
** One process:
*** Owns memory, files, sockets
** Many threads:
*** Handle requests in parallel
*** Share cached data
*** Run faster and cheaper

The other concept a process has is a thread of execution, usually shortened to
just thread. The thread has a program counter associated with it that keeps track
of which instruction to execute next. It has registers, which hold its current work-
ing variables. It also has a stack, which contains the thread‚Äôs execution history, one
frame for each procedure called but not yet returned from. Although a thread must
execute in some process, the thread and its process are different concepts and can
be treated separately. Processes are used to group resources together; threads are
the entities scheduled for execution on the CPU.

What threads add to the process model is to allow multiple executions to take
place in the same process environment (and address space), to a large degree inde-
pendent of one another. Having multiple threads running in parallel in one process
is analogous to having multiple processes running in parallel in one computer. In
the former case, the threads share an address space and other resources. In the lat-
ter case, processes share physical memory, disks, printers, and other resources.
Because threads have some of the properties of processes, they are sometimes call-
ed lightweight processes. The term multithreading is also used to describe the
situation of allowing multiple threads in the same proces

some CPUs have direct hardware support for multithreading and allow a thread
switch to happen on a nanosecond time scale.

When a multithreaded process is run on a single-CPU system, the threads take
turns running. we saw how multiprogramming of processes works. By
switching back and forth among multiple processes, the system gives the illusion
of separate sequential processes running in parallel. Multithreading works the same
way. The CPU switches rapidly back and forth among the threads, providing the
illusion that the threads are running in parallel, albeit on a slower CPU. With three
compute-bound threads in a process, the threads would appear to be running in par-
allel, each one on a CPU with one-third the speed of the real CPU.

The different threads in a process are not as independent as different processes.
All threads have exactly the same address space, which means that they also share
the same global variables. Since every thread can access every memory address
within the process‚Äô address space, one thread can read, write, or even wipe out an-
other thread‚Äôs stack. There is no protection between threads because
+
. it is impossible, and 
. it should not be necessary.

Unlike different processes, which may
be from different users and which may be mutually hostile to one another, a proc-
ess is always owned by a single user, who has presumably created multiple threads
so that they can cooperate, not fight with each other. In addition to sharing an ad-
dress space, all the threads can share the same set of open files, child processes,
signals, alarms, and so forth
|===
|Per-process items |Per-thread items

|Address space |Program counter

|Global variables |Registers

|Open files |Stack

|Child processes State |

|Pending alarms |

|Signals and signal handlers |

|Accounting information |
|===
The first column lists some items shared by all threads in a process. The second one lists some items private to each thread.

In above table The items in the first column are process properties, not thread properties. For
example, if one thread opens a file, that file is visible to the other threads in the
process and they can read and write it. This is logical, since the process is the unit
of resource management, not the thread. If each thread had its own address space,
open files, pending alarms, and so on, it would be a separate process. What we are trying to achieve with the thread concept is the ability for multiple threads of
execution to share a set of resources so that they can work together closely to per-
form some task.

Like a traditional process (i.e., a process with only one thread), a thread can be
in any one of several states: 

* *running*:A running
thread currently has the CPU and is active
* *blocked*: In contrast, a blocked thread is waiting
for some event to unblock it. For example, when a thread performs a system call to
read from the keyboard, it is blocked until input is typed. A thread can block wait-
ing for some external event to happen or for some other thread to unblock it.
* *ready*: A
ready thread is scheduled to run and will as soon as its turn comes up.
* *terminated*

The transitions between thread states are the same as those between process states

Each thread has its own stack. Each thread‚Äôs stack contains one frame for each procedure called but
not yet returned from. This frame contains the procedure‚Äôs local variables and the
return address to use when the procedure call has finished. For example, if proce-
dure X calls procedure Y and Y calls procedure Z, then while Z is executing, the
frames for X, Y, and Z will all be on the stack. Each thread will generally call dif-
ferent procedures and thus have a different execution history. This is why each
thread needs its own stack.

When multithreading is present, processes usually start with a single thread
present. This thread has the ability to create new threads by calling a library proce-
dure such as `thread_create`. A parameter to thread create specifies the name of a
procedure for the new thread to run. It is not necessary (or even possible) to specify
anything about the new thread‚Äôs address space, since it automatically runs in the
address space of the creating thread. Sometimes threads are hierarchical, with a
parent-child relationship, but often no such relationship exists, with all threads
being equal. With or without a hierarchical relationship, the creating thread is
usually returned a thread identifier that names the new thread.

When a thread has finished its work, it can exit by calling a library procedure,
say, `thread_exit`. It then vanishes and is no longer schedulable. In some thread systems, one thread can wait for a (specific) thread to exit by calling a procedure,
for example, thread join. This procedure blocks the calling thread until a (specif-
ic) thread has exited. In this regard, thread creation and termination is very much
like process creation and termination, with approximately the same options as well.

Another common thread call is `thread_yield`, which allows a thread to volun-
tarily give up the CPU to let another thread run. Such a call is important because
there is no clock interrupt to actually enforce multiprogramming as there is with
processes. Thus, it is important for threads to be polite and voluntarily surrender
the CPU from time to time to give other threads a chance to run. Other calls allow
one thread to wait for another thread to finish some work, for a thread to announce
that it has finished some work, and so on.

so in the classical thread model:

* Process: Resource container
* Thread: Execution unit
----
Process
 ‚îú‚îÄ‚îÄ Memory
 ‚îú‚îÄ‚îÄ Open files
 ‚îî‚îÄ‚îÄ Threads
      ‚îú‚îÄ‚îÄ Thread 1 (PC, registers, stack)
      ‚îú‚îÄ‚îÄ Thread 2 (PC, registers, stack)
      ‚îî‚îÄ‚îÄ Thread 3 (PC, registers, stack)
----

=== POSIX Threads
To make it possible to write portable threaded programs, IEEE has defined a
standard for threads in IEEE standard 1003.1c. The threads package it defines is
called Pthreads. Most UNIX systems support it. The standard defines over 60
function calls
|===
|Thread call |Description
|pthread create |Create a new thread
|pthread exit |Terminate the calling thread
|pthread join |Wait for a specific thread to exit
|pthread yield |Release the CPU to let another thread run
|pthread attr |init Create and initialize a thread‚Äôs attribute structure
|pthread attr |destroy Remove a thread‚Äôs attribute structure
|===
In POSIX threads (Pthreads), every thread is like a small independent worker inside a program.

Each thread has its own set of characteristics that the system keeps track of.

All Pthreads threads have certain properties. Each one has an identifier, a set of
registers (including the program counter), and a set of attributes, which are stored
in an attribute structure. The attributes include the stack size, scheduling parame-
ters, and other items needed to use the thread.

* Thread identifier: Each thread gets a unique ID, which lets the system (and you, the programmer) distinguish it from other threads.
+
[,C]
----
pthread_t thread1, thread2;
pthread_create(&thread1, NULL, worker, NULL);
pthread_create(&thread2, NULL, worker, NULL);
// thread1 and thread2 have unique identifiers
----
* Registers: A thread has its own CPU registers, which are like the ‚Äúscratchpad‚Äù memory for the CPU.
** Program Counter: Program Counter (PC) is a special register that tells the CPU which instruction to execute next. 
+
This allows threads to run independently because each keeps track of where it is in its own code.
* Thread attributes: Threads also have a set of attributes, which are stored in an attribute structure (pthread_attr_t in C).
+
These attributes control how the thread behaves. Common examples include:
+
** Stack size: How much memory the thread can use for its own function calls and local variables.
** Scheduling parameters: Determines how the OS schedules this thread relative to others (priority, policy, etc.).
** Detach state: Whether the thread can be joined later or cleans itself up automatically.
+
Example of setting attributes:
+
[,C]
----
pthread_t thread;
pthread_attr_t attr;

// Initialize attribute structure
pthread_attr_init(&attr);

// Set stack size
pthread_attr_setstacksize(&attr, 1024 * 1024); // 1 MB stack

// Create a thread with these attributes
pthread_create(&thread, &attr, worker, NULL);

// Destroy attributes when done
pthread_attr_destroy(&attr);
----

Having separate registers and program counters lets multiple threads run simultaneously without interfering with each other.

Attributes allow fine control over thread behavior, which is important for performance, memory usage, and responsiveness in your program.

Think of a thread like a chef in a kitchen:

* Identifier ‚Üí Each chef has a name badge.
* Registers + program counter ‚Üí Each chef has their own recipe book and notes, so they know what step to cook next.
* Attributes ‚Üí Some chefs might have special tools, workstations, or priorities in the kitchen.

A new thread is created using the pthread create call. The thread identifier of
the newly created thread is returned as the function value. This call is intentionally
very much like the fork system call (except with parameters), with the thread iden-
tifier playing the role of the PID, mostly for identifying threads referenced in other
calls. When a thread has finished the work it has been assigned, it can terminate by
calling pthread exit. This call stops the thread and releases its stack.

Threads sometimes need to wait for another thread to finish its work and exit
before it can continue. In a multi-threaded program, some threads depend on the results of other threads before they can continue. Example: Imagine you‚Äôre building a sandwich: one person slices bread, another spreads butter. The second person must wait until the first is done.

pthread_join is the ‚Äúwaiting mechanism‚Äù, In POSIX threads, the function pthread_join is used when one thread wants to wait for another thread to finish. This ensures that the program doesn‚Äôt continue with incomplete work.

How pthread_join works? The thread that calls pthread_join is paused until the thread it‚Äôs waiting for terminates. You need to provide the thread identifier (pthread_t) of the thread to wait for. Optionally, you can get the return value of the finished thread.
[,C]
----
#include <pthread.h>
#include <stdio.h>

void* worker(void* arg) {
    printf("Worker thread is working...\n");
    return (void*)42;  // Thread returns a value
}

int main() {
    pthread_t thread;
    void* result;

    // Create a thread
    pthread_create(&thread, NULL, worker, NULL);

    // Wait for the thread to finish
    pthread_join(thread, &result);

    printf("Thread finished with result: %ld\n", (long)result);
    return 0;
}
----
Step-by-step:

* Main thread creates a worker thread.
* Worker thread does some work.
* Main thread calls pthread_join(thread, &result) ‚Üí main thread pauses here.
* When worker finishes, pthread_join returns, and main thread can continue.

*Why this matters*

* Synchronization: Ensures threads don‚Äôt proceed before necessary data is ready.
* Resource cleanup: pthread_join also helps reclaim resources used by the finished thread.
* Data safety: Prevents situations where one thread accesses incomplete results from another.

*Analogy*

Think of threads like students in a classroom:

* Student A finishes a group project section.
* Student B cannot start the next part until A is done. pthread_join is like the teacher saying: ‚ÄúWait until Student A hands in their section before you continue.‚Äù

Threads can voluntarily give up the CPU

==== yielding
Sometimes a thread is not blocked (it could continue running), but it wants to let other threads run. This is called yielding. In POSIX threads, the function for this is pthread_yield (or sometimes sched_yield in Linux).

Threads belong to the same process, and often they are cooperating. By yielding, a thread shows ‚ÄúI‚Äôve done enough for now, other threads should have a chance to run.This can help improve fairness or avoid starvation of other threads.

There is no such yield call for processes because processes are usually independent and competitive. A process could voluntarily call sleep() to pause briefly, but that‚Äôs not common.  However, since the threads of a process are
working together and their code is invariably written by the same programmer,
sometimes the programmer wants them to give each other another chance

Yielding is like sharing the CPU politely so all threads get a chance to progress.
[,c]
----
#include <pthread.h>
#include <sched.h>
#include <stdio.h>

void* worker(void* arg) {
    for(int i = 0; i < 5; i++) {
        printf("Thread %ld working iteration %d\n", (long)arg, i);
        sched_yield(); // voluntarily give other threads a chance
    }
    return NULL;
}

int main() {
    pthread_t t1, t2;

    pthread_create(&t1, NULL, worker, (void*)1);
    pthread_create(&t2, NULL, worker, (void*)2);

    pthread_join(t1, NULL);
    pthread_join(t2, NULL);

    return 0;
}

----
* Both threads take turns printing numbers.
* sched_yield() lets the other thread run, even if the CPU isn‚Äôt busy.
* Without yield, one thread might hog the CPU and make the output less interleaved.

Analogy

Think of threads like students working on a group project:

* Student A has been typing for a while.
* Student A says: ‚ÄúI‚Äôve done enough for now; let Student B type for a bit.‚Äù ‚Üí That‚Äôs pthread_yield.
* Processes are like different companies: they usually fight for all the resources and don‚Äôt cooperate in this way.

=== Implementing Threads
there are three main threading models‚Äîuser-space threads, kernel threads, and hybrid (or ‚Äúcombined‚Äù) models.
[cols="1,1,1", options="header"]
|===
| Thread Model | Pros | Cons
| User-Space Threads | Very fast, flexible | Blocks entire process on I/O, no true parallelism
| Kernel Threads | True parallelism, kernel scheduling | Higher overhead for creation and switching
| Hybrid Threads | Combines user speed with kernel parallelism | Complex to implement, rare today
|===

=== Implementing Threads in User Space
*How it works*

* Threads are managed entirely in user space by a library(put the threads package entirely in user space).
* The kernel sees one process; it doesn‚Äôt know about the individual threads( As far as the kernel is concerned, it is managing
ordinary, single-threaded processe).
* All of these implementations have the same general structure. The threads run on top of a run-time system, which is a collection of
procedures that manage threads, such as pthread_create, pthread_exit, pthread_join, and pthread_yield, but usually there are more.
+
image::{figures}/thread-user-level-thread-vs-kernel-level-thread.png[(a) A user-level threads package. (b) A threads package managed by the kernel.]

When threads are managed in user space, each process needs its own private
thread table to keep track of the threads in that process. This table is analogous to
the kernel‚Äôs process table, except that it keeps track only of the per-thread proper-
ties, such as each thread‚Äôs program counter, stack pointer, registers, state, and so
forth. The thread table is managed by the run-time system. When a thread is moved
to the ready or blocked state, the information needed to restart it is stored in the
thread table, exactly the same way as the kernel stores information about processes
in the process table.

*user-level thread context switching*

When a thread does something that may cause it to become blocked locally, for
example, waiting for another thread in its process to complete some work. The thread might block locally, but the kernel does not know because we are in a user-level threads package.

. *it calls a run-time system procedure*
** The thread calls a runtime library procedure (e.g., uthread_yield() in user-space thread libraries).
** The library is responsible for scheduling user threads.
. *Check Blocked State*
** This procedure checks to see if the thread must be put
into blocked state((e.g., waiting on a lock, a condition variable, or I/O)). 
. *Save Registers*
** If so, it stores the thread‚Äôs registers (i.e., its own,program counter, stack pointer, general-purpose registers) in the thread
table, 
** This is essentially the thread's context.
. *Select Next Thread*
** it looks in the table for a ready thread to run.
. *Load Registers*
** it reloads the machine(the saved) registers with the new thread‚Äôs saved values from the thread table.
. *Switch Happens Instantly*
** As soon as the stack pointer and program
counter have been switched, the new thread comes to life again automatically(he new thread resumes execution automatically). 

*Why This is Fast*

If the machine happens to have an instruction to store all the registers and another
one to load them all, the entire thread switch can be done in just a handful of
instructions. Doing thread switching like this is at least an order of magnitude‚Äî
maybe more‚Äîfaster than trapping to the kernel and is a strong argument in favor
of user-level threads packages. In contrast, kernel thread switching requires:

* A trap to the kernel (syscall)
* Kernel scheduler execution
* Possibly updating TLBs, page tables, and other kernel structures

and this is much slower.

Also, when a thread is finished running for the moment, for example,
when it calls thread yield, 

* the code of thread yield saves the thread‚Äôs information
in the thread table 
* and then calls the thread scheduler to pick another thread to run.

The procedure that saves the thread‚Äôs state and the scheduler are just local proce-
dures, so invoking them is much more efficient than making a kernel call. There is
no need for a trap, a context switch, flushing of caches, and so on. This makes
thread scheduling very fast.

Think of user-level threads as a mini-scheduler inside each process. The process itself decides which thread runs and when, instead of the kernel imposing a global schedule.

*Pros*

* a user-level threads package can be implemented on an operating system that does
not support threads. All operating systems used to fall into this category, and even
now some still do. With this approach, threads are implemented by a library.
* Extremely fast thread creation, switching, and synchronization (no syscalls).
* Fine-grained control by the programmer.
* Custom Scheduling per Process
** They allow each process to have
its own customized scheduling algorithm.
** Example:
*** A process might prioritize a garbage-collector thread differently from worker threads. 
*** With kernel threads, the kernel scheduler may preempt the thread at inconvenient times. 
** Benefit: more predictable behavior for time-sensitive threads.
* No Kernel Intervention
** Since the kernel sees the process as a single schedulable entity, thread switching does not require traps or syscalls.
** This avoids unnecessary preemption and gives the application full control over scheduling. for example,For some applications, 
those with a garbage-collector thread, not having to worry about a thread being
stopped at an inconvenient moment is a plus.
* Better Scalability
** Kernel threads require kernel table entries and stacks for each thread.
** Limitation: creating thousands of kernel threads can consume significant kernel memory.
** User-level threads only need user-space data structures, so:
*** Thousands or tens of thousands of threads are feasible.
*** Memory overhead is much lower.
* Predictable Performance
** User-level threads are not interrupted by the kernel scheduler, so latency-sensitive threads (e.g., in real-time applications) can run without unexpected delays.
** Ideal for applications with cooperatively scheduled workloads

*Summary Table of Key Advantages*

[cols="1,1", options="header"]
|===
| Advantage | Explanation
| Custom Scheduling | Each process can implement its own thread scheduling algorithm
| Low Overhead | Context switching happens entirely in user space, avoiding syscalls
| Scalability | Thousands of threads possible without kernel resource exhaustion
| Predictable Execution | Threads are not preempted unexpectedly by kernel
|===

*Cons*

* *Blocking System Calls*
** Problem: User-level threads are invisible to the kernel. If a user-level thread calls a blocking system call (e.g., read(), recv(), or sleep()), the kernel blocks the entire process, not just the thread because the kernel schedules only the single process.
** Impact: All other threads in that process are halted even if they are runnable.
** Consequence: Limits usefulness for I/O-intensive applications unless you use non-blocking I/O or asynchronous I/O.
** Suppose that a thread reads from the keyboard before any keys have
been hit. Letting the thread actually make the system call is unacceptable, since
this will stop all the threads. One of the main goals of having threads in the first
place was to allow each thread to use blocking calls, but to prevent one blocked thread from affecting the others. With blocking system calls, it is hard to see how
this goal can be achieved readily.
** Changing system calls to always be nonblocking (e.g., return 0 bytes if no data,, a read on the
keyboard would just return 0 bytes if no characters were already buffered) is impractical, because: It requires modifying the operating system and Many existing programs expect the standard blocking behavior of syscalls. Besides, one argument
for user-level threads was precisely that they could run with existing operating systems. 
** Somewhat analogous to the problem of blocking system calls is the problem of
page faults.
computers can be set up in such a way that not all of the program is in main memo-
ry at once. If the program calls or jumps to an instruction that is not in memory, a
page fault occurs and the operating system will go and get the missing instruction
(and its neighbors) from disk. This is called a page fault. The process is blocked
while the necessary instruction is being located and read in. If a thread causes a
page fault, the kernel, unaware of even the existence of threads, naturally blocks
the entire process until the disk I/O is complete, even though other threads might
be runnable.
*** Imagine a user-level thread system with:
+
Thread A: garbage collector
+
Thread B: network server
+
Thread C: computation
+
If Thread A touches a rarely used memory page:
+
Page fault occurs
+
Kernel blocks the process
+
Thread B and C stop running
+
Disk I/O completes
+
Entire process resumes
+
This defeats the whole purpose of fast user-level scheduling.

* *No Preemption in User-Level Threads*
** Preemption: A scheduler interrupts a currently running process to give the CPU to another, higher-priority process, allowing multitasking.
** Another problem with user-level thread packages is that if a thread starts run-
ning, no other thread in that process will ever run unless the first thread voluntarily
gives up the CPU. Within a single process, there are no clock interrupts, making it
impossible to schedule processes in round-robin fashion (taking turns). Unless a
thread exits the run-time system of its own free will, the scheduler will never run.
** If one user-level thread runs forever, no other thread in the process will ever run. because:
*** User-level threads run entirely in user space
*** The kernel does not know they exist
*** The kernel delivers timer interrupts to the process, not to the user-level scheduler
*** Therefore: 
**** No forced context switch between user threads
**** Scheduling is cooperative, not preemptive 
** in the oppisite, What Normally Enables Round-Robin Scheduling for kernel threads:
**** A hardware timer interrupt occurs
**** Kernel regains control
**** Kernel scheduler:
**** Saves current thread state
**** Picks another runnable thread
**** Context switch happens automatically
*** This mechanism does not exist inside a user-level thread package.
** Consequence: Scheduling is cooperative in user kernel thread
*** User-level threads rely on voluntary yield:
**** yield()
**** Blocking on a lock
**** Explicit calls into the runtime system
*** If a thread:
**** Runs a long computation
**** Contains an infinite loop
**** Never yields
+
All other threads starve forever
** Why This Is a Serious Problem
*** No Fairness Guarantees
**** One misbehaving thread can monopolize the CPU
**** Bugs become catastrophic
*** No Time Sharing
**** Round-robin scheduling is impossible
**** Priority enforcement is unreliable
*** Hard to Debug
**** Bugs appear as ‚Äúsystem hangs‚Äù
**** The kernel sees the process as runnable
* *No True Parallelism on Multi-Core Systems*
** Problem: The kernel schedules processes, not individual user-level threads.
** Impact: Even with multiple cores, all threads of a process run on only one core at a time.
** Consequence: Cannot exploit multiple CPU cores for CPU-bound tasks without hybrid approaches.
* *Complexity for Synchronization*
** Problem: Libraries must implement thread synchronization (mutexes, condition variables, etc.) entirely in user space.
** Impact: Must carefully manage thread states, ready queues, and blocking; potential for bugs.
** Consequence: More complex to implement correctly compared to kernel threads where the kernel handles blocking/wakeup safely.
* *Signals and Interrupt Handling*
** Problem: Signals are delivered at the process level, not thread level.
** Impact: Only one thread may handle the signal; mapping signals to threads can be tricky.
** Consequence: Complicates implementation of signal handlers in multithreaded programs.
* *Difficulty Integrating with Kernel Services*
** Problem: Many OS services assume a kernel thread per execution context.
** Impact: Features like CPU affinity, real-time scheduling, or priority inheritance are unavailable or hard to implement.
** Consequence: Cannot fully leverage kernel features without hybrid mapping.
* *Debugging and Profiling Challenges*
** Problem: The kernel sees only one thread per process.
** Impact: Tools like top, perf, or strace cannot observe individual threads.
** Consequence: Harder to diagnose performance issues or deadlocks.
* *User-level threads are worst exactly where threads are most useful.*
** Another, and really the most devastating, argument against user-level threads is
that programmers typically want threads precisely in applications where threads
block often, as, for example, in a multithreaded Web server. These threads are con-
stantly making system calls. Once a trap has occurred to the kernel to carry out the
system call, it is hardly any more work for the kernel to switch threads if the old
one has blocked, and having the kernel do this eliminates the need for constantly
making select system calls to see if read system calls are safe. For applications that
are CPU bound and rarely block, what is the point of having threads at all? No one
would seriously propose computing the first n prime numbers or playing chess
using threads because there is nothing to be gained by doing it that way.
** Programmers Use Threads in the First Place, forTypical thread-heavy applications: Multithreaded web servers, Network servers, Databases, File servers. All of this apps have Common characteristics: Many concurrent tasks, Tasks block frequently on:read(), write(), accept(), disk I/O, network I/O, so Threads exist to hide I/O latency.
** But What Happens with User-Level Threads in These Apps? Frequent system call, for Every I/O operation:
+
----
user thread ‚Üí system call ‚Üí kernel
----
+
If the call blocks: Kernel blocks the entire process, All user threads stop, User-level scheduler is powerless.
** To survive, user-level thread libraries must(Runtime must avoid blocking (the ‚Äúselect wrapper‚Äù hack)): Call select() / poll() / epoll() before every read(), Only perform the system call if it won‚Äôt block, Otherwise: Switch to another thread
+
This leads to: Extra system calls, Extra context switching, Complex wrapper code, Worse performance
** Kernel Is Already Involved Anyway,At that point: You‚Äôve already paid the cost of: Mode switch, Kernel entry, But Switching kernel threads costs almost nothing extra.
** Kernel Threads Do the Right Thing Automatically, When a kernel thread blocks: Thread enters kernel, Kernel sees it will block,Kernel scheduler: Puts that thread to sleep, Runs another thread immediately, So No:select() wrappers, polling loops, user-space schedulers
** SO this Makes User-Level Threads Pointless for I/O Apps
+
|===
|Feature	|User-Level Threads	|Kernel Threads
|Frequent I/O	|‚ùå bad	|‚úÖ ideal
|Blocking handling	|Complex	|Automatic
|Scheduler	User-space	|Kernel
|Performance	Worse	|Better
|Code complexity	High	|Low
|===
** Also For applications that are CPU bound and rarely block(Prime number generation, Chess engines, Compression, Scientific computation),which is: has Minimal blocking, Long-running computations, what is the point of having threads at all?Threads don‚Äôt help because:
*** Only one CPU core is used unless parallelized
*** Context switching adds overhead
*** No I/O latency to hide
** Threads Actually Make Sense in: Frequent blocking, I/O concurrency, Parallelism across CPUs, Responsiveness, All of these require kernel support. so CPU-bound programs don‚Äôt benefit from threads
** User-level threads lose their advantage as soon as system calls are frequent, and since threads are mainly useful for I/O-bound workloads, kernel-level threads are the superior design.

*Summary Table*

[cols="1,1", options="header"]
|===
| Problem | Explanation / Impact
| Blocking System Calls | One thread blocks, all threads blocked
| No True Parallelism | Cannot use multiple cores simultaneously
| Complex Synchronization | All thread management must be in user-space
| Signals Handling | Signals delivered at process level, difficult to assign to threads
| Limited Kernel Features | Cannot leverage kernel scheduling, CPU affinity, or priorities fully
| Debugging / Profiling | Hard to observe individual threads from kernel tools
|===


=== Implementing Threads in Kernel Space(Kernel-Level Threads)
In  Kernel Space threads no need for a run-time system or thread table in
each process. Instead, the kernel has a thread table that keeps track of all the
threads in the system. When a thread wants to create a new thread or destroy an
existing thread, it makes a kernel call, which then does the creation or destruction
by updating the kernel thread table.

The kernel‚Äôs thread table holds each thread‚Äôs registers, state, and other infor-
mation. The information is the same as with user-level threads, but now kept in the
kernel instead of in user space (inside the run-time system). This information is a
subset of the information that traditional kernels maintain about their single-
threaded processes, that is, the process state. In addition, the kernel also maintains
the traditional process table to keep track of processes.

All calls that might block a thread are implemented as system calls, at consid-
erably greater cost than a call to a run-time system procedure. When a thread
blocks, the kernel may choose to run either another thread from the same process
(if one is ready) or a thread from a different process. With user-level threads, the
run-time system keeps running threads from its own process until the kernel takes
the CPU away from it (or there are no ready threads left to run)

Kernel threads do not require any new, nonblocking system calls. In addition,
if one thread in a process causes a page fault, the kernel can easily check to see if
the process has any other runnable threads, and if so, run one of them while wait-
ing for the required page to be brought in from the disk.

**How it works**

* Kernel-space threads are known and managed directly by the operating system kernel.
* Each thread has a kernel data structure (e.g., task_struct in Linux).
* Threads in the same process:
** Share the same address space.
** Share open files, signal handlers, and other resources.
** Have separate stacks, registers, and scheduling states.
* Thread creation:
** Created via kernel call (e.g., clone() in Linux).
** Kernel allocates:
*** Kernel stack
*** Thread control block
*** Scheduling information
* Blocking behavior:
** If one thread blocks (I/O, page fault), only that thread blocks.
** Other threads in the process continue running.
* Scheduling:
** Threads are scheduled like processes.
** Kernel scheduler picks the next runnable thread from all system threads.

**Context switching**

* What is switched:
** CPU registers
** Program counter (instruction pointer)
** Stack pointer
** Processor flags
** CPU-specific state (FPU, SIMD registers)
** Scheduling metadata
* Where it happens:
** Inside the kernel
** Triggered by:
*** Blocking system calls
*** Page faults
*** Timer interrupts (preemption)
*** Explicit yielding (sched_yield())
* Cost:
** More expensive than user-level thread switching due to kernel mode execution and scheduler logic.
** Still cheaper than full process switching because address space is shared.

**Pros**

* True parallelism: Multiple threads can run on multiple CPU cores.
* Correct handling of blocking: Blocking system calls only block the calling thread.
* Preemptive scheduling: Kernel timer interrupts ensure fair CPU sharing; no thread can run forever.
* Simpler programming model: No select() wrappers or async I/O hacks required; blocking calls behave naturally.
* Better integration with OS: Kernel manages priorities, CPU affinity, real-time scheduling; signals and resource limits handled correctly.
* Scales well for I/O-bound workloads: Ideal for servers, databases, and network applications.

**Cons**

* Higher overhead than user-level threads: Creation and context switches are more expensive; kernel structures consume memory.
* Kernel resource consumption: Each thread requires kernel stack, scheduler entries, and accounting data; extreme thread counts can stress memory.
* Less application-specific control: Scheduling policy is mostly kernel-defined; custom per-application schedulers are harder.
* System call overhead: Thread operations often require kernel transitions.
* Potential scalability limits: Extremely large thread counts may degrade performance; event-driven models can outperform threads at extreme scale.

**Kernel Threads vs User Threads**
|===
| Feature | Kernel Threads | User Threads
| Kernel aware | Yes | No
| Blocking behavior | Per-thread | Whole process blocks
| Scheduling | Kernel | User runtime
| Parallel execution | Yes | No (without kernel help)
| Context switch cost | Higher | Lower
| Practical usability | High | Limited
|===

**Summary**

* Kernel-space threads are the dominant threading model in modern OSes.
* Provide safe blocking, true parallelism, and preemptive multitasking.
* Added overhead is worth the trade-off for correctness, simplicity, and scalability.

**One-sentence exam answer**

* Kernel-space threads are managed by the OS kernel, allowing preemptive scheduling and per-thread blocking at the cost of higher creation and context-switch overhead.

*Challanges*

* *Kernel Threads and fork()*
** what happens to threads when a process forks?
*** In a single-threaded process, fork():
*** Duplicates the process
*** Child process gets a copy of the parent‚Äôs address space
*** One thread exists in child (the thread that called fork())

** In a multi-threaded process with kernel threads:
*** Parent has multiple kernel threads
*** When fork() is called:
*** Only the calling thread is usually duplicated in the child
*** Other threads do not exist in the child
* In many cases, the best choice depends on what the process is planning to do next.
** only one thread?
*** Simplicity:
**** Child process has a clean start with a single thread.
**** Prevents unexpected race conditions or resource conflicts.
*** Compatibility with exec():
**** If child is going to call exec() to start a new program:
***** Only the calling thread matters
***** Other threads would be irrelevant and waste resources

** reproducing all threads
*** If the child process continues executing the same program:
**** Some applications might need all threads duplicated
**** For example, a server forking workers that continue the same workload

*** However, duplicating all threads:
**** Adds complexity
**** Requires careful handling of:
**** Thread stacks
**** Mutexes/locks
**** File descriptors in consistent state

** Kernel threads solve blocking and preemption problems, but they do not automatically solve process creation semantics.
*** Forking a multi-threaded process is non-trivial, and OS defaults:
**** Child starts with only one thread
**** Parent threads are not copied

*** Behavior can depend on:
**** Intent of the child process (call exec() vs continue executing)
**** Application requirements
** in Linux
*** Linux (NPTL) behavior:
**** fork() duplicates calling thread only
**** File descriptors, memory mappings are copied
**** Other threads do not continue in child
*** POSIX recommends: After fork in a multithreaded process, only async-signal-safe functions should be called in the child before exec.


=== Implementing Threads in hybrid Space
Various ways have been investigated to try to combine the advantages of user-
level threads with kernel-level threads. One way is use kernel-level threads and
then multiplex user-level threads onto some or all of them.
When this approach is used, the programmer can determine how many kernel
threads to use and how many user-level threads to multiplex on each one. This
model gives the ultimate flexibility.

With this approach, the kernel is aware of only the kernel-level threads and
thus schedules those. Some of those threads may have multiple user-level threads
multiplexed on top of them. These user-level threads are created, destroyed, and
scheduled just like user-level threads in a process that runs on an operating system
without multithreading capability. In this model, each kernel-level thread has some
set of user-level threads that take turns using it.

**Two-level scheduling overview (Hybrid M:N model)**
[width="100%",subs="verbatim,quotes"]
----
+------------------------------------------------------+
|                    User Space                        |
|                                                      |
|  User Thread U1   User Thread U2   User Thread U3    |
|        |                 |                 |         |
|        +--------+--------+--------+--------+         |
|                 |                 |                  |
|          User-Level Thread Scheduler (Runtime)       |
|                 |                 |                  |
+-----------------+-----------------+------------------+
                  |                 |
                  v                 v
+-----------------+-----------------+------------------+
|                    Kernel Space                      |
|                                                      |
|         Kernel Thread K1       Kernel Thread K2      |
|                 |                     |              |
|                 +----------+----------+              |
|                            |                         |
|                    Kernel Scheduler                  |
|                            |                         |
|                         CPU Core(s)                  |
+------------------------------------------------------+

**User-level scheduling detail**

+---------------- Kernel Thread K1 ----------------+
|                                                  |
|   U1  --->  U2  --->  U3  --->  U1 (time slice)    |
|                                                   |
| * Context switches occur in user space           |
| * No kernel involvement                          |
| * Very fast                                      |
+---------------------------------------------------+

**Kernel-level scheduling detail**

+------------------- CPU -------------------+
|                                           |
|   K1  <---->  K2  <---->  K3  (preempted)  |
|                                           |
| * Timer interrupts                       |
| * Blocking system calls                  |
| * Page faults                            |
+-------------------------------------------+

**Blocking system call behavior**

User Thread U2 makes read():

U2
 |
 v
K1  ---->  [ BLOCKED in kernel ]
 |
 |  (runtime schedules another user thread)
 v
K2  ---->  U3 continues execution

Result:
* Only K1 is blocked
* Other kernel threads continue running
* User threads are rescheduled on available kernel threads

**Page fault behavior**

User Thread U1
 |
 v
K1  ---->  [ PAGE FAULT ]
 |
 |  (kernel suspends K1)
 v
K2  ---->  U2 runs

Effect:
* Fault affects only one kernel thread
* Process does not stall
* Runtime maintains forward progress

**Signal delivery path**

Signal
  |
  v
Kernel chooses a kernel thread (K1 or K2)
  |
  v
Runtime maps signal to:
* A dedicated signal-handling user thread
* Or the currently running user thread

Risk:
* Requires careful signal masking
* Incorrect mapping causes race conditions

**Comparison with pure models (visual intuition)**

User-level only (1:1 illusion)
+-----------------------------+
| U1 -> U2 -> U3 -> U4        |
| One kernel thread           |
| Blocking stops everything   |
+-----------------------------+

Kernel-level only (1:1 model)
+-----------------------------+
| U1 -> K1 -> CPU             |
| U2 -> K2 -> CPU             |
| Simple, predictable         |
+-----------------------------+

Hybrid (M:N model)
+-----------------------------+
| U1 U2 U3 U4                 |
|  |  |  |                   |
| K1     K2                   |
|  |       |                 |
| CPU     CPU                 |
+-----------------------------+

**Key intuition**

* Kernel schedules kernel threads.
* Runtime schedules user threads.
* Two schedulers cooperate but do not share intent.
* Power comes from flexibility; danger comes from complexity.

**What hybrid threads are**

* Hybrid threading combines user-level threads and kernel-level threads.
* M user-level threads are multiplexed onto N kernel threads (M ‚â• N).
* The kernel schedules kernel threads.
* A user-space runtime schedules user threads on top of kernel threads.
----

**How it works**

* The process creates:
** Multiple user-level threads managed by a runtime library
** A smaller number of kernel threads created via clone()
* Each kernel thread acts as an execution vehicle for user threads.
* The runtime decides:
** Which user thread runs
** On which kernel thread
* Kernel sees only kernel threads; user threads are invisible to it.

**Scheduling model**

* Two-level scheduling:
** Kernel scheduler:
*** Schedules kernel threads preemptively across CPUs
** User-level scheduler:
*** Schedules user threads cooperatively or preemptively on kernel threads
* User threads can run in parallel as long as enough kernel threads exist.

**Context switching**

* **User-level context switch**

** Happens entirely in user space.
** Switches between user threads on the same kernel thread.
** Saves/restores registers and user stack pointer.
** Very fast.
* **Kernel-level context switch**

** Happens in kernel mode.
** Switches between kernel threads.
** Triggered by:
*** Timer interrupts
*** Blocking system calls
*** Page faults
* A scheduling event may involve:
** One user-level switch
** One kernel-level switch
** Or both

**Blocking behavior**

* Blocking system calls:
** Block only the kernel thread that made the call.
** Other kernel threads continue running.
** Runtime schedules other user threads on remaining kernel threads.
* Page faults:
** Block only the affected kernel thread.
** Other user threads continue executing.
* This avoids the ‚Äúall threads stop‚Äù problem of pure user-level threads.

**Memory management**

* All threads share the same process address space.
* User threads:
** Have stacks allocated in user-space memory.
** Managed by the runtime.
* Kernel threads:
** Have kernel stacks.
** Managed by the kernel.
* Runtime maintains mappings between:
** User thread stacks
** Kernel thread execution contexts

**Synchronization**

* Uncontended locks handled entirely in user space.
* Contended locks use kernel assistance (e.g., futex).
* Combines:
** Low overhead of user-level locks
** Correct blocking semantics of kernel-level locks

**Signal handling**

* Signals are delivered to kernel threads.
* Runtime must map signal handling to user threads.
* Often requires:
** Dedicated signal-handling kernel thread
** Careful signal masking
* Signal handling is more complex than in pure kernel-thread models.

**fork() semantics**

* fork() duplicates only the calling kernel thread.
* User-thread state is copied but inactive in the child.
* Runtime must:
** Reinitialize scheduling structures
** Decide whether to recreate user threads
* fork() followed by exec() is the safest pattern.

**Pros**

* Avoids process-wide blocking on I/O.
* Allows many user threads with fewer kernel threads.
* Faster user-level context switching when uncontended.
* Better scalability than pure kernel threads in some workloads.
* Combines flexibility of user-level threads with correctness of kernel threads.

**Cons**

* Very complex runtime implementation.
* Two-level scheduling overhead.
* Harder to debug due to M:N mapping.
* Signal handling and fork() semantics are difficult.
* Kernel and runtime scheduling decisions can conflict.

**When hybrid threads make sense**

* Applications with:
** Very large numbers of concurrent tasks
** Frequent blocking operations
** Limited kernel thread resources
* Runtime-managed environments (e.g., language runtimes).

**When hybrid threads are avoided**

* General-purpose system programming.
* Simpler server architectures.
* Applications requiring predictable scheduling and debugging.

**Real-world examples**

* Solaris threads (classic M:N model).
* Early Linux threading experiments.
* Modern language runtimes:
** Go (goroutines)
** Erlang (lightweight processes)
** JVM virtual threads (conceptually similar, but evolved)

**Summary**

* Hybrid threads aim to get the best of user-level and kernel-level threading.
* They solve blocking problems while keeping context switches fast.
* The cost is complexity, especially for signals and fork().
* Modern OSes prefer 1:1 kernel threads unless a language runtime manages the model carefully.

**One-sentence exam answer**

* Hybrid threads multiplex many user-level threads over fewer kernel threads, combining fast user-space scheduling with kernel-level blocking support at the cost of significant implementation complexity.


=== Comparision of User space and Kernel Space and Hybrid
User-level threads are ideal for workloads where threads mostly cooperate in-process and rarely block on kernel resources.

Kernel involvement is needed only for things like:

I/O that cannot be handled in user-space

True parallelism across cores

Many modern threading libraries use a hybrid approach:

Fast user-space context switching

Falling back on kernel threads (clone + futex) when necessary

**Thread Lifecycle: User-Level vs Kernel Threads vs Hybrid Threads (Side-by-Side Flow)**

[cols="1,1,1,1", options="header"]
|===
| | User-Level Threads | Kernel Threads | Hybrid Threads (M:N)

| **Creation** 
a|
* Allocated in user-space runtime library
* Registers, stack, thread control block in process memory
* No kernel involvement
a|
* Kernel allocates task_struct and kernel stack
* clone() syscall creates new thread
* Thread visible to kernel scheduler
a|
* Runtime library creates user-level threads (M)
* Runtime maps threads onto N kernel threads via clone()
* Kernel sees only N threads; runtime multiplexes M threads onto them

| **Ready-to-Run** 
a|
* Runtime adds thread to **user-level run queue**

* Scheduler selects next runnable thread cooperatively
* Other threads **cannot run** until current thread yields
a|
* Kernel scheduler adds thread to run queue
* Threads scheduled preemptively
* Multiple threads can run on multiple cores
a|
* User-level threads queued in runtime scheduler
* Kernel threads scheduled preemptively
* Runtime decides which user thread runs on which kernel thread

| **Context Switching**
a|
* Happens entirely in user space
* Saves CPU registers, stack pointer, program counter
* Very fast
* Only occurs when thread yields or blocks on user-level lock
a|
* Happens in kernel mode
* Saves CPU registers, program counter, kernel stack pointer
* Preemptive: triggered by timer, syscall, or explicit yield
a|
* Two levels:
** User-level switch: runtime switches threads on same kernel thread
** Kernel-level switch: one kernel thread to another
* Fast for user-level switch, slower if kernel thread needs to switch

| **Blocking / System Calls**
a|
* Blocking syscall blocks **entire process**

* Wrappers or nonblocking I/O needed
* Page faults block all threads
a|
* Blocking syscall blocks only calling thread
* Kernel scheduler runs other threads
* Page faults block only the faulting thread
a|
* Blocking syscall blocks **only kernel thread**

* Runtime can schedule other user threads on **different kernel threads**

* Page faults block only the kernel thread; other user threads continue

| **Memory Management**
a|
* All threads share process address space
* Stack and registers in user-space heap
* Runtime tracks stacks
a|
* All threads share process address space
* Kernel allocates kernel stack and thread control block
* Kernel handles memory protection
a|
* User-level threads: stacks in user-space heap
* Kernel threads: kernel stack per thread
* Runtime manages mapping M:N and user stacks

| **Synchronization**
a|
* Implemented entirely in user space (mutex, rwlock, barrier)
* Busy-wait or cooperative blocking
a|
* Kernel provides futex-based primitives
* Mutex, condvars, rwlocks handled by kernel
a|
* User-level synchronization for M threads
* Kernel synchronization only when M threads block on N kernel threads
* Uses combination of futex + runtime locks

| **Termination / Cleanup**
a|
* Thread exits via runtime function
* Runtime frees stack and control block
* Kernel unaware
a|
* Thread exits via syscall
* Kernel frees kernel stack, task_struct, scheduler data
* Other threads continue
a|
* User thread exits: runtime schedules another user thread on same kernel thread
* Kernel thread exits: kernel frees resources
* Runtime cleans user-level resources

| **fork() Behavior**
a|
* fork() duplicates the process once
* Runtime thread tables copied blindly
* Scheduler state may be inconsistent
* Rarely used safely in practice
a|
* Only the calling thread is duplicated
* Other threads do not exist in child
* Kernel copies address space and file descriptors
a|
* Only kernel threads are visible to fork()
* Child process contains:
** One kernel thread
** Runtime thread state copied but inactive
* Runtime must reinitialize thread system

| **fork() + exec()**
a|
* Possible but unsafe without runtime cooperation
* Thread library state may be corrupted
a|
* Intended use case
* Child calls exec() immediately
* POSIX-recommended pattern
a|
* Safe and common
* Runtime is replaced during exec()

| **Continuing Execution After fork()**
a|
* Extremely difficult
* Thread state consistency not guaranteed
a|
* Possible but limited
* Only one thread exists in child
a|
* Complex
* Runtime must reconstruct scheduling state
* Often avoided in real systems

| **Pros / Cons Summary**
a|
* Pros: Extremely fast, customizable scheduling, low kernel overhead
* Cons: Blocking syscalls and page faults halt all threads, cooperative scheduling, complex runtime
a|
* Pros: Preemptive, safe blocking, true parallelism, kernel scheduling, multi-core
* Cons: Slower context switch, more memory overhead, kernel involvement required
a|
* Pros: Combines user-level speed with kernel-level blocking handling
* Can multiplex many threads over few kernel threads
* Cons: More complex runtime
* Context switching may involve two layers
* Harder debugging due to M:N mapping
|===

**visual ASCII timeline showing how threads run over time,**

**Thread Execution Timeline: User-Level vs Kernel vs Hybrid Threads**

[width="100%",subs="verbatim,quotes"]
----
Time ‚Üí 
| User-Level Threads       | Kernel Threads             | Hybrid Threads (M:N)
| T1 running               | K1 running                | U1 on K1 running
| T1 runs long computation | K1 runs, K2 preempted     | U2 on K2 running
| T1 calls blocking read ‚Üí **ALL STOP** | K1 blocks on read ‚Üí K2 runs | U1 blocks on I/O ‚Üí U2 runs on K1, U3 runs on K2
| No other thread runs     | Other threads continue    | Runtime schedules another U-thread on free kernel thread
| T1 unblocks manually     | K1 unblocked, scheduled   | User thread resumes on its kernel thread
| Context switch happens only if T1 yields | Preemptive context switch possible | Two-level context switch (user runtime + kernel scheduler)
| T2 runs                  | K2 runs                   | U3 runs
| T2 calls blocking read ‚Üí **ALL STOP** | Blocking handled per-thread | Blocking only affects the kernel thread; runtime schedules other user threads
| T3 runs                  | K1 runs                   | U4 runs on K1
| ...                       | ...                        | ...
| Threads exit and free stacks | Kernel frees task_struct | User threads freed by runtime; kernel threads freed by kernel


**Legend:**

* **T1, T2, T3** ‚Üí user-level threads
* **K1, K2** ‚Üí kernel threads
* **U1, U2, U3, U4** ‚Üí user threads in hybrid model
* **Bold ‚ÄúALL STOP‚Äù** ‚Üí entire process blocked
* Horizontal flow ‚Üí time progresses left to right
* Context switches:
  - User-level: runtime scheduler
  - Kernel: preemptive timer interrupt or blocking syscall
  - Hybrid: two-level switch (runtime + kernel)
----
From Diagram

* User-Level Threads
** If a thread blocks (I/O or page fault), all threads stop.
** Context switches occur only when a thread yields voluntarily.

* Kernel Threads
** Each thread can block independently.
** Preemptive timer ensures fair scheduling.
** Multiple threads can run in parallel on different cores.

** Hybrid Threads (M:N)
** Many user threads share a smaller number of kernel threads.
** Blocking a user thread only blocks its kernel thread.
** Runtime schedules other user threads on free kernel threads.
** Context switches can occur at both user and kernel levels.

*CPU cores explicitly, with User-Level, Kernel, and Hybrid threads, including parallel execution, blocking, and context switching.*
**Thread Execution Timeline on Multiple CPU Cores**

[width="100%",subs="verbatim,quotes"]
----
Time ‚Üí 
CPU Core 1                       | CPU Core 2
---------------------------------|------------------
**User-Level Threads**  
T1 running                       | T2 waiting (cannot run, same process)
T1 running                       | T2 waiting
T1 blocks on read ‚Üí **ALL STOP** | T2 stops too
T1 resumes after yield           | T2 runs after T1 yields

**Kernel Threads**  
K1 running                       | K2 running
K1 running                       | K2 running
K1 blocks on read                | K2 continues running
K1 unblocked                     | K2 continues running
K1 runs again                    | K2 runs another thread

**Hybrid Threads (M:N)**  
U1 on K1 running                 | U2 on K2 running
U3 on K1 running                 | U4 on K2 running
U1 blocks on I/O                 | U2 continues running
U3 scheduled on K1               | U4 continues on K2
U1 resumes on K1                 | U2 continues running
U5 scheduled on K1               | U6 scheduled on K2
U3 yields                        | U4 yields
U7 scheduled on K1               | U8 scheduled on K2
----
**Legend:**

* **T1, T2** ‚Üí User-level threads
* **K1, K2** ‚Üí Kernel threads
* **U1‚ÄìU8** ‚Üí User threads in Hybrid model
* **Bold ‚ÄúALL STOP‚Äù** ‚Üí entire process blocked in user-level threading
* CPU Core columns show **parallel execution**

* Blocking behavior differs:
  - User-level: blocks entire process
  - Kernel: blocks only the calling thread
  - Hybrid: blocks only the kernel thread; runtime schedules other user threads
* Context switching:
  - User-level: runtime scheduler (cooperative)
  - Kernel: preemptive (timer interrupts or syscalls)
  - Hybrid: two-level (user runtime + kernel scheduler)

*Highlights of This Timeline*

* User-Level Threads
** Only one thread can run per process, even on multiple cores.
** Blocking halts all threads, no parallelism.

* Kernel Threads
** Each thread can run on a separate CPU core.
** Blocking does not stop other threads.
** Preemptive timer ensures fair time sharing.

* Hybrid Threads (M:N)
** Multiple user threads can run on fewer kernel threads.
** Blocking a user thread blocks only its kernel thread.
** Runtime maps ready user threads onto free kernel threads.
** Explains two-level context switching visually.


=== Threads Challanges
Threads introduce a number of serious compli-
cations into the programming model. 

* *Fork issues*, consider the effects of the
UNIX fork system call. 
** If the parent process has multiple threads, should the child
also have them? If not, the process may not function properly, since all of them
may be essential. 
+
on a system such as Linux, a fork of a multithreaded proc-
ess will create only a single thread in the child. However, using Posix threads a
program can use the pthread atfork() call to register fork handlers (procedures that
are called when a fork occurs), so it can start additional threads and do whatever is needed to get going properly again.
** However, if the child process gets as many threads as the parent,
what happens if a thread in the parent was blocked on a read call, say, from the
keyboard? Are two threads now blocked on the keyboard, one in the parent and
one in the child? When a line is typed, do both threads get a copy of it? Only the
parent? Only the child? 
** The same problem exists with open network connections.
* Another class of problems is related to the fact that threads share many data
structures. 
** What happens if one thread closes a file while another one is still read-
ing from it? Suppose one thread notices that there is too little memory and starts
allocating more memory. Partway through, a thread switch occurs, and the new
thread also notices that there is too little memory and also starts allocating more
memory. Memory will probably be allocated twice. 

* *Kernel Threads and Signals*
** Classical UNIX model
*** Signals were designed for processes, not threads.
*** A process originally had:
**** One execution flow
**** One signal handler context
** Threads break this assumption
*** A multithreaded process has:
**** Multiple execution flows
**** Shared address space
*** When a signal arrives: Which thread should handle it?
** This question has no universally ‚Äúcorrect‚Äù answer.
** Possible Signal Handling Models
. Any thread may handle the signal
*** The operating system chooses one runnable thread to deliver the signal.
*** This is how Linux behaves by default.
*** Linux behavior
**** A signal sent to a process may be delivered to:
**** Any thread that does not block that signal
**** The kernel selects a thread (implementation-dependent, not guaranteed order)
*** Problem
**** Non-deterministic behavior
**** Hard to reason about correctness
**** Different executions may choose different threads
. Threads register interest in signals
*** Each thread declares: ‚ÄúI want signal X‚Äù
*** Kernel delivers signal only to interested threads
*** Issue
**** Adds complexity to kernel and threading model
**** What if multiple threads want the same signal?
**** OS must still choose one ‚Üí ambiguity remains
. Block signal in all threads except one (common practice)
*** Linux / POSIX solution
**** Use pthread_sigmask():
**** Block signal in all threads
**** Leave one ‚Äúsignal-handling thread‚Äù unblocked
*** Result
**** Signals always go to a known thread
**** Behavior becomes deterministic
*** This is the recommended design pattern

** **Why Signals + Threads Are Dangerous**

***  **Asynchronous execution**

**** Signal handlers interrupt execution at arbitrary points.
**** A handler may run while:
*** A lock is held
*** A data structure is inconsistent
***  **Limited safe operations**

**** In signal handlers, only async-signal-safe functions are allowed.
**** Many common operations are unsafe:
***** malloc()
***** printf()
***** Locking primitives
***  **Interaction with thread synchronization**

**** A signal handler running in a thread that:
***** Holds a mutex
***** Attempts to lock the same mutex
**** This can lead to deadlock.

** **Why This Is a Thread-Induced Problem**

***  **In single-threaded processes**

**** Signal delivery is simple.
**** Only one execution context exists.
***  **In multithreaded processes**

**** Signal delivery interacts with:
***** Scheduling
***** Locks
***** Memory consistency
**** The programmer must manually impose discipline.

** **Practical Reality in Linux**

*** Signals are still process-directed.
*** Signals are delivered to threads based on:
**** Signal masks
**** Thread state
*** The kernel does not know application intent.
*** Therefore: Signal handling correctness is the programmer‚Äôs responsibility.

.From AI
[%collapsible]
======
[tabs]
=====
ChatGPT::
[tabs]
====
ASCII diagram comparing process vs thread::
----
==========================
SINGLE-THREADED PROCESS
==========================

+--------------------------------------------------+
|                    PROCESS                       |
|--------------------------------------------------|
|  Resources (owned by the process):               |
|                                                  |
|   ‚Ä¢ Virtual address space (code, heap, globals)  |
|   ‚Ä¢ Open files                                   |
|   ‚Ä¢ Sockets                                      |
|   ‚Ä¢ Permissions / UID                            |
|   ‚Ä¢ Working directory                            |
|                                                  |
|--------------------------------------------------|
|  Execution (only one):                           |
|                                                  |
|   Thread                                          |
|    ‚Ä¢ Program Counter (PC)                        |
|    ‚Ä¢ CPU registers                               |
|    ‚Ä¢ Stack (function calls, locals)              |
|                                                  |
+--------------------------------------------------+



=====================================
MULTI-THREADED PROCESS (CLASSICAL)
=====================================

+--------------------------------------------------+
|                    PROCESS                       |
|--------------------------------------------------|
|  Shared Resources (one copy):                    |
|                                                  |
|   ‚Ä¢ Virtual address space                        |
|     - Code                                      |
|     - Heap                                      |
|     - Globals                                   |
|   ‚Ä¢ Open files                                  |
|   ‚Ä¢ Sockets                                     |
|   ‚Ä¢ Permissions                                 |
|                                                  |
|--------------------------------------------------|
|  Multiple Executions (threads):                  |
|                                                  |
|   +-------------------+   +-------------------+ |
|   |     Thread 1      |   |     Thread 2      | |
|   |-------------------|   |-------------------| |
|   | PC                |   | PC                | |
|   | Registers         |   | Registers         | |
|   | Stack             |   | Stack             | |
|   +-------------------+   +-------------------+ |
|                                                  |
|   +-------------------+                          |
|   |     Thread 3      |                          |
|   |-------------------|                          |
|   | PC                |                          |
|   | Registers         |                          |
|   | Stack             |                          |
|   +-------------------+                          |
|                                                  |
+--------------------------------------------------+



==========================
MULTIPLE PROCESSES
==========================

+----------------------+    +----------------------+
|       PROCESS A      |    |       PROCESS B      |
|----------------------|    |----------------------|
|  Memory              |    |  Memory              |
|  Open files          |    |  Open files          |
|  Sockets             |    |  Sockets             |
|----------------------|    |----------------------|
|  Thread A1           |    |  Thread B1           |
|   ‚Ä¢ PC               |    |   ‚Ä¢ PC               |
|   ‚Ä¢ Registers        |    |   ‚Ä¢ Registers        |
|   ‚Ä¢ Stack            |    |   ‚Ä¢ Stack            |
+----------------------+    +----------------------+
----
+
Key visual takeaways
+
* Process = resource container
* Thread = execution path
* Threads:
** Share memory and files
** Have their own PC, registers, and stack
* Processes:
** Do not share memory by default
** Communicate via IPC instead
+
Mental shortcut
+
----
Process  = "What I own"
Thread   = "What I am doing"
----

CPU + RAM interaction during scheduling::
+
1. Big picture: CPU ‚Üî RAM during execution
----
                 +=====================+
                 |         CPU         |
                 |---------------------|
                 |  Registers          |
                 |   ‚Ä¢ PC              |
                 |   ‚Ä¢ SP              |
                 |   ‚Ä¢ General regs    |
                 |   ‚Ä¢ Flags           |
                 +----------^----------+
                            |
                     context switch
                            |
                 +----------v----------+
                 |         RAM         |
                 |---------------------|
                 |  Process resources  |
                 |  +----------------+ |
                 |  | Code           | |
                 |  | Heap           | |
                 |  | Globals        | |
                 |  +----------------+ |
                 |                    |
                 |  Per-thread stacks |
                 |  +--------------+  |
                 |  | Stack T1     |  |
                 |  | Stack T2     |  |
                 |  +--------------+  |
                 +--------------------+

----
* CPU: only holds one thread‚Äôs state at a time
* RAM: holds everything persistent
* Scheduling = saving CPU state ‚Üí RAM, loading another thread‚Äôs state ‚Üê RAM
+
2. What exactly is saved during a context switch?
+
----
CPU (running Thread A)
---------------------
 PC  ‚îÄ‚îÄ‚îê
 SP  ‚îÄ‚îÄ‚îº‚îÄ‚îÄ saved into RAM
 R0..Rn‚îò
 FLAGS

RAM
------------------------------------
 Thread A control block:
   ‚Ä¢ saved PC
   ‚Ä¢ saved registers
   ‚Ä¢ saved SP
----
+
Then:
----
RAM
------------------------------------
 Thread B control block:
   ‚Ä¢ saved PC
   ‚Ä¢ saved registers
   ‚Ä¢ saved SP
         ‚îÇ
         ‚ñº
CPU loads Thread B state
----
+
3. Thread scheduling (same process)
+
Only execution state changes
----
Before switch:
--------------
CPU  ‚Üí Thread 1
RAM  ‚Üí One address space

After switch:
-------------
CPU  ‚Üí Thread 2
RAM  ‚Üí SAME address space
----
+
ASCII view:
+
----
PROCESS (one address space in RAM)
+------------------------------------------------+
| Code | Heap | Globals                           |
|----------------------------------------------- |
| Stack T1        Stack T2                        |
|   ‚Üë                ‚Üë                           |
| CPU using this    CPU will use this            |
| stack now         stack after switch           |
+------------------------------------------------+
----
+
* Cost
* Save/restore registers
* Change stack pointer
* No page table switch->Fast
+
4. Process scheduling (different processes)
+
Execution + memory context changes
+
----
Before switch:
--------------
CPU ‚Üí Process A, Thread A1
MMU ‚Üí Page Table A

After switch:
-------------
CPU ‚Üí Process B, Thread B1
MMU ‚Üí Page Table B
----
+
ASCII view:
----
RAM
+--------------------+    +--------------------+
|     PROCESS A      |    |     PROCESS B      |
|--------------------|    |--------------------|
| Code               |    | Code               |
| Heap               |    | Heap               |
| Stack A1           |    | Stack B1           |
+--------------------+    +--------------------+

CPU
+--------------------+
| PC, regs, SP       |
+--------------------+

MMU
+--------------------+
| Page Table Pointer |
|  (A ‚Üí B switch)    |
+--------------------+
----
Cost
+
* Save/restore registers
* Switch page tables
* Flush / reload TLB -> Slower than thread switch
+
5. Scheduler‚Äôs view (simplified)
----
RUN QUEUE (in RAM)
----------------------------------
| Thread A1 | Thread A2 | Thread B1 |
----------------------------------
        ^                     ^
   same process          different process
----
+
Scheduler decision:
+
* Pick next runnable thread
* Save current CPU state
* Load next CPU state
* If process differs ‚Üí switch memory context
+
6. Linux reality (ties to your earlier question)
+
In Linux:
+
* Both processes and threads are task_struct
* Difference is What they share
** Memory (mm_struct)
** Files (files_struct)
** Signals, etc.
+
----
task_struct
  |
  +-- registers (saved on switch)
  +-- stack pointer
  +-- mm_struct   (shared by threads)

----
So scheduling logic is mostly identical; the cost difference comes from Whether mm_struct changes or not
+
7. One-line intuition
+
----
Thread switch  = change "who runs"
Process switch = change "who runs" + "what memory they see"
----

Mapping to task_struct, mm_struct, and thread_info::
1. High-level correspondence
----
CONCEPTUAL MODEL        LINUX STRUCTURE
------------------------------------------------
Process (resources)  ‚Üí  mm_struct
Thread (execution)   ‚Üí  task_struct
Per-CPU saved state  ‚Üí  task_struct + kernel stack
Low-level metadata   ‚Üí  thread_info (arch-specific)
----
Linux does not have a separate ‚Äúthread object‚Äù, A thread is a task_struct.
+
2. RAM layout during scheduling (Linux view)
----
RAM
================================================================================

+-----------------------------+
| task_struct (Thread A1)     |  ‚Üê schedulable entity
|-----------------------------|
| state                       |
| pid / tid                   |
| prio / policy               |
| se (CFS entity)             |
| mm  --------------------+--|----+
| files                     |      |
| fs                        |      |
| signal                    |      |
| thread                    |      | execution context
+-----------------------------+      |
                                      |
+-----------------------------+      |
| Kernel stack (Thread A1)    |      |
|-----------------------------|      |
| pt_regs (saved CPU regs)   |‚óÑ-----+
| call frames                |
+-----------------------------+

+-----------------------------+
| mm_struct (Process A)       |  ‚Üê resource container
|-----------------------------|
| pgd (page tables)           |
| mmap (VMA list)             |
| start_code / end_code       |
| start_brk (heap)            |
| arg/env                     |
| mm_users                    |
+-----------------------------+
----
+
All threads in the same process point to the SAME mm_struct
+
3. task_struct: the thread + scheduler unit
+
What it represents: Exactly one schedulable execution context
----
task_struct
------------------------------------------------
Scheduling:
  state
  prio
  policy
  cpu
  se              (CFS scheduling entity)

Identity:
  pid   (thread ID)
  tgid  (process ID)

Resource links:
  mm        ‚Üí mm_struct (shared by threads)
  active_mm
  files
  fs
  signal

Execution:
  thread    (arch-specific CPU state)
  stack     (kernel stack base)
-----
Mapping to the diagram
+
----
CPU registers  ‚Üê‚Üí task_struct.thread
Stack pointer ‚Üê‚Üí kernel stack (task stack)
PC             ‚Üê‚Üí pt_regs on kernel stack
----
+
4. mm_struct: the process resource 
+
What it represents: Everything related to memory visibility
----
mm_struct
------------------------------------------------
pgd              ‚Üí page tables (MMU root)
mmap             ‚Üí list of VMAs
mmap_lock
start_code
end_code
start_brk        ‚Üí heap
arg_start/env_start
rss stats
----
Mapping to the diagram
----
"Process memory" in RAM
   ‚îú‚îÄ Code
   ‚îú‚îÄ Heap
   ‚îú‚îÄ Globals
   ‚îî‚îÄ VMAs
        ‚Üë
        mm_struct
----
Thread vs process switch
----
Thread switch:
  next->mm == prev->mm
  ‚Üí NO page table switch

Process switch:
  next->mm != prev->mm
  ‚Üí load next->mm->pgd into CR3
----
5. thread_info: lowest-level execution metadata
+
What it is (modern Linux)
+
* Arch-specific
* Mostly merged into task_struct
* Historically at bottom of kernel stack
----
thread_info
-------------------------------------
flags        (TIF_NEED_RESCHED, etc.)
preempt_count
cpu
task ‚Üí task_struct
----
Mapping
----
CPU interrupt / syscall entry
        ‚Üì
thread_info flags
        ‚Üì
scheduler decision
----
You can think of it as: ‚ÄúMinimal metadata the kernel needs immediately on entry‚Äù
+
6. Context switch: exact field mapping
+
Step-by-step (timer interrupt)
+
----
1. CPU interrupt
   ‚Üí switch to kernel mode
   ‚Üí current kernel stack

2. SAVE CPU STATE
   ‚Üí pt_regs pushed on kernel stack

3. current = this_cpu_current_task

4. pick next task_struct

5. context_switch(prev, next)
----
What gets switched
----
SAVE (prev task_struct):
  prev->thread.sp
  prev->thread.ip
  prev->thread.regs (arch-dependent)

LOAD (next task_struct):
  next->thread.sp
  next->thread.ip
  next->thread.regs
----
Memory context
----
if prev->mm != next->mm:
    load next->mm->pgd into CR3
----
7. Visual: thread vs process switch in Linux terms
----
THREAD SWITCH
------------------------------------------------
task_struct A1  ‚Üí task_struct A2
mm_struct A     ‚Üí mm_struct A   (same)

PROCESS SWITCH
------------------------------------------------
task_struct A1  ‚Üí task_struct B1
mm_struct A     ‚Üí mm_struct B
----
8. One-screen mental map
+
----
CPU
 ‚îú‚îÄ Registers
 ‚îú‚îÄ PC
 ‚îî‚îÄ SP
        ‚Üì save/restore
Kernel Stack
 ‚îî‚îÄ pt_regs
        ‚Üì owned by
task_struct  ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ scheduler operates here
        ‚Üì
mm_struct    ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ defines memory view
----
9. Key insight (this is exam-gold)
+
* Linux schedules task_struct, not processes
* A process is just a group of tasks sharing one mm_struct

Compare threads vs processes in context switching cost::
1. What a context switch really costs
+
A context switch cost comes from what must be saved and restored and what must be changed in the CPU/MMU.
+
At minimum, every switch must:
+
* Save CPU registers
* Save stack pointer
* Load another thread‚Äôs registers
* Update scheduler metadata
+
The extra cost depends on whether the memory context changes.
+
2. Thread vs Process switch (Linux)
----
+----------------------+----------------------+----------------------+
|                      | THREAD SWITCH       | PROCESS SWITCH      |
+----------------------+----------------------+----------------------+
| Schedulable unit     | task_struct         | task_struct         |
| CPU registers        | Save / Restore      | Save / Restore      |
| Kernel stack         | Switch              | Switch              |
| mm_struct            | SAME                | DIFFERENT           |
| Page tables (CR3)    | NO CHANGE           | CHANGE              |
| TLB                  | KEPT                | FLUSH / RELOAD      |
| Cache locality       | Preserved           | Often lost          |
| Syscall cost         | Low                 | Higher              |
| Relative cost        | FAST                | SLOWER              |
+----------------------+----------------------+----------------------+
----
Key point: Linux does the same scheduling work ‚Äî the difference is in MMU work.
+
3. CPU-level view
+
Thread switch
----
CPU
------------------------------------------------
Save registers ‚Üí Load registers
SP ‚Üí different kernel stack
CR3 unchanged
TLB unchanged
---
Minimal disruption.
+
Process switch
----
CPU
------------------------------------------------
Save registers ‚Üí Load registers
SP ‚Üí different kernel stack
CR3 ‚Üí load new page table root
TLB ‚Üí flushed or partially invalidated
----
Much heavier.
+
4. Why MMU work dominates cost
+
Page table switch (CR3)
+
* Changes entire virtual address space
* Forces the CPU to:
** Invalidate TLB entries
** Reload translations from memory
* Can cost hundreds to thousands of cycles
+
Threads skip this completely.
+
5. Cache effects (often ignored, but huge)
+
Thread switch
+
* Same code
* Same data
* Same heap
+
‚û°Ô∏è CPU caches stay warm
+
Process switch
+
* Different code
* Different data
* Different memory layout
+
‚û°Ô∏è Cache misses increase
+
This is why process switches feel slower even when raw instruction counts look similar.
+
6. Rough order-of-magnitude costs (typical x86-64)
+
(Not exact ‚Äî for intuition only)
+
----
Thread context switch   ‚âà 100‚Äì300 cycles
Process context switch  ‚âà 1,000‚Äì5,000+ cycles
----
The gap widens under:
+
* Heavy memory use
* Large working sets
* Frequent switches
+
7. Scheduler‚Äôs actual decision point
+
Linux scheduler logic:
----
pick next task_struct

if prev->mm == next->mm:
    fast path (thread switch)
else:
    slow path (process switch)
----
So threads are cheaper by design, not by special scheduling rules.
+
8. Practical consequences
+
Use threads when:
+
* Tasks share data heavily
* Low latency matters
* High throughput is needed
+
Examples:
+
* Web servers
* Databases
* GUI applications
+
Use processes when:
+
* Isolation matters
* Fault containment is required
* Security boundaries are important
+
Examples:
+
Browsers (tab isolation)

Microservices

Sandboxing

9. Exam-ready one-liner

Thread switches change execution only; process switches change execution and memory.
Memory changes dominate context-switch cost.

ASCII reference sheet::
----
================================================================================
LINUX PROCESS / THREAD / CPU / RAM / SCHEDULING ‚Äî ASCII REFERENCE MODEL
================================================================================


[ CPU ]
------------------------------------------------
| PC | SP | FLAGS | RAX RBX RCX ... | CR3 |
------------------------------------------------
                 |
                 | timer interrupt / syscall
                 v


[ KERNEL STACK  (per task) ]
------------------------------------------------
| pt_regs                                       |
|   r15 r14 r13 r12 rbp rbx                     |
|   r11 r10 r9  r8  rax rcx rdx rsi rdi         |
|   orig_rax                                    |
|   rip cs eflags rsp ss                       |
------------------------------------------------
| scheduler frames                              |
------------------------------------------------
| bottom of kernel stack                        |
------------------------------------------------
                 |
                 v


[ task_struct  (ONE PER THREAD) ]
--------------------------------------------------------------------------------
| state            (RUNNING / SLEEPING / ...)                                   |
| pid   (TID)                                                                   |
| tgid  (PID = process id)                                                      |
| cpu                                                                             |
| prio / policy                                                                 |
| se                  (CFS scheduling entity)                                   |
|                                                                               |
| stack  ------------------------+                                              |
| thread (arch context)           |                                             |
|   sp                            | execution state                              |
|   ip                            |                                             |
|   saved regs                    |                                             |
|                                                                               |
| mm        --------------------+ |                                             |
| active_mm                     | |                                             |
| files                          | | resource links                              |
| fs                             | |                                             |
| signal                         | |                                             |
--------------------------------------------------------------------------------
                 |
                 | mm pointer (shared by threads)
                 v


[ mm_struct  (ONE PER PROCESS) ]
--------------------------------------------------------------------------------
| pgd               (page table root ‚Üí CR3)                                     |
| mmap              (VMA linked list)                                            |
| mmap_lock                                                                    |
| start_code  end_code                                                          |
| start_brk   brk                                                               |
| arg_start  arg_end                                                            |
| env_start  env_end                                                            |
| mm_users                                                                      |
--------------------------------------------------------------------------------
                 |
                 | defines virtual address space
                 v


[ USER VIRTUAL MEMORY (per mm_struct) ]
--------------------------------------------------------------------------------
| Code segment                                                                  |
|-------------------------------------------------------------------------------|
| Globals / BSS                                                                 |
|-------------------------------------------------------------------------------|
| Heap (brk / malloc)                                                           |
|-------------------------------------------------------------------------------|
| mmap regions                                                                  |
|-------------------------------------------------------------------------------|
| Stack (per thread, user mode)                                                 |
--------------------------------------------------------------------------------


[ thread_info  (arch / low-level) ]
--------------------------------------------------------------------------------
| flags           (TIF_NEED_RESCHED, etc.)                                       |
| preempt_count                                                               |
| cpu                                                                            |
| task  ‚Üí task_struct                                                           |
--------------------------------------------------------------------------------


[ RUN QUEUE (per CPU) ]
--------------------------------------------------------------------------------
| task_struct A1 | task_struct A2 | task_struct B1 | ...                         |
--------------------------------------------------------------------------------
      |                |                  |
      | same mm        | same mm           | different mm
      v                v                  v


[ CONTEXT SWITCH LOGIC ]
--------------------------------------------------------------------------------
save prev->thread.sp / ip / regs
load next->thread.sp / ip / regs

if prev->mm != next->mm:
    load next->mm->pgd into CR3
    flush / update TLB
--------------------------------------------------------------------------------


[ THREAD SWITCH ]
--------------------------------------------------------------------------------
task_struct A1  ‚Üí task_struct A2
mm_struct A     ‚Üí mm_struct A
(no address space change)
--------------------------------------------------------------------------------


[ PROCESS SWITCH ]
--------------------------------------------------------------------------------
task_struct A1  ‚Üí task_struct B1
mm_struct A     ‚Üí mm_struct B
(CR3 switch + TLB effects)
--------------------------------------------------------------------------------


[ CORE IDENTITIES ]
--------------------------------------------------------------------------------
thread    = task_struct
process  = set of task_struct sharing one mm_struct
scheduler operates on task_struct
memory view defined by mm_struct
--------------------------------------------------------------------------------


[ ONE-LINE TRUTH ]
--------------------------------------------------------------------------------
Process = resources
Thread  = execution
Linux schedules execution, not processes
--------------------------------------------------------------------------------

----
====

=====
======
=== Linux thread model



.From AI
[%collapsible]
======
[tabs]
=====
ChatGPT::
+
The key idea in one sentence
+
Threads help programs run faster only when they can do computation and I/O at the same time.
+
They do not help if everything is just heavy CPU work.
+
First, a few simple definitions
1. CPU-bound work
+
This means:
+
The program spends most of its time calculating
+
The CPU is busy all the time
+
Examples:
+
Encrypting a large file
+
Compressing video
+
Calculating primes
+
Matrix multiplication
+
Here, the CPU is the bottleneck.
+
2. I/O-bound work
+
This means:
+
The program spends a lot of time waiting
+
Waiting for disk, network, keyboard, or screen
+
Examples:
+
Reading files from disk
+
Downloading data from the network
+
Waiting for user input
+
Writing logs to disk
+
Here, the program often sits idle, waiting for I/O to finish.
+
3. Threads
+
Threads are like:
+
Multiple workers inside the same program, sharing memory.
+
Each thread can:
+
Run on the CPU
+
Or wait for I/O
+
What the book is saying (in plain words)
+
Threads do NOT make CPU-only programs faster.
+
Why?
+
If all threads want the CPU
+
The CPU can only execute one thing at a time (per core)
+
Threads just take turns ‚Üí no real speedup
+
Example 1: CPU-bound only (no benefit)
+
Imagine:
+
One program
+
Doing heavy math
+
No disk, no network
+
Single-thread
[CPU calculating] ‚Üí done
+
Multi-thread
[Thread A calculating]
[Thread B calculating]
[Thread C calculating]
‚Üí CPU switches between them
‚Üí Same total work
+
‚úÖ No speed improvement
‚ùå Sometimes even slower (context switching overhead)
+
Now the important part üëá
When threads DO help: computation + I/O together
Example 2: Computation + Disk I/O (big benefit)
+
Suppose a program:
+
Reads data from disk
+
Processes it
+
Writes results back to disk
+
Single-threaded program
Read from disk (CPU idle)  ‚Üê waiting
Process data (CPU busy)
Write to disk (CPU idle) ‚Üê waiting
+
CPU timeline:
+
IDLE ‚Üí BUSY ‚Üí IDLE ‚Üí BUSY ‚Üí IDLE
+
+
üö´ CPU wasted a lot of time
+
Multi-threaded program
+
Thread A:
+
Reading data from disk
+
Thread B:
+
Processing previously read data
+
Thread C:
+
Writing results to disk
+
Now the timeline looks like:
+
Thread A waiting for disk
Thread B using CPU
Thread C waiting for disk
+
+
CPU timeline:
+
BUSY ‚Üí BUSY ‚Üí BUSY
+
‚úÖ CPU is almost never idle
‚úÖ Disk and CPU work overlap
‚úÖ Program finishes faster
+
Example 3: Web server (classic example)
+
A web server:
+
Receives requests
+
Reads files
+
Sends data over the network
+
Without threads
Handle request 1 ‚Üí wait for disk
Handle request 2 ‚Üí wait for network
CPU often idle
+
With threads
Thread 1: waiting for disk
Thread 2: sending data over network
Thread 3: preparing next response
+
+
While one thread waits, another runs.
+
üöÄ Much better performance
+
Why the book says ‚Äúoverlap‚Äù
+
‚ÄúOverlap‚Äù means:
+
Doing useful work while something else is waiting
+
Specifically:
+
CPU work overlaps with I/O waiting
+
Instead of: CPU waits ‚Üí does nothing
+
Key takeaway (memorize this)
+
‚ùå Threads do not help when all work is CPU-bound
+
‚úÖ Threads help a lot when:
+
There is both computation and I/O
+
Threads allow CPU and I/O to overlap
+
One-line mental model
+
Threads are useful when your program spends time waiting.
While one thread waits, another can work.
=====

Gemini::
[tabs]
=====
ChatGPT::
+
[tabs]
====
Concept:: Concept

Concept:: Concept
====

=====
======
One way of envisaging threads is as a set of processes that share the same
virtual memory, as well as a range of other attributes. Each thread is executing the
same program code and shares the same data area and heap. However, each thread
has it own stack containing local variables and function call linkage information.

Threads can communicate with each other via the global variables that they
share. The threading API provides condition variables and mutexes, which are primi-
tives that enable the threads of a process to communicate and synchronize their
actions, in particular, their use of shared variables. 

Threads can also communicate
with one another using the IPC and synchronization mechanisms 

The primary advantages of using threads are that 

* they make it easy to share
data (via global variables) between cooperating threads and that some algorithms
transpose more naturally to a multithreaded implementation than to a multiprocess
implementation. 
* Furthermore, a multithreaded application can transparently take
advantage of the possibilities for parallel processing on multiprocessor hardware.

=== Making Single-Threaded Code Multithreaded

==== How single-threaded code becomes multithreaded
**Core idea**

* Start with a program that assumes a single flow of execution.
* Introduce multiple concurrent flows that:
** Share the same address space
** Execute independently
** Interleave unpredictably

**Step 1: Identify independent activities**

* Look for work that:
** Can run concurrently
** Does not strictly depend on immediate results from other work
* Examples:
** Web servers: accept connections, read requests, generate responses
** GUI applications: event handling, background computation
** System tools: I/O handling and computation
* Risk:
** Apparent independence may hide implicit dependencies

**Step 2: Share state between threads**

* Single-threaded code often relies on:
** Global variables
** Static buffers
** Implicit execution order
* In multithreaded code:
** These become shared memory
* Problem:
** Concurrent access leads to races

**Step 3: Introduce synchronization**

* Shared state must be protected using:
** Mutexes
** Read‚Äìwrite locks
** Semaphores
** Atomic operations
** Condition variables
* Consequence:
** Execution is no longer linear
** Progress depends on lock acquisition

**Step 4: Decide execution structure**

* Common models:
** Thread-per-task
** Thread pool
** Producer‚Äìconsumer
** Pipeline
* Design choice affects:
** Performance
** Memory usage
** Complexity
** Debuggability
* Poor design example:
** Creating one thread per request causes excessive overhead

**Step 5: Handle blocking operations**

* Single-threaded assumption:
** Blocking is acceptable
* Multithreaded reality:
** Blocking may stall other threads
** Locks held during blocking cause system-wide delays
* Common mistake:
** Performing blocking I/O while holding a mutex

**Step 6: Make termination and cleanup safe**

* Threads may still:
** Hold locks
** Access freed memory
** Use closed file descriptors
* Requires:
** Thread joining
** Cancellation handling
** Clear resource ownership rules
 
==== Fundamental problems introduced

===== Problem of: Race conditions

The code of a thread normally consists of multiple procedures, just
like a process. These may have local variables, global variables, and parameters.
Local variables and parameters do not cause any trouble, but variables that are glo-
bal to a thread but not global to the entire program are a problem. These are vari-
ables that are global in the sense that many procedures within the thread use them
(as they might use any global variable), but other threads should logically leave
them alone.

As an example, consider the errno variable maintained by UNIX. When a
process (or a thread) makes a system call that fails, the error code is put into errno.
----
        Thread 1                Thread 2
           |                     
		   |
Time	Access (errno set)
  |		   |
  |		   |
  V		   ------------------------
								  |
						Open (errno overwritten)
								  |
		   -----------------------|
		   |
		   |
    Errno inspected
----
In Fig, thread 1 executes the system call access to find out if it has permis-
sion to access a certain file. The operating system returns the answer in the global
variable errno. After control has returned to thread 1, but before it has a chance to
read errno, the scheduler decides that thread 1 has had enough CPU time for the
moment and switches to thread 2. Thread 2 executes an open call that fails, which
causes errno to be overwritten and thread 1‚Äôs access code to be lost forever. When
thread 1 starts up later, it will read the wrong value and behave incorrectly

Various solutions to this problem are possible. One is to prohibit global vari-
ables altogether. However worthy this ideal may be, it conflicts with much existing software. Another is to assign each thread its own private global variables. In this way, each thread has its own private copy of errno and
other global variables, so conflicts are avoided. In effect, this decision creates a
new scoping level, variables visible to all the procedures of a thread (but not to
other threads), in addition to the existing scoping levels of variables visible only to
one procedure and variables visible everywhere in the program.

Accessing the private global variables is a bit tricky, however, since most pro-
gramming languages have a way of expressing local variables and global variables,
but not intermediate forms. It is possible to allocate a chunk of memory for the glo-
bals and pass it to each procedure in the thread as an extra parameter. While hardly
an elegant solution, it works.
Alternatively, new library procedures can be introduced to create, set, and read
these threadwide global variables. The first call might look like this:
create global("bufptr");
It allocates storage for a pointer called bufptr on the heap or in a special storage
area reserved for the calling thread. No matter where the storage is allocated, only
the calling thread has access to the global variable. If another thread creates a glo-
bal variable with the same name, it gets a different storage location that does not
conflict with the existing one.
Two calls are needed to access global variables: one for writing them and the
other for reading them. For writing, something like
set global("bufptr", &buf);
will do. It stores the value of a pointer in the storage location previously created by
the call to create global. To read a global variable, the call might look like
bufptr = read global("bufptr");
It returns the address stored in the global variable, so its data can be accessed.

===== Problem with Leagacy library procedures
The next problem in turning a single-threaded program into a multithreaded
one is that many library procedures are not reentrant. That is, they were not
designed to have a second call made to any given procedure while a previous call
has not yet finished. For example, sending a message over the network may well be
programmed to assemble the message in a fixed buffer within the library, then to
trap to the kernel to send it. What happens if one thread has assembled its message
in the buffer, then a clock interrupt forces a switch to a second thread that
immediately overwrites the buffer with its own message?

Similarly, memory-allocation procedures such as malloc in UNIX, maintain
crucial tables about memory usage, for example, a linked list of available chunks
of memory. While malloc is busy updating these lists, they may temporarily be in
an inconsistent state, with pointers that point nowhere. If a thread switch happens
to occur while the tables are inconsistent and a new call comes in from a different
thread, an invalid pointer may be used, leading to a program crash.

* Many standard library procedures were designed for single-threaded use
* Not safe to have a second call to the same procedure while the first call is in progress
* Examples:
** Network send routines using a fixed internal buffer
** Memory allocation routines like `malloc` and `free`

**Example 1: Network message assembly**

* Library internally uses a fixed buffer to assemble a message
* Steps:
** Thread A assembles message in buffer
** Clock interrupt occurs ‚Üí context switch
** Thread B assembles its message in the same buffer
* Consequence:
** Thread A‚Äôs message is overwritten
** Corrupted or incorrect messages may be sent
** Leads to unpredictable behavior

**Example 2: Memory allocation (`malloc`)**
* `malloc` maintains internal tables of free memory chunks (e.g., linked lists)
* While `malloc` is updating tables:
** Tables may be temporarily inconsistent
** Pointers may be invalid
* If thread switch occurs during this period:
** Another thread calling `malloc` may use invalid pointers
** Program may crash or corrupt memory

**Implications**
* Non-reentrant libraries cannot safely be called concurrently from multiple threads
* To fix:
** Rewrite the library to be thread-safe
** Add internal locking mechanisms
** Ensure reentrancy
* Challenges:
** Nontrivial to rewrite
** Subtle bugs are easily introduced

**Key takeaway**
* Moving from single-threaded to multithreaded code requires careful auditing of all library calls
* Only reentrant or thread-safe functions are safe to use concurrently

**One-sentence exam answer**
* Many library procedures are non-reentrant, so concurrent calls from multiple threads may overwrite internal buffers or corrupt memory, requiring either thread-safe rewrites or proper synchronization.

Fixing all these
problems effectively means 

* rewriting the entire library. Doing so is a nontrivial
activity with a real possibility of introducing subtle errors.
* A different solution is to provide each procedure with a wrapper that sets a bit
to mark the library as being in use. Any attempt for another thread to use a library
procedure while a previous call has not yet completed is blocked. Although this
approach can be made to work, it greatly eliminates potential parallelism.
** Solution: Wrappers to serialize library calls
*** Idea:
****  Provide each non-reentrant procedure with a wrapper
**** Wrapper sets a ‚Äúbusy‚Äù flag when the library is in use
**** Any other thread attempting to call the same procedure is blocked
*** Example concept
+
[,c]
----
//Suppose `send_message()` is non-reentrant:
static int busy = 0;

void send_message_wrapper(const char *msg) {
	while (__sync_lock_test_and_set(&busy, 1)) {
		// busy-wait or sleep until previous call finishes
	}
	send_message(msg); // original library call
	__sync_lock_release(&busy);
}
----
* Behavior:
** Only one thread may enter send_message at a time
** Other threads wait for the procedure to become free

* Pros
** Simple to implement
** Ensures library safety without rewriting entire library

* Cons
** Eliminates potential parallelism
** Threads are blocked even if they don‚Äôt share data
** Can become a bottleneck in multithreaded programs
** Poor scalability for high-concurrency applications

* Key insight
** Wrappers work as a stopgap measure, but they reduce concurrency
** Better long-term solution: use reentrant/thread-safe libraries or TLS

* One-sentence exam answer

Wrapping non-reentrant library calls with a busy flag ensures safety but serializes access, reducing parallelism and scalability.

Here‚Äôs a reference table of common C library functions, showing reentrant/thread-safe vs non-reentrant
[cols="1,1,1", options="header"]
|===
| Function | Reentrant / Thread-safe | Notes / Comments

| `malloc`, `free`
| ‚úÖ Thread-safe in modern glibc
| Protected by internal locks; early UNIX versions were not thread-safe

| `strtok`
| ‚ùå Non-reentrant
| Uses static internal state; use `strtok_r` instead

| `asctime`, `ctime`
| ‚ùå Non-reentrant
| Use `asctime_r`, `ctime_r` instead

| `localtime`, `gmtime`
| ‚ùå Non-reentrant
| Use `localtime_r`, `gmtime_r` instead

| `rand`
| ‚ùå Non-reentrant
| Use `rand_r` instead (per-thread seed)

| `printf`, `fprintf`
| ‚úÖ Generally thread-safe in glibc
| Internal locks prevent corruption of FILE streams; buffers may still need care

| `perror`
| ‚úÖ Thread-safe in modern glibc
| Internally calls `write` and uses errno; safe if errno is thread-local

| `getenv`, `putenv`
| ‚ùå Non-reentrant
| Environment is global; concurrent access can cause races

| `signal`
| ‚ùå Generally non-reentrant
| Use `sigaction` with proper signal masks for multithreading

| `read`, `write`, `open`, `close`
| ‚úÖ Thread-safe (system calls)
| Kernel ensures concurrency safety

| `strerror`
| ‚ùå Non-reentrant
| Use `strerror_r` for thread-safe error messages

| `localtime_r`, `gmtime_r`, `asctime_r`, `ctime_r`
| ‚úÖ Reentrant versions
| Allocate thread-local structures instead of static ones

| `pthread_*` APIs
| ‚úÖ Thread-safe by design
| For example: `pthread_create`, `pthread_mutex_lock`, `pthread_join` are safe
|===

===== Problem with thread-private global variables

* Some variables are logically global to a single thread but not the entire program
* Ordinary globals are shared across threads, causing conflicts
* Examples: `errno`, temporary buffers, caches, or thread context data

**Possible solutions**

* **Prohibit global variables entirely**
** Avoids conflicts completely
** Conflicts with much existing software
** Example:  Replace global buffer:
```c
// BAD: global buffer
char buf[1024];
// GOOD: allocate buffer in main() or thread function
void thread_func() {
	char buf[1024]; // local to thread
}
```

* **Assign each thread its own private global variables**
** Each thread gets a separate copy of variables like `errno`
** Creates a **new scoping level**:
  ** Procedure-local variables
  ** Thread-global variables (shared within thread only)
  ** Program-global variables (shared across all threads)

**Accessing thread-private globals**

*Programming languages usually only provide:*

* Local variables
* Global variables
* Intermediate form (thread-global) requires extra handling

**Solution 1: Pass memory explicitly**

* Allocate a chunk of memory for thread-private globals
* Pass it as an extra parameter to every procedure
* Example:
```c
typedef struct {
	int bufsize;
	char *buf;
} thread_globals;

void process(thread_globals *tg) {
	tg->bufsize = 1024;
	tg->buf = malloc(tg->bufsize);
	// do work with tg->buf
}

void *thread_func(void *arg) {
	thread_globals tg;
	process(&tg);
	free(tg.buf);
	return NULL;
}
```
* Pros:
** Works without modifying language or compiler
* Cons:
** Tedious
** Hard to maintain
** Not elegant

**Solution 2: Library-based thread globals**

Introduce library routines to manage thread-global variables

Examples of usage:
```c
// Allocate thread-global variable
create_global("bufptr");

// Write to thread-global variable
char *buf = malloc(1024);
set_global("bufptr", buf);

// Read from thread-global variable
char *thread_buf = read_global("bufptr");
strcpy(thread_buf, "Thread-specific data");
```
Notes:

* Each thread gets its own storage for bufptr
* Threads creating a variable with the same name get separate locations
* Simplifies function calls because no extra parameter is needed
* This is the approach used by glibc for errno and other thread-specific variables(Thread-local storage (TLS))
** errno and similar thread-local variables use POSIX Thread-Specific Data (TSD) via pthread_key_create and Thread-Local Storage (TLS).
** Modern glibc uses compiler-supported TLS (__thread / _Thread_local) for speed.
+
Example (glibc):
+
----
__thread int errno;.
----
** Behind the scenes:
*** Each thread gets a pointer to a thread control block (TCB)
*** TLS variables are offsets from the TCB
*** Access does not require extra parameters, just a direct read/write
* Linux kernel itself does not provide global variables per user thread, but allows kernel threads to maintain per-task data (in task_struct).
** The kernel creates per-thread (task_struct) memory for each kernel thread: Stack (kernel stack, user stack), Thread ID, CPU registers and scheduling info
** Variables like errno are not in the kernel, they live in user-space TLS managed by glibc.
** Kernel ensures:
*** Thread safety for kernel data
*** Separation between kernel thread stacks
** So kernel threads don‚Äôt need create_global() ‚Äì that‚Äôs purely a user-space concern.

Key insights

* Each thread has isolated storage for its globals
* Avoids conflicts without eliminating all global variables
* Introduces an additional scoping level:
** Procedure-local
** Thread-global
** Program-global


**Thread code structure**

* A thread‚Äôs code typically consists of:
** Multiple procedures (functions)
** Just like a normal process
* These procedures may use:
** Local variables
** Parameters
** Global variables

**Local variables and parameters**

* Local variables:
** Are stored on the thread‚Äôs stack
** Are private to each thread
* Parameters:
** Are passed via the stack or registers
** Are also private to the executing thread
* Result:
** No correctness problems arise from locals or parameters

**Thread-private global variables (the core problem)**

* Some variables are:
** Used by many procedures within one thread
** Logically global to that thread
** But should not be shared with other threads
* These variables are:
** Not local variables
** Not global to the entire program
** Conceptually ‚Äúglobal per thread‚Äù

**Why this is a problem**

* In traditional languages (C, C++):
** Global variables are shared by all threads
* There is no built-in notion of:
** Variables that are global to a thread only
* If implemented as normal globals:
** Other threads can accidentally read or modify them
** This leads to race conditions and corruption

**Typical examples**

* Per-thread state such as:
** Error codes
** Temporary buffers
** Caches
** Context or session data
* Each thread needs its own copy:
** But procedures within the same thread must share it

**Common solutions**

* Thread-local storage (TLS):
** Each thread has its own instance of a variable
** Accessed using language or runtime support
* Examples:
** C: __thread or _Thread_local
** POSIX: pthread_key_create(), pthread_setspecific()
** Compilers/runtime manage per-thread copies

**Key insight**

* The problem is not concurrency itself.
* The problem is:
** Distinguishing between program-global state
** And thread-private shared state
* Without explicit support:
** Programmers are forced into unsafe designs

**One-sentence exam answer**

* Variables that are logically global to a single thread but shared across its procedures cannot be expressed using ordinary globals, requiring thread-local storage to avoid unintended sharing between threads.

* Occur when:
** Multiple threads access shared data
** At least one access is a write
** No proper synchronization exists
* Effects:
** Incorrect results
** Rare and non-reproducible bugs


**Deadlocks**

* Occur when:
** Thread A holds lock X and waits for Y
** Thread B holds lock Y and waits for X
* Common causes:
** Inconsistent lock ordering
** Blocking while holding locks
** Recursive locking errors

**Starvation**

* Some threads:
** Never acquire CPU time or locks
* Causes:
** Priority inversion
** Unfair scheduling
** Lock contention

**Memory visibility problems**

* Single-threaded assumption:
** Writes are immediately visible
* Multithreaded reality:
** CPU caches
** Instruction reordering
** Weak memory models
* Without synchronization:
** Threads observe stale or inconsistent data

**Loss of determinism**

* Single-threaded programs:
** Same input produces same behavior
* Multithreaded programs:
** Execution order depends on timing and scheduling
* Result:
** Bugs become timing-dependent
** Testing cannot cover all interleavings

**Error handling complexity**

* Failure in one thread raises questions:
** Should the process exit?
** Should other threads stop?
** How are errors propagated?
* No universal correct policy exists

**Debugging difficulty**

* Debugging changes execution timing
* Logging alters thread interleavings
* Bugs may disappear when observed
* Common outcome:
** Works in debug, fails in release

**Signals and asynchronous events**

* Signals are:
** Process-directed
** Delivered to arbitrary threads
* Signal handlers:
** Interrupt execution at unsafe points
* Requires:
** Signal masking
** Dedicated signal-handling threads

**Why this is fundamentally hard**

* Humans reason sequentially
* CPUs execute concurrently
* OS schedulers are nondeterministic
* Hardware reorders memory operations
* Multithreading breaks:
** Linear time assumptions
** Single control-flow reasoning

**Why modern systems avoid manual threading**

* Alternatives include:
** Event-driven architectures
** Async/await models
** Message passing
** Actor systems
** Managed runtimes (Go, Erlang, JVM virtual threads)
* Goal:
** Concurrency without shared mutable state

**One-sentence exam answer**

* Making single-threaded code multithreaded requires introducing shared state and synchronization, which destroys determinism and introduces races, deadlocks, memory visibility issues, and severe
===== Signal Problems
Next, consider signals. Some signals are logically thread specific; others are
not. For example, if a thread calls alarm, it makes sense for the resulting signal to
go to the thread that made the call. However, when threads are implemented entire-
ly in user space, the kernel does not even know about threads and can hardly direct
the signal to the right one. An additional complication occurs if a process may only
have one alarm pending at a time and several threads call alarm independently.

Other signals, such as keyboard interrupt, are not thread specific. Who should
catch them? One designated thread? All the threads? Furthermore, what happens
if one thread changes the signal handlers without telling other threads about it?
And what happens if one thread wants to catch a particular signal (say, the user hit-
ting CTRL-C), and another thread wants this signal to terminate the process? This
situation can arise if one or more threads run standard library procedures and oth-
ers are user-written. Clearly, these wishes are incompatible. In general, signals are
difficult enough to manage even in a single-threaded environment. Going to a mul-
tithreaded environment does not make them any easier to handle
==== Examples
*Read multiple files and count lines*

[,c]
----
#include <stdio.h>

#define FILE_COUNT 3

int count_lines(const char *filename) {
    FILE *fp = fopen(filename, "r");
    if (!fp) return 0;
    int count = 0;
    char buf[1024];
    while (fgets(buf, sizeof(buf), fp)) {
        count++;
    }
    fclose(fp);
    return count;
}

int main() {
    const char *files[FILE_COUNT] = {"file1.txt", "file2.txt", "file3.txt"};
    int total_lines = 0;

    for (int i = 0; i < FILE_COUNT; i++) {
        total_lines += count_lines(files[i]);
    }

    printf("Total lines: %d\n", total_lines);
    return 0;
}
----
Behavior

* Processes files sequentially
* Total runtime is sum of individual file processing times

*Step 1: Identify independent work*

* Counting lines in each file can run concurrently
* Files are independent
* Only shared state is total_lines

*Step 2: Introduce multithreading*

* Use pthreads
* Each thread counts lines in one file
* Protect total_lines using a mutex
[,c]
----
#include <stdio.h>
#include <pthread.h>

#define FILE_COUNT 3

const char *files[FILE_COUNT] = {"file1.txt", "file2.txt", "file3.txt"};
int total_lines = 0;
pthread_mutex_t lock;

void *count_file(void *arg) {
    const char *filename = (const char *)arg;
    FILE *fp = fopen(filename, "r");
    if (!fp) return NULL;

    int count = 0;
    char buf[1024];
    while (fgets(buf, sizeof(buf), fp)) count++;

    fclose(fp);

    // Update shared state safely
    pthread_mutex_lock(&lock);
    total_lines += count;
    pthread_mutex_unlock(&lock);

    return NULL;
}

int main() {
    pthread_t threads[FILE_COUNT];
    pthread_mutex_init(&lock, NULL);

    // Create threads
    for (int i = 0; i < FILE_COUNT; i++) {
        pthread_create(&threads[i], NULL, count_file, (void *)files[i]);
    }

    // Join threads
    for (int i = 0; i < FILE_COUNT; i++) {
        pthread_join(threads[i], NULL);
    }

    printf("Total lines: %d\n", total_lines);
    pthread_mutex_destroy(&lock);
    return 0;
}
----
Step 3: Problems introduced

* Race conditions
** Without mutex, threads could update total_lines concurrently ‚Üí wrong result

* Deadlocks
** Not in this simple example, but possible if multiple locks exist

* Memory visibility
** Each thread may cache total_lines; mutex ensures proper ordering

* Debugging complexity
** Bugs may appear intermittently depending on scheduler

* Resource limits
** Creating too many threads for many files can overwhelm the system

---

**using thread-local storage (TLS)**

Here‚Äôs a refactored multithreaded example using thread-local storage (TLS), which avoids the mutex and reduces contention.

* In previous example, `total_lines` was shared
* Requires a mutex for every update
* Introduces overhead and potential contention
* TLS can avoid this by keeping per-thread totals

**Step 1: Use thread-local storage**

* Each thread keeps its own `local_total`
* No mutex needed
* Final total is computed after all threads join

Thread-local storage allows each thread to maintain its own independent data, eliminating mutex contention for per-thread variables while still supporting aggregation after threads complete.

**Example using TLS in pthreads**
```c
#include <stdio.h>
#include <pthread.h>

#define FILE_COUNT 3

const char *files[FILE_COUNT] = {"file1.txt", "file2.txt", "file3.txt"};

// Thread-local storage for each thread's line count
__thread int local_total = 0;

void *count_file(void *arg) {
    const char *filename = (const char *)arg;
    FILE *fp = fopen(filename, "r");
    if (!fp) return NULL;

    int count = 0;
    char buf[1024];
    while (fgets(buf, sizeof(buf), fp)) count++;

    fclose(fp);

    // Store in thread-local variable
    local_total = count;

    return (void *)(long)local_total; // Return the per-thread count
}

int main() {
    pthread_t threads[FILE_COUNT];
    int total_lines = 0;

    // Create threads
    for (int i = 0; i < FILE_COUNT; i++) {
        pthread_create(&threads[i], NULL, count_file, (void *)files[i]);
    }

    // Join threads and accumulate totals
    for (int i = 0; i < FILE_COUNT; i++) {
        void *retval;
        pthread_join(threads[i], &retval);
        total_lines += (int)(long)retval; // sum thread-local totals
    }

    printf("Total lines: %d\n", total_lines);
    return 0;
}
```

Benefits of TLS approach

* No mutex needed ‚Üí avoids locking overhead
* Each thread has isolated storage ‚Üí no race conditions
* Simpler and safer for per-thread state

Potential issues

* TLS memory overhead per thread
* Returning value from thread is still needed to combine results
* Not all variables can be TLS if shared between threads
* For very many threads, kernel TLS segment may become a bottleneck

Key takeaways

* TLS is ideal for per-thread private data
* Shared aggregation still requires some merge step after thread completion
* Improves performance and correctness in multithreaded refactor
* Used extensively in *glibc (errno) and NPTL threads*

Key lessons

* Identify independent work carefully
* Shared state must be protected
* Use synchronization primitives (mutexes, semaphores)
* Keep per-thread data local whenever possible
* Minimize blocking and contention for better performance

===== stack management
In many systems, when a process‚Äô stack overflows, the kernel just provides that process with
more stack automatically. When a process has multiple threads, it must also have
multiple stacks. If the kernel is not aware of all these stacks, it cannot grow them
automatically upon stack fault. In fact, it may not even realize that a memory fault
is related to the growth of some thread‚Äôs stack.

* Single-threaded processes:
  ** Kernel can automatically grow the stack on overflow
  ** Memory faults are associated with the process stack

* Multithreaded processes:
  ** Each thread has its own stack
  ** Kernel may not be aware of all thread stacks
  ** Stack overflows in a thread may not be automatically handled
  ** Memory faults may not be recognized as related to stack growth

**Implications**

* Each thread‚Äôs stack must be preallocated carefully
* Threads cannot rely on automatic stack growth like single-threaded processes
* Libraries (e.g., pthreads) usually require specifying stack size on thread creation
* Incorrect stack size may lead to crashes or subtle memory corruption

**Common solutions**

* Specify explicit stack size using `pthread_attr_setstacksize()` when creating threads
* Avoid deep recursion or large stack-allocated arrays in threads
* Use thread-local heap allocation instead of large stack buffers for per-thread data

**One-sentence exam answer**
* In multithreaded programs, each thread needs its own stack, and the kernel cannot automatically grow these stacks, so careful stack size management is required to avoid overflows.

==== making code thread-safe
Here‚Äôs a complete reference table of solutions for making code thread-safe, combining TLS, wrappers, mutexes, and reentrant library functions


[cols="1,1,1,1,1", options="header"]
|===
| Solution | How it works | Pros | Cons / Notes | Example

| Thread-Local Storage (TLS)
| Each thread keeps its own copy of data using `__thread` or `pthread_key_create`
| No mutex needed; avoids races; automatic per-thread isolation
| TLS memory overhead per thread; must still combine results; limited size in some OSes
| `__thread int local_total; // each thread has its own copy`

| Library-managed thread globals
| Library routines create per-thread global variables: `create_global()`, `set_global()`, `read_global()`
| Provides a per-thread global scope; avoids passing extra parameters
| Extra library layer; requires runtime support; memory still per-thread
| `create_global("bufptr"); set_global("bufptr", &buf); char *b = read_global("bufptr");`

| Wrappers / Busy flags
| Wrap non-reentrant library calls with a busy flag; block other threads until call completes
| Simple; avoids rewriting library; works with legacy code
| Eliminates parallelism; threads blocked unnecessarily; high contention possible
| ```
static int busy = 0;
while (__sync_lock_test_and_set(&busy,1)) {}
send_message(msg);
__sync_lock_release(&busy);

| Mutex / Locks around shared resources
| Protect shared variables/resources using pthread_mutex_lock / unlock
| Works with any code; ensures correctness; widely understood
| Can cause deadlocks; blocking reduces parallelism; performance overhead
| ```
pthread_mutex_lock(&lock);
total += local_count;
pthread_mutex_unlock(&lock);


| Reentrant / thread-safe library functions
| Use `_r` variants of non-reentrant functions (e.g., `strtok_r`, `malloc` in glibc)
| Safe for multithreaded use; minimal extra work
| Not all functions have reentrant versions; legacy code may need rewrite
| `char *token = strtok_r(str, delim, &saveptr);`

| Pass memory explicitly
| Allocate per-thread storage and pass it as a parameter to functions
| Works without library changes; fully controllable
| Tedious; modifies function signatures; not scalable
| ```
typedef struct { int count; } thread_data;
void process(thread_data *td);
thread_data td;
process(&td);

|===
**Highlights:**  
* TLS and library-managed globals are **modern, efficient solutions** (used in glibc for `errno` and NPTL).  
* Wrappers / busy flags are **legacy-compatible but limit parallelism**.  
* Mutexes provide **general correctness but can hurt performance**.  
* `_r` reentrant functions are the **preferred approach** when available.  
* Passing memory explicitly is **manual and tedious**, mostly educational or legacy.  

== Event-Driven Servers
An event means:socket is readable, Socket is writable, Connection closed, Timer expired,The OS notifies the server when something interesting happens.

An event-driven server uses one (or a few) threads that wait for events (socket ready, data arrived, timer expired) and react to them, instead of blocking a thread per client.

Event-driven servers use non-blocking I/O and kernel event notification (like epoll) so a small number of threads can efficiently manage thousands of connections by reacting only when real work is needed.

Event-driven servers are applications built as many small state machines, driven by kernel events (epoll), just like the OS itself is built from interacting state machines.

The The key idea of the third approach is Use nonblocking system calls and events, instead of threads.
=== state machine
In OS design, a state machine consists of:

* States: what the system is currently doing
* Events: things that happen (interrupts, I/O ready, timer)
* Transitions: how the state changes when an event occurs
* Actions: work done during transitions

classic OS state machines:

* Process states: NEW ‚Üí READY ‚Üí RUNNING ‚Üí BLOCKED ‚Üí READY
* Thread states
* Disk I/O states
* Network protocol states (TCP)

*Threaded server vs event-driven server (state perspective)*

In Thread-per-request model

* The OS manages the state machine
* Each thread blocks and wakes automatically
* Example: RUNNING ‚Üí BLOCKED (read) ‚Üí READY ‚Üí RUNNING
* The programmer:
** Writes sequential code
** Relies on the kernel to manage states

In Event-driven model

* You manage the state machine explicitly
* No blocking
* One thread handles many logical ‚Äúflows‚Äù

In an event-driven server, each connection has its own state(One client = one state machine).

Example states for an HTTP connection:
----
NEW
‚Üì
READING_REQUEST
‚Üì
PROCESSING
‚Üì
SENDING_RESPONSE
‚Üì
CLOSED
----
These are not OS thread states,They are application-level states.

Events come from: epoll, timers, internal logic,Examples:
|===
|Event	|Meaning
|socket readable	|request data arrived
|socket writable	|can send response
|timer expired	|timeout
|client closed	|connection ended
|===
The event loop is like the OS scheduler for state machines.
----
while (true) {
    events = epoll_wait();

    for (event in events) {
        connection = event.owner;
        advance_state_machine(connection, event);
    }
}
----
This is pure state-machine execution.

Async code hides the state machine, but it‚Äôs still there. This:
----
data = await reader.read()
process(data)
await writer.drain()
----
Is transformed internally into something like:
----
STATE_WAIT_READ
‚Üí STATE_PROCESS
‚Üí STATE_WAIT_WRITE
----
Each await:

* Saves state
* Returns control to the event loop
* Resumes later on an event

This design is called event-driven because:

* Nothing blocks
* The server reacts to events
* Each event causes a short burst of work
* Control always returns to the event loop

*The problem with thread-per-request*

In a classic threaded web server: Each client has one worker thread, so Thousands of clients need thousands of threads, the Costs of using many threads:

* Large memory usage (stack per thread)
* Context switching overhead
* Scheduler pressure
* Cache thrashing

eventhough Most web requests, Spend most of their time waiting for: Network,Disk or Database, and Not actually using the CPU.

*Analogy Example: Call center vs waiter model*

* Thread-per-request
** One waiter stands at each table
** Even when customer is silent, waiter waits
* Event-driven
** One waiter watches all tables
** Only walks to a table when someone raises a hand

*Blocking vs non-blocking I/O*

Blocking I/O (threaded model)
----
read(socket) ‚Üí sleep until data arrives
----
* Thread blocks
* OS scheduler switches threads

Non-blocking I/O (event-driven)
----
read(socket) ‚Üí returns immediately
----
* If no data ‚Üí ‚Äúnot ready‚Äù, If data is not ready The call does not block.
* Server continues handling other clients

*Why this matters in OS design*
* Scalability
** Threads don‚Äôt scale linearly, State machines do
** Thousands of connections: means Thousands of states and Only a few threads
* Explicit control
** OS designers like:Predictability,Minimal overhead,Explicit transitions
** Event-driven = deterministic control flow.
* Kernel ‚Üî user-space symmetry
|===
|Kernel	|User-space
|Process state machine	|Connection state machine
|Interrupt	|epoll event
|Scheduler	|event loop
|===
Same design philosophy at different layers.

=== Advantages of Event-Driven Servers
*1. Scalability*

A primary advantage of event-driven servers is their ability to scale to a very large number of concurrent connections. Because a small number of threads can manage thousands of clients, the system avoids the overhead associated with creating and scheduling large numbers of threads. From an operating system perspective, fewer threads imply:

* Reduced context-switching overhead(Fewer threads ‚Üí fewer context switches)
* Lower scheduler pressure(Scheduler overhead is minimal)
* Improved cache locality

*2. Reduced Memory Consumption*

Thread-based servers require a separate stack for each thread, often several megabytes in size. In contrast, event-driven servers store only minimal per-request state in user-defined data structures(No per-thread stacks,State stored in compact data structures (tables)). As a result, memory usage grows slowly as concurrency increases.

This property is especially important for servers operating under memory constraints or handling large client populations.

*3. High Throughput*

Event-driven servers tend to achieve high throughput because the CPU is rarely idle. Since no thread blocks while waiting for I/O, the processor continuously performs useful work, handling events as they arrive(Work happens in short bursts triggered by events). This makes the model particularly effective for I/O-bound workloads.Ideal for: Web servers, Proxies, Message brokers.

*Natural fit for I/O-bound workloads*

Most server workloads are:

* Network waits
* Disk waits
* Database waits

Event-driven models excel here.

*4. Predictable Scheduling Behavior*

Because the server executes within a small number of threads, execution is more predictable than in highly multithreaded systems(Predictable performance). Preemption by the scheduler occurs less frequently(No thread preemption in the middle of logic), and performance degradation under load is often more gradual. its Easier to reason about CPU usage under load.

*5. Conceptual Alignment with Operating System Design*

Event-driven servers closely mirror internal operating system mechanisms. Kernels themselves are largely event-driven, responding to interrupts and managing state machines for processes, devices, and protocols. This conceptual symmetry enables efficient interaction between user-space servers and kernel facilities such as epoll.

=== Disadvantages of Event-Driven Servers
*1. Increased Programming Complexity*

The most significant drawback of event-driven designs is their complexity. The familiar sequential control flow of traditional programs is lost. Instead, computation is fragmented across multiple event handlers, each corresponding to a particular state of execution. so The biggest downside.

* No linear, sequential code
* Logic split across states and callbacks
* Easy to forget transitions

Classic problem: ‚ÄúWhat state was I in when this event arrived?

This increases the cognitive burden on the programmer and raises the likelihood of subtle logical errors.

*2. Explicit State Management*

In a threaded design, the operating system automatically preserves execution context, including the call stack and local variables. In an event-driven system, this responsibility is shifted to the programmer. All relevant state must be explicitly stored and restored as the server switches between requests. this approach effectively simulates threads and their stacks manually.
|===
|Threads give you: |Event-driven requires:
|
Call stack

Local variables

Automatic resume
|
Explicit state tables

Careful bookkeeping
|===
so we are Simulating threads the hard way.

*3. Debugging Difficulty*

Debugging event-driven programs is often more difficult than debugging threaded ones. Stack traces tend to be shallow and provide limited insight into the logical flow of execution. Errors frequently manifest as invalid state transitions rather than simple control-flow mistakes(Bugs appear as ‚Äúimpossible states‚Äù).

Race-like logic errors even with one thread

*4. Sensitivity to Blocking Operations*

Event-driven servers rely on the assumption that no operation blocks. A single blocking system call can stall the entire server, preventing progress for all active clients. Consequently, careful auditing of all code paths is required to ensure that only non-blocking operations are used. so One blocking call:

* Blocks the entire server
* Stops progress for all clients

Example bug:
----
read(fd, buf, size);   // accidentally blocking
----

*5. Poor Suitability for CPU-Bound Workloads*

Because event handlers execute within a limited number of threads, long-running computations can monopolize the CPU and delay the handling of other events. As a result, event-driven designs are ill-suited for workloads dominated by CPU-intensive tasks unless combined with additional mechanisms such as worker threads. 

so If one event handler Runs long computation:It starves all other clients, for this reason Event-driven servers hate long CPU tasks.

*6. Error handling is harder*

Failures happen mid-state:

* Disk fails
* Client disconnects
* Timeout fires

Each state needs explicit recovery logic.

=== Practical Challenges
These are not theoretical disadvantages ‚Äî they are engineering challenges.

*1. State Explosion*

As server functionality increases, the number of states and transitions grows rapidly. Managing this complexity requires careful design, often with explicit state diagrams or table-driven finite-state machines. Without such discipline, implementations can become difficult to maintain and extend.

Example:
----
WAITING_FOR_DISK
WAITING_FOR_DB
WAITING_FOR_CACHE
WAITING_FOR_WRITE
TIMEOUT
----
Managing this cleanly is hard.

*2. Control-Flow Fragmentation*

In the absence of language-level support such as coroutines, event-driven programs often rely on callbacks. This can lead to deeply nested and hard-to-follow code structures, commonly referred to as ‚Äúcallback hell.‚Äù

Without coroutines:
----
read ‚Üí callback ‚Üí write ‚Üí callback ‚Üí close
----
Logic becomes deeply nested and unreadable.

*3. Event Ordering and Timing Issues*

Events may occur in unexpected orders or at unexpected times. The server must be prepared to handle all valid interleavings(FSM must handle every possible order.), including late arrivals, early completions, and simultaneous events. Ensuring correctness under these conditions is nontrivial.

*4. Integration with Signals and Asynchronous Notifications*

Some asynchronous operations, particularly disk I/O, rely on signals or similar notification mechanisms. These mechanisms are inherently difficult to reason about and integrate cleanly into application-level event loops(Missed or duplicated events cause bugs).

*5. Testing and Verification Difficulties*

Testing event-driven systems is challenging due to their sensitivity to timing and event ordering(Hard to reproduce timing-dependent bugs). Reproducing bugs often requires precise control over event sequences(Requires event simulation), which can be difficult to achieve in practice(Unit testing FSM logic is non-trivial).

*6. Fairness and Starvation Concerns*

If an event handler performs excessive work before returning control to the event loop(busy connection), it may delay the processing of other events(Can dominate the event loop,Starve others). 

Ensuring fairness requires careful design, including limits on per-event computation and frequent yielding back to the event loop. so Servers must: Limit per-event work, Yield control frequently

=== Mitigation Strategies
Modern systems employ several techniques to address these issues:

. Async/await and coroutines restore a sequential programming style while preserving the underlying event-driven execution(Compiler/runtime builds FSM automatically).
. Hybrid architectures combine event-driven I/O with thread pools for CPU-heavy tasks.
. Structured finite-state machines and formal state diagrams improve clarity and correctness.
** Explicit FSM diagrams
** Table-driven transitions
** Clear state ownership
. Use Timeouts everywhere
** Prevents stalled states
** Essential for robustness.
. Comprehensive testing frameworks simulate event sequences and timing variations.
. Monitoring and logging tools track event handling performance and detect starvation.

=== When to use event-driven
Good fit:

* High concurrency
* Mostly I/O-bound
* Performance-critical servers

Poor fit:

* Heavy CPU computation
* Simple, low-load applications
* Teams inexperienced with async/FSM logic.

== Single-Threaded Versus Multi-Threaded Versus Event-driven Servers
Server architecture plays a central role in determining the performance, scalability, and complexity of networked systems. In operating systems, three fundamental server designs are commonly discussed: single-threaded, multi-threaded, and event-driven servers. Each design represents a different trade-off between simplicity, resource usage, and concurrency. 

*1. Single-Threaded Servers*

A single-threaded server executes entirely within one thread of control. Requests are handled sequentially: the server accepts a request, processes it to completion, sends a response, and only then proceeds to the next request.

In its simplest form, such a server relies on blocking system calls. When the server performs a blocking operation‚Äîsuch as reading from a socket or accessing the disk‚Äîit is suspended by the operating system until the operation completes. During this time, the server performs no useful work, even if other requests are pending.

The principal advantage of this approach is its conceptual simplicity. The server logic follows a straightforward, sequential control flow that is easy to implement, reason about, and debug. However, this design performs poorly under load, particularly for I/O-bound workloads, because blocking operations prevent the server from exploiting available concurrency.

As a result, purely single-threaded blocking servers are generally unsuitable for high-performance or high-concurrency environments.

*2. Multi-Threaded Servers*

Multi-threaded servers extend the single-threaded model by allowing multiple threads to execute concurrently within the same process. A common design uses a dispatcher thread to accept incoming requests and a pool of worker threads to handle them. Each worker thread processes a request using blocking system calls, relying on the operating system to suspend and resume execution as needed.

This design restores the illusion of sequential execution for each request while enabling concurrency across requests. When one thread blocks on I/O, others may continue executing, thereby improving throughput and responsiveness.

From an operating-system standpoint, multi-threaded servers benefit from kernel-managed scheduling, stacks, and execution contexts. However, this convenience comes at a cost. Threads consume significant memory for their stacks, and frequent context switches introduce overhead. As the number of threads grows, contention for shared resources and increased scheduling complexity can limit scalability.

Consequently, while multi-threaded servers perform well at moderate concurrency levels and are easier to program than event-driven systems, they may not scale efficiently to very large numbers of simultaneous connections.

*3. Event-Driven Servers*

Event-driven servers adopt a fundamentally different approach. Instead of associating each request with a dedicated thread, they rely on non-blocking or asynchronous system calls and an event notification mechanism (such as epoll on Linux). Typically, a small number of threads‚Äîoften just one‚Äîexecute an event loop that repeatedly waits for events and reacts to them.

When a request cannot be completed immediately (e.g., it requires disk I/O), the server initiates a non-blocking operation and records the request‚Äôs state in a data structure. Execution then proceeds to handle other events. When the I/O operation completes, the operating system generates an event, allowing the server to resume processing the corresponding request.

In this model, the traditional sequential process abstraction is lost. The programmer must explicitly manage request state and control flow, effectively implementing a finite-state machine for each active request. While this increases implementation complexity, it enables extremely high scalability, as thousands of concurrent requests can be managed without creating thousands of threads.

Event-driven servers are therefore widely used in high-throughput systems where resource efficiency is critical.

*4. Comparative Perspective*

These three architectures represent different points in the design space:

* Single-threaded servers emphasize simplicity but sacrifice performance.
* Multi-threaded servers balance programmability and concurrency but incur significant overhead at scale.
* Event-driven servers maximize efficiency and scalability at the cost of increased programming complexity.

Modern systems often adopt hybrid designs, combining event-driven I/O with worker threads for CPU-intensive tasks, in order to benefit from the strengths of each approach.

Three ways to construct a server.
|===
| Model | Characteristics
| Threads | Parallelism, blocking system calls
| Single-threaded process | No parallelism, blocking system calls
| Finite-state machine/event-driven | Parallelism, nonblocking system calls, interrupts
|===
Summary Table 
[cols="1,3,3,3", options="header"]
|===
| Aspect
| Single-Threaded Server
| Multi-Threaded Server
| Event-Driven Server

| Basic Execution Model
| One thread handles all requests sequentially
| Multiple threads handle requests concurrently
| One or few threads react to events via an event loop

| I/O Behavior
| Blocking system calls
| Blocking system calls per thread
| Non-blocking or asynchronous system calls

| Concurrency
| None (requests handled one at a time)
| Provided by OS threads
| Provided by event multiplexing

| State Management
| Implicit in call stack
| Implicit per thread stack
| Explicit, application-managed state tables

| Programming Simplicity
| Very simple
| Moderate
| Complex

| Memory Usage
| Very low
| High (stack per thread)
| Low (small per-request state)

| Context Switching Overhead
| Minimal
| High with many threads
| Very low

| Scalability
| Poor
| Moderate
| Excellent

| Suitability for I/O-bound Workloads
| Poor
| Good
| Excellent

| Suitability for CPU-bound Workloads
| Poor
| Good
| Poor unless combined with worker threads

| Debugging Difficulty
| Easy
| Moderate
| Difficult

| Typical Use Cases
| Simple servers, teaching examples
| General-purpose web servers
| High-throughput, large-scale servers

| OS Abstraction Used
| Sequential process
| Threads and scheduler
| Finite-state machines and events
|===

== Relationship Between Server Architectures and Process/Thread State Diagrams
In operating systems, *state diagrams* are used to describe how processes and threads move between execution states such as running, ready, and blocked. Each server architecture discussed‚Äîsingle-threaded, multi-threaded, and event-driven‚Äîcorresponds to a distinct interpretation and use of these states. Understanding this relationship clarifies why the performance and scalability characteristics of these architectures differ.

Single-threaded servers rely entirely on the operating system‚Äôs process state machine, while multi-threaded servers replicate this model across many threads. Event-driven servers, by contrast, minimize OS-level state transitions and instead implement application-level finite-state machines, thereby shifting control from the kernel to user space.

The evolution from single-threaded to multi-threaded to event-driven servers represents a gradual migration of state management responsibility: from the operating system, to the kernel scheduler managing many threads, and finally to the application itself in the form of explicit finite-state machines.

*1. Single-Threaded Servers and Process State Diagrams*

A single-threaded server maps directly onto the classical process state model: RUNNING ‚Üí BLOCKED ‚Üí READY ‚Üí RUNNING

Interpretation

* The server consists of one process with one thread.
* When the server performs a blocking operation (e.g., read() on a socket or disk):
* The operating system transitions the process from RUNNING to BLOCKED.
* While blocked:
** The server performs no computation.
** Incoming requests cannot be serviced.
* Once the I/O completes: The process transitions to READY, and later back to RUNNING.

Connection to the Table
|===
|Table Aspect	|Explanation
|Concurrency: None	|Only one execution context exists
|I/O Behavior: Blocking	|Direct cause of RUNNING ‚Üí BLOCKED
|Scalability: |Poor	Entire server is blocked on I/O
|OS Abstraction Used	|Sequential process
|===
The operating system fully manages execution state, but the server fails to exploit concurrency.

*2. Multi-Threaded Servers and Thread State Diagrams*

In a multi-threaded server, the process contains multiple threads, each with its own state machine:
----
Thread A: RUNNING ‚Üí BLOCKED ‚Üí READY
Thread B: READY ‚Üí RUNNING
Thread C: BLOCKED
----
Interpretation

* Each client request is typically handled by a separate worker thread.
* When one thread blocks on I/O:
** Only that thread enters the BLOCKED state.
** Other threads remain READY or RUNNING.
* The operating system scheduler multiplexes the CPU among runnable threads.

Connection to the Table
|===
|Table Aspect	|Explanation
|Concurrency: Provided by OS threads	|Multiple thread state machines
|Context Switching Overhead	|Frequent thread switches
|Memory Usage	|Stack per thread
|Programming Simplicity	|OS preserves execution context
|===
The OS manages many thread state transitions, which improves responsiveness but introduces overhead and scalability limits.

*3. Event-Driven Servers and State Diagrams*

Event-driven servers fundamentally alter the meaning of ‚Äústate.‚Äù

Key Observation: In event-driven servers, OS-level thread states remain almost constant, while application-level states change frequently.

OS-Level Thread State: The event loop thread behaves as follows:
----
RUNNING ‚Üí BLOCKED (epoll_wait) ‚Üí RUNNING
----
The thread blocks only while waiting for events. It never blocks on disk or network I/O.

Application-Level Request State (Finite-State Machine)

Each request is represented by a logical state machine, for example:
----
RECEIVED
‚Üì
WAITING_FOR_DISK
‚Üì
PROCESSING
‚Üì
WAITING_FOR_SOCKET
‚Üì
DONE
----
Transitions are triggered by events, not by blocking calls.

Connection to the Table
|===
|Table Aspect	|Explanation
|State Management: Explicit	|Request state stored manually
|OS Abstraction Used	|FSMs + events
|Context Switching	|Minimal (few threads)
|Scalability	|High (many FSMs, few threads)
|===
Here, state management shifts from the OS to the application.

*4. Unified View: Who Manages State?*

The three architectures differ primarily in where state is stored and who controls transitions.

* Single-threaded:
** OS manages all state transitions

* Multi-threaded:
** OS manages many independent thread state machines

* Event-driven:
** OS manages few threads
** Application manages many request FSMs

*5. Extended Summary Table (State Diagram Perspective)*

Below is an extended version of the comparison table, explicitly linked to state diagrams.

[cols="1,3,3,3", options="header"]
|===
| Aspect
| Single-Threaded Server
| Multi-Threaded Server
| Event-Driven Server

| OS-Level Execution Units
| One process, one thread
| One process, many threads
| One or few threads

| OS-Level State Diagram
| RUNNING ‚Üî BLOCKED ‚Üî READY
| Multiple independent thread state machines
| Event-loop thread mostly RUNNING/BLOCKED(epoll)

| Blocking Behavior
| Entire server blocks
| Only blocking thread blocks
| Threads never block on I/O

| Application-Level State
| Implicit in call stack
| Implicit per thread stack
| Explicit FSM per request

| Who Saves Execution Context
| Operating system
| Operating system
| Application

| Context Switch Frequency
| Low
| High (many threads)
| Very low

| Concurrency Mechanism
| None
| OS scheduler
| Event multiplexing

| Scalability Limiting Factor
| Blocking process
| Thread overhead
| FSM complexity

| Conceptual Model
| Sequential process
| Concurrent threads
| Finite-state machines

| Typical OS Analogy
| Single foreground task
| Multiprogramming
| Interrupt-driven kernel
|===

== Concurrency Models in User Programs and Operating System Kernels
The three approaches for handling client requests‚Äîsingle-threaded, multi-threaded, and event-driven‚Äîare not limited to user-level server programs. These models are equally applicable, and in many cases even more significant, within the operating system kernel itself. Concurrency is a fundamental requirement for kernel performance, responsiveness, and scalability, particularly on modern multiprocessor systems.

the operating system employs the same concepts internally. Indeed, many abstractions‚Äîsuch as threads, scheduling, synchronization, and event handling‚Äîare more critical to kernel implementation than to user-space applications.

Concurrency models such as multithreading and event-driven execution are not merely programming techniques for user applications; they are fundamental architectural choices that shape the structure and behavior of operating system kernels themselves.

The study of concurrency in user-level servers provides direct insight into kernel design: whether in Linux‚Äôs multithreaded kernel or MINIX 3‚Äôs event-driven microkernel architecture, operating systems apply the same principles‚Äîthreads, events, and finite-state machines‚Äîto meet their performance and reliability goals.

*Concurrency Inside the Kernel*

An operating system kernel must handle many activities concurrently, including:

* Interrupt handling
* System call execution
* Device I/O completion
* Scheduling decisions
* Memory management operations

As with servers, the kernel must decide how concurrency is structured: whether execution is serialized, divided among multiple threads, or driven by events.

*Multi-Threaded Kernels*

In a multi-threaded kernel, concurrency is achieved by executing kernel code in multiple threads of control. Each thread may independently execute kernel routines, often on different CPUs.

Example: Linux Kernel

The Linux kernel on modern Intel (x86) CPUs is a multi-threaded kernel:

* Each CPU can execute kernel code concurrently.
* Kernel threads exist for background tasks (e.g., flushing buffers, handling I/O).
* User threads entering the kernel via system calls execute kernel code in parallel with other threads.
* From a state-diagram perspective:
** Each kernel thread has its own execution state (RUNNING, READY, BLOCKED).
** The kernel relies heavily on synchronization mechanisms (spinlocks, mutexes) to protect shared data structures.

This design emphasizes parallelism and throughput, but also introduces complexity in correctness, particularly with respect to race conditions and deadlocks.

*Event-Driven Kernels*

In contrast, an event-driven kernel structures its execution around events and explicit state machines. Rather than relying heavily on threads, kernel components react to events such as interrupts, messages, or signals, and advance internal state accordingly.

Example: MINIX 3

MINIX 3 adopts an event-driven architecture composed of many small servers:

* Each server is implemented as a finite-state machine.
* Servers communicate via message passing.
* Execution proceeds in response to events rather than through long-running threads.

In this model:

* The kernel itself remains small and simple.
* Most operating system services (e.g., file system, device drivers) run as user-space servers.
* State is explicit, and transitions are driven by messages and events.

This design prioritizes reliability, fault isolation, and clarity of control flow over raw parallel performance.

*User Programs and Kernels: A Unified Perspective*

The key insight emphasized by this passage is that the same concurrency models recur at different system layers:
|===
|Layer	|Concurrency Mechanism
|User-level server	|Threads or events
|Kernel	|Threads or events
|Device handling	|Interrupts and FSMs
|===
The difference lies not in the concepts themselves, but in where they are applied and what constraints dominate (performance, safety, or simplicity).

*Design Trade-offs in Kernel Concurrency*

* Multi-threaded kernels (e.g., Linux):
** Maximize hardware utilization
** Scale well on multicore systems
** Require complex synchronization

* Event-driven kernels (e.g., MINIX 3):
** Favor explicit state machines
** Reduce shared-state complexity
** Improve robustness and fault tolerance
** May sacrifice some raw performance

== Date and Time
Two types of time are of interest to a process:

* Real time is measured either from some standard point (calendar time) or from
some fixed point, typically the start, in the life of a process (elapsed or wall clock
time). 
+
On UNIX systems, calendar time is measured in seconds since midnight
on the morning of January 1, 1970, Universal Coordinated Time (usually
abbreviated UTC), and coordinated on the base point for timezones defined
by the longitudinal line passing through Greenwich, England. This date, which
is close to the birth of the UNIX system, is referred to as the Epoch.
* Process time, also called CPU time, is the total amount of CPU time that a process
has used since starting. CPU time is further divided into 
** system CPU time, the
time spent executing code in kernel mode (i.e., executing system calls and per-
forming other kernel services on behalf of the process), 
** and user CPU time, the
time spent executing code in user mode (i.e., executing normal program code).

The time command displays the real time, the system CPU time, and user CPU time
taken to execute the processes in a pipeline.

== /proc File System
Like several other UNIX implementations, Linux provides a /proc file system,
which consists of a set of directories and files mounted under the /proc directory.

The /proc file system is a virtual file system that provides an interface to kernel
data structures in a form that looks like files and directories on a file system. This
provides an easy mechanism for viewing and changing various system attributes. In
addition, a set of directories with names of the form /proc/PID, where PID is a pro-
cess ID, allows us to view information about each process running on the system.

The contents of /proc files are generally in human-readable text form and can
be parsed by shell scripts. A program can simply open and read from, or write to,
the desired file. In most cases, a process must be privileged to modify the contents
of files in the /proc directory.

The /proc file system is not specified by any standards, and the
details that we describe are Linux-specific.

== Development
=== System calls
This table shows the entire lifecycle of a UNIX process:

* Creation ‚Üí fork, vfork, clone
* Program loading ‚Üí exec*
* Identity & permissions ‚Üí getuid, setuid, groups
* Scheduling & control ‚Üí nice, sched_yield
* Signals ‚Üí kill
* Termination & cleanup ‚Üí exit, _exit, wait*
[cols="2,4,5,5", options="header"]
|===
| Call method | Description | Parameters | Return value

| fork()
| Create a new process by duplicating the calling process
| _void_
| *pid_t*  
Child PID in parent, `0` in child, `-1` on error

| vfork()
| Create a child process sharing address space until `exec()` or `exit()`
| _void_
| *pid_t*  
Child PID in parent, `0` in child, `-1` on error

| clone()
| Create a process or thread with fine-grained control over shared resources
| `int (*fn)(void *)`, `void *child_stack`, `int flags`, `void *arg`, ...
| *pid_t*  
PID of child on success, `-1` on error

| execve()
| Replace current process image with a new program
| `const char *pathname`, `char *const argv[]`, `char *const envp[]`
| *int*  
Does not return on success, `-1` on error

| execv()
| Execute a program with argument vector
| `const char *path`, `char *const argv[]`
| *int*  
Does not return on success, `-1` on error

| execvp()
| Execute a program, searching `PATH`
| `const char *file`, `char *const argv[]`
| *int*  
Does not return on success, `-1` on error

| _exit()
| Terminate process immediately (no stdio cleanup)
| `int status`
| _void_  
Does not return

| exit()
| Terminate process after flushing stdio buffers
| `int status`
| _void_  
Does not return

| wait()
| Wait for any child process to terminate
| `int *wstatus`
| *pid_t*  
PID of terminated child, `-1` on error

| waitpid()
| Wait for a specific child process
| `pid_t pid`, `int *wstatus`, `int options`
| *pid_t*  
PID of child, `0` or `-1` depending on options

| waitid()
| Wait for process state changes with detailed info
| `idtype_t idtype`, `id_t id`, `siginfo_t *infop`, `int options`
| *int*  
`0` on success, `-1` on error

| getpid()
| Get calling process ID
| _void_
| *pid_t*  
Caller PID

| getppid()
| Get parent process ID
| _void_
| *pid_t*  
Parent PID

| getuid()
| Get real user ID of process
| _void_
| *uid_t*  
Real UID

| geteuid()
| Get effective user ID
| _void_
| *uid_t*  
Effective UID

| getgid()
| Get real group ID of process
| _void_
| *gid_t*  
Real GID

| getegid()
| Get effective group ID
| _void_
| *gid_t*  
Effective GID

| setuid()
| Set process user identity
| `uid_t uid`
| *int*  
`0` on success, `-1` on error

| setgid()
| Set process group identity
| `gid_t gid`
| *int*  
`0` on success, `-1` on error

| setsid()
| Create a new session and detach from terminal
| _void_
| *pid_t*  
Session ID, `-1` on error

| getsid()
| Get session ID of a process
| `pid_t pid`
| *pid_t*  
Session ID

| setpgid()
| Set process group ID
| `pid_t pid`, `pid_t pgid`
| *int*  
`0` on success, `-1` on error

| getpgid()
| Get process group ID
| `pid_t pid`
| *pid_t*  
Process group ID

| nice()
| Change process scheduling priority
| `int inc`
| *int*  
New nice value, `-1` on error

| sched_yield()
| Yield CPU voluntarily
| _void_
| *int*  
`0` on success

| kill()
| Send a signal to a process or group
| `pid_t pid`, `int sig`
| *int*  
`0` on success, `-1` on error
|===

=== System Call Details

==== fork()
Fork is the only way to create a new process in
POSIX. It creates an exact duplicate of the original process, including all the file descriptors, registers‚Äîeverything. After the fork, the original process and the copy
(the parent and child) go their separate ways. All the variables have identical val-
ues at the time of the fork, but since the parent‚Äôs data are copied to create the child,
subsequent changes in one of them do not affect the other one

The parent and child are almost identical:

|===
|What is copied?	|Meaning

|Memory image	|Same code + data

|Environment variables	|PATH, HOME, etc.

|Open files	|Same files, same positions
|===

the memory
of the child may be shared copy-on-write with the parent. This means that parent
and child share a single physical copy of the memory until one of the two modifies
a value at a location in memory‚Äîin which case the operating system makes a copy
of the small chunk of memory containing that location. Doing so minimizes the
amount of memory that needs to be copied a priori, as much can remain shared.
Moreover, part of the memory, for instance, the program text does not change at, so
it can always be shared between parent and child. The fork call returns a value,
which is zero in the child and equal to the child‚Äôs PID (Process IDentifier) in the
parent. Using the returned PID, the two processes can see which one is the parent
process and which one is the child process. Moreover, part of the memory, for instance, the program text does not change at, so
it can always be shared between parent and child.

The fork call returns a value,
which is zero in the child and equal to the child‚Äôs PID (Process IDentifier) in the
parent. Using the returned PID, the two processes can see which one is the parent
process and which one is the child process.

In most cases, after a fork, the child will need to execute different code from
the parent. Consider the case of the shell. It reads a command from the terminal,
forks off a child process, waits for the child to execute the command, and then
reads the next command when the child terminates. To wait for the child to finish,
the parent executes a waitpid system call, which just waits until the child terminates
(any child if more than one exists). Waitpid can wait for a specific child, or for any
old child by setting the first parameter to <1. When waitpid completes, the address
pointed to by the second parameter, statloc, will be set to the child process‚Äô exit
status (normal or abnormal termination and exit value). Various options are also
provided, specified by the third parameter. For example, returning immediately if
no child has already exited.

===== What it does
Creates a new process by duplicating the calling process.
The child initially shares memory pages via copy-on-write.

Think of it like: Making a photocopy of a running program

After fork() You have two processes, Parent (original), Child (copy)

===== Why it matters
This is the classic UNIX process creation primitive.
Shells, servers, and supervisors rely on it.

===== Syscall or libc wrapper?
*glibc wrapper*  
Implemented using the `clone()` syscall with fixed flags.

===== Kernel source paths
* Entry: `sys_clone`
* Files:
** `kernel/fork.c`
** `include/linux/sched/task.h`

===== glibc source paths
* `posix/fork.c`
* `sysdeps/unix/sysv/linux/fork.c`
* `sysdeps/unix/sysv/linux/clone.S`

===== Parameters
_None_

===== Return value
* `> 0` ‚Üí child PID (parent)
* `0` ‚Üí child process
* `-1` ‚Üí error (`errno` set)

===== Example
[source,c]
----
pid_t pid = fork();
if (pid ==== 0)
    execl("/bin/ls", "ls", NULL);
wait(NULL);
----

===== step-by-step code walk-through of fork()
. User-space code (what you write)
+
[source,c]
----
#include <unistd.h>

int main(void) {
    pid_t pid = fork();

    if (pid .. 0) {
        // Child
    } else {
        // Parent
    }
}
----
+
At this point:
+
** You are in **user mode**

** `fork()` is just a **function symbol**, not a syscall yet
+
The real work begins inside glibc.
. glibc: fork() wrapper
+
Key idea: `fork()` is **NOT a real syscall**, glibc implements it using the **clone() syscall** with special flags.
+
glibc source paths
+
** `posix/fork.c`
** `sysdeps/unix/sysv/linux/fork.c`
** `sysdeps/unix/sysv/linux/clone.S`
. glibc fork() (simplified)
+
[source,c]
----
pid_t
__libc_fork(void)
{
    return _Fork();
}
----
+
[source,c]
----
pid_t
_Fork(void)
{
    return __clone(SIGCHLD, 0);
}
----
+
Key points:
+
** glibc converts `fork()` ‚Üí `clone()`
** `SIGCHLD` flag means:
  - child sends SIGCHLD when it exits
  - parent/child get **separate address spaces**
. glibc: clone() syscall stub
+
Assembly transition to kernel, glibc now executes a **syscall instruction**.
+
glibc source path
+
** `sysdeps/unix/sysv/linux/clone.S`
+
[source,asm]
----
mov    $__NR_clone, %rax
syscall
----
+
At this moment:
** CPU switches from **user mode ‚Üí kernel mode**

** Control jumps into the Linux kernel syscall entry point
. Kernel entry point (architecture layer)
+
x86-64 example
+
Kernel source paths
+
** `arch/x86/entry/syscall_64.c`
** `arch/x86/entry/common.c`
+
[source,c]
----
SYSCALL_DEFINE0(clone)
{
    return do_fork(...);
}
----
+
What happens here:
+
* Registers are saved
* Syscall number is decoded
* Kernel dispatches to `sys_clone()`
. Kernel: sys_clone()
+
Kernel source path
+
** `kernel/fork.c`
+
[source,c]
----
SYSCALL_DEFINE5(clone, ...)
{
    return do_fork(clone_flags, stack_start,
                   ptid, ctid, tls);
}
----
+
This is where **process creation truly begins**.
. Kernel: do_fork()
+
Kernel source path
+
** `kernel/fork.c`
+
[source,c]
----
long do_fork(...)
{
    struct task_struct *p;

    p = copy_process(...);

    wake_up_new_task(p);

    return p->pid;
}
----
+
This function:
+
.. Creates a new `task_struct`
.. Copies process state
.. Schedules the child
.. Returns PID to parent
. Kernel: copy_process()
+
Kernel source path
+
** `kernel/fork.c`
+
This is the **heart of fork**.
+
Major steps inside `copy_process()`:
+
.. Allocate task_struct
+
[source,c]
----
p = dup_task_struct(current);
----
+
Each process is represented by a `task_struct`.
.. Copy memory (COW)
+
[source,c]
----
copy_mm(p);
----
+
*** Parent and child share memory pages
*** Pages are marked **copy-on-write**

*** Actual copying happens only on write
.. Copy file descriptors
+
[source,c]
----
copy_files(p);
----
+
*** Open files are shared
*** Reference counts are increased
.. Copy signal handlers
+
[source,c]
----
copy_sighand(p);
----
. Kernel: wake_up_new_task()
+
Kernel source path
+
** `kernel/sched/core.c`
+
[source,c]
----
wake_up_new_task(p);
----
+
This:
+
** Marks child runnable
** Inserts it into the scheduler run queue
+
Now:
+
** Parent continues execution
** Child is eligible to run
. Return path: kernel ‚Üí glibc ‚Üí user space
+
Parent return
+
** `sys_clone()` returns **child PID**

** glibc returns PID > 0
+
Child return
+
** Kernel sets return value to `0`
** Child resumes execution *after fork()*
+
This is how **one call returns twice**.
. Final illusion: "two processes ran the same code"
+
What really happened:
+
| Layer | Reality |
|------|--------|
| User | `fork()` returned twice |
| glibc | Used `clone()` |
| Kernel | Duplicated `task_struct` |
| MMU | Enabled copy-on-write |
| Scheduler | Ran parent & child |
. Full call trace
+
[source,text]
----
user main()
  ‚Üì
fork()
  ‚Üì
glibc fork()            posix/fork.c
  ‚Üì
glibc clone()           clone.S
  ‚Üì
syscall instruction
  ‚Üì
sys_clone()             kernel/fork.c
  ‚Üì
do_fork()
  ‚Üì
copy_process()
  ‚Üì
wake_up_new_task()
  ‚Üì
return to user space
----

---

==== clone()

===== What it does
Creates a new process or thread with configurable resource sharing.

===== Why it matters
Threads, containers, and modern runtimes depend on it.

===== Syscall or libc wrapper?
*True syscall*

===== Kernel source paths
* Entry: `sys_clone`
* Files:
** `kernel/fork.c`
** `kernel/sched/core.c`

===== glibc source paths
* `sysdeps/unix/sysv/linux/clone.S`
* `sysdeps/unix/sysv/linux/clone-internal.c`

===== Parameters
* `flags` ‚Äì resource sharing (VM, FS, files, signals)
* `child_stack` ‚Äì stack pointer
* `ptid`, `ctid`, `tls` ‚Äì thread bookkeeping

===== Return value
* child PID on success
* `-1` on error

===== Example
[source,c]
----
clone(fn, stack, CLONE_VM | CLONE_FS | SIGCHLD, arg);
----

=== Thread Management
NPTL can refer to the
Native POSIX Thread Library, the standard threading implementation for modern Linux, providing efficient, kernel-level threading (1:1 model) for parallel application
*Thread Management & Control*
[cols="1,3,3,3", options="header"]
|===
| Call Method | Description | Parameters | Return Type

| *pthread_create*
| Create a new thread
a|
* `pthread_t *thread`
* `const pthread_attr_t *attr`
* `void *(*start_routine)(void *)`
* `void *arg`
a|
* `int`
** `0` ‚Üí success
** Error number

| *pthread_exit*
| Terminate calling thread
a|
* `void *retval`
a|
* `void`

| *pthread_join*
| Wait for thread termination
a|
* `pthread_t thread`
* `void **retval`
a|
* `int`

| *pthread_detach*
| Detach thread
a|
* `pthread_t thread`
a|
* `int`

| *pthread_self*
| Get calling thread ID
a|
* (none)
a|
* `pthread_t`

| *pthread_equal*
| Compare thread IDs
a|
* `pthread_t t1`
* `pthread_t t2`
a|
* `int`

| *pthread_cancel*
| Request thread cancellation
a|
* `pthread_t thread`
a|
* `int`

| *pthread_setcancelstate*
| Enable/disable cancellation
a|
* `int state`
* `int *oldstate`
a|
* `int`

| *pthread_setcanceltype*
| Set cancellation type
a|
* `int type`
* `int *oldtype`
a|
* `int`

| *pthread_testcancel*
| Cancellation point
a|
* (none)
a|
* `void`

| *pthread_kill*
| Send signal to a thread
a|
* `pthread_t thread`
* `int sig`
a|
* `int`

| *pthread_once*
| One-time initialization
a|
* `pthread_once_t *once_control`
* `void (*init_routine)(void)`
a|
* `int`

|===

*Mutexes (Mutual Exclusion Locks)*

[cols="1,3,3,3", options="header"]
|===
| Call Method | Description | Parameters | Return Type

| *pthread_mutex_init*
| Initialize mutex
a|
* `pthread_mutex_t *mutex`
* `const pthread_mutexattr_t *attr`
a|
* `int`

| *pthread_mutex_destroy*
| Destroy mutex
a|
* `pthread_mutex_t *mutex`
a|
* `int`

| *pthread_mutex_lock*
| Lock mutex (blocking)
a|
* `pthread_mutex_t *mutex`
a|
* `int`

| *pthread_mutex_trylock*
| Try locking mutex
a|
* `pthread_mutex_t *mutex`
a|
* `int`
** `EBUSY` if locked

| *pthread_mutex_unlock*
| Unlock mutex
a|
* `pthread_mutex_t *mutex`
a|
* `int`

|===

*Condition Variables*
[cols="1,3,3,3", options="header"]
|===
| Call Method | Description | Parameters | Return Type

| *pthread_cond_init*
| Initialize condition variable
a|
* `pthread_cond_t *cond`
* `const pthread_condattr_t *attr`
a|
* `int`

| *pthread_cond_destroy*
| Destroy condition variable
a|
* `pthread_cond_t *cond`
a|
* `int`

| *pthread_cond_wait*
| Wait for condition
a|
* `pthread_cond_t *cond`
* `pthread_mutex_t *mutex`
a|
* `int`

| *pthread_cond_timedwait*
| Timed wait for condition
a|
* `pthread_cond_t *cond`
* `pthread_mutex_t *mutex`
* `const struct timespec *abstime`
a|
* `int`
** `ETIMEDOUT` possible

| *pthread_cond_signal*
| Wake one waiting thread
a|
* `pthread_cond_t *cond`
a|
* `int`

| *pthread_cond_broadcast*
| Wake all waiting threads
a|
* `pthread_cond_t *cond`
a|
* `int`

|===
*Thread-Local Storage (TLS)*
[cols="1,3,3,3", options="header"]
|===
| Call Method | Description | Parameters | Return Type

| *pthread_key_create*
| Create TLS key
a|
* `pthread_key_t *key`
* `void (*destructor)(void *)`
a|
* `int`

| *pthread_key_delete*
| Delete TLS key
a|
* `pthread_key_t key`
a|
* `int`

| *pthread_setspecific*
| Set TLS value
a|
* `pthread_key_t key`
* `const void *value`
a|
* `int`

| *pthread_getspecific*
| Get TLS value
a|
* `pthread_key_t key`
a|
* `void *`

|===

==== Thread functions

===== pthread_create

**What it does**

- Spawns a new thread in the calling process.

**Why it matters**

- Fundamental API to start concurrent execution within a process.

**Parameters**

- **thread**: out, pointer to store new thread ID.
- **attr**: in, pointer to thread attributes object; `NULL` for defaults.
- **start_routine**: in, function the thread will execute.
- **arg**: in, argument to pass to `start_routine`.

**Return value**

- `0` on success.
- Error codes like `EINVAL`, `EAGAIN`, `EPERM`.

**Implementation (glibc)**

- [glibc pthread_create.c](https://codebrowser.dev/glibc/glibc/nptl/pthread_create.c.html)

**Kernel / syscall mapping**

- Uses `clone(2)` with `CLONE_VM | CLONE_THREAD | CLONE_SIGHAND | CLONE_FS | CLONE_FILES` flags.
- See [clone(2) man page](https://man7.org/linux/man-pages/man2/clone.2.html)

===== pthread_join

**What it does**

- Waits for a thread to terminate and retrieves its return value.

**Parameters**

- **thread**: in, ID of the thread to wait for.
- **retval**: out, pointer to location for thread‚Äôs exit value; can be `NULL`.

**Return value**

- `0` on success.
- Errors: `EINVAL` if thread not joinable, `ESRCH` if thread does not exist.

**Implementation**
- Uses futex wait/wake internally to block efficiently until thread exits.

===== pthread_exit

**What it does**

- Terminates execution of the calling thread.

**Parameters**

- **retval**: in, value returned to any joining thread.

**Return value**

- Does not return.

**Implementation**
- Teardown notifies waiters via futex wake on internal thread control block.

===== pthread_detach

**What it does**

- Marks a thread as detached so its resources are automatically reclaimed.

**Parameters**

- **thread**: in, ID of the thread to detach.

**Return value**

- `0` on success, `EINVAL` if invalid thread.

===== pthread_self & pthread_equal

**What they do**
- `pthread_self()` returns calling thread‚Äôs ID.
- `pthread_equal(t1,t2)` compares two thread IDs.

**Return value**

- `pthread_self()`: thread ID
- `pthread_equal()`: nonzero if equal, 0 otherwise

===== Mutex API

====== pthread_mutex_init / destroy

**What it does**

- Initialize or destroy a mutex.

**Parameters**

- **mutex**: pointer to mutex object
- **attr**: pointer to mutex attributes; can be NULL

**Return value**

- `0` on success, `EBUSY` or `EINVAL` on error

**Implementation**
- [glibc pthread_mutex_init.c](https://codebrowser.dev/glibc/glibc/nptl/pthread_mutex_init.c.html)
- Uses `futex(2)` on contention.

====== pthread_mutex_lock / trylock / unlock

**What it does**

- Locks or unlocks a mutex.

**Return value**

- `0` on success
- Blocks until lock acquired for `lock()`, returns `EBUSY` for `trylock()` if unavailable

**Kernel mapping**
- Contended locks use `futex_wait` and `futex_wake`

===== Condition Variables

====== pthread_cond_init / destroy

- Initializes or destroys a condition variable.

====== pthread_cond_wait / timedwait / signal / broadcast

**What it does**

- Wait releases mutex and blocks until signaled.
- Signal wakes one waiter; broadcast wakes all.

**Kernel mapping**
- Uses `futex(2)` internally for waiting and wake-up.

**Implementation example**
- [glibc pthread_cond_wait.c](https://codebrowser.dev/glibc/glibc/nptl/pthread_cond_wait.c.html)

===== Read‚ÄìWrite Locks

**Functions**
- `pthread_rwlock_init`, `pthread_rwlock_destroy`
- `pthread_rwlock_rdlock`, `pthread_rwlock_wrlock`
- `pthread_rwlock_tryrdlock`, `pthread_rwlock_trywrlock`
- `pthread_rwlock_unlock`

**Kernel mapping**
- Multiple counters and futexes used for reader/writer queues.

===== Barriers

**Functions**
- `pthread_barrier_init`
- `pthread_barrier_wait`
- `pthread_barrier_destroy`

**Use**
- Synchronizes a group of threads to rendezvous before proceeding.

===== Thread Attributes (pthread_attr_*)

**Functions**
- `pthread_attr_init`
- `pthread_attr_destroy`
- `pthread_attr_setdetachstate`
- `pthread_attr_setschedpolicy`
- `pthread_attr_setschedparam`

===== Scheduling & CPU Affinity

**Functions**
- `pthread_setschedparam`, `pthread_getschedparam`
- `pthread_setaffinity_np`, `pthread_getaffinity_np`

**Kernel mapping**
- `sched_setaffinity(2)`, `sched_setscheduler(2)`

===== POSIX vs Linux NPTL Mapping

[cols="1,1,1", options="header"]
|===
|POSIX API (pthread_*) | glibc NPTL Internals | Kernel Syscalls
|Thread creation | pthread_create ‚Üí clone(2) | clone(2) with CLONE_THREAD, CLONE_VM, CLONE_FS, CLONE_FILES
|Mutexes & Conditional Waits | pthread_mutex_lock ‚Üí futex(2) | futex(2) (FUTEX_WAIT, FUTEX_WAKE)
|Scheduling & Affinity | pthread_setschedparam | sched_setaffinity(2), sched_setscheduler(2)
|===

===== References

- [glibc pthread_create.c source](https://codebrowser.dev/glibc/glibc/nptl/pthread_create.c.html)
- [glibc pthread_mutex_init.c source](https://codebrowser.dev/glibc/glibc/nptl/pthread_mutex_init.c.html)
- [glibc pthread_cond_wait.c source](https://codebrowser.dev/glibc/glibc/nptl/pthread_cond_wait.c.html)
- [linux kernel futex.c source](https://elixir.bootlin.com/linux/latest/source/kernel/futex.c)
- [clone(2) man page](https://man7.org/linux/man-pages/man2/clone.2.html)
- [pthread(7) man page](https://man7.org/linux/man-pages/man7/pthreads.7.html)


==== POSIX Pthread Attribute APIs
[cols="1,3,3,3", options="header"]
|===
| Call Method | Description | Parameters | Return Type

| *pthread_attr_init*
| Initialize thread attribute object
a|
* `pthread_attr_t *attr`
a|
* `int`

| *pthread_attr_destroy*
| Destroy thread attribute object
a|
* `pthread_attr_t *attr`
a|
* `int`

| *pthread_attr_setdetachstate*
| Set detach state
a|
* `pthread_attr_t *attr`
* `int detachstate`
** `PTHREAD_CREATE_JOINABLE`
** `PTHREAD_CREATE_DETACHED`
a|
* `int`

| *pthread_attr_getdetachstate*
| Get detach state
a|
* `const pthread_attr_t *attr`
* `int *detachstate`
a|
* `int`

| *pthread_attr_setstacksize*
| Set thread stack size
a|
* `pthread_attr_t *attr`
* `size_t stacksize`
a|
* `int`

| *pthread_attr_getstacksize*
| Get thread stack size
a|
* `const pthread_attr_t *attr`
* `size_t *stacksize`
a|
* `int`

| *pthread_attr_setguardsize*
| Set guard size
a|
* `pthread_attr_t *attr`
* `size_t guardsize`
a|
* `int`

| *pthread_attr_getguardsize*
| Get guard size
a|
* `const pthread_attr_t *attr`
* `size_t *guardsize`
a|
* `int`

| *pthread_attr_setinheritsched*
| Set scheduling inheritance
a|
* `pthread_attr_t *attr`
* `int inheritsched`
** `PTHREAD_INHERIT_SCHED`
** `PTHREAD_EXPLICIT_SCHED`
a|
* `int`

| *pthread_attr_getinheritsched*
| Get scheduling inheritance
a|
* `const pthread_attr_t *attr`
* `int *inheritsched`
a|
* `int`

| *pthread_attr_setschedpolicy*
| Set scheduling policy
a|
* `pthread_attr_t *attr`
* `int policy`
** `SCHED_OTHER`
** `SCHED_FIFO`
** `SCHED_RR`
a|
* `int`

| *pthread_attr_getschedpolicy*
| Get scheduling policy
a|
* `const pthread_attr_t *attr`
* `int *policy`
a|
* `int`

| *pthread_attr_setschedparam*
| Set scheduling parameters
a|
* `pthread_attr_t *attr`
* `const struct sched_param *param`
a|
* `int`

| *pthread_attr_getschedparam*
| Get scheduling parameters
a|
* `const pthread_attr_t *attr`
* `struct sched_param *param`
a|
* `int`

| *pthread_mutexattr_init*
| Initialize mutex attribute object
a|
* `pthread_mutexattr_t *attr`
a|
* `int`

| *pthread_mutexattr_destroy*
| Destroy mutex attribute object
a|
* `pthread_mutexattr_t *attr`
a|
* `int`

| *pthread_mutexattr_settype*
| Set mutex type
a|
* `pthread_mutexattr_t *attr`
* `int type`
** `PTHREAD_MUTEX_NORMAL`
** `PTHREAD_MUTEX_RECURSIVE`
** `PTHREAD_MUTEX_ERRORCHECK`
a|
* `int`

| *pthread_mutexattr_gettype*
| Get mutex type
a|
* `const pthread_mutexattr_t *attr`
* `int *type`
a|
* `int`

| *pthread_mutexattr_setprotocol*
| Set mutex priority protocol
a|
* `pthread_mutexattr_t *attr`
* `int protocol`
** `PTHREAD_PRIO_NONE`
** `PTHREAD_PRIO_INHERIT`
** `PTHREAD_PRIO_PROTECT`
a|
* `int`

| *pthread_mutexattr_getprotocol*
| Get mutex priority protocol
a|
* `const pthread_mutexattr_t *attr`
* `int *protocol`
a|
* `int`

| *pthread_condattr_init*
| Initialize condition attribute object
a|
* `pthread_condattr_t *attr`
a|
* `int`

| *pthread_condattr_destroy*
| Destroy condition attribute object
a|
* `pthread_condattr_t *attr`
a|
* `int`

| *pthread_condattr_setclock*
| Set clock used by condition variable
a|
* `pthread_condattr_t *attr`
* `clockid_t clock_id`
a|
* `int`

| *pthread_condattr_getclock*
| Get clock used by condition variable
a|
* `const pthread_condattr_t *attr`
* `clockid_t *clock_id`
a|
* `int`

| *pthread_rwlockattr_init*
| Initialize rwlock attribute object
a|
* `pthread_rwlockattr_t *attr`
a|
* `int`

| *pthread_rwlockattr_destroy*
| Destroy rwlock attribute object
a|
* `pthread_rwlockattr_t *attr`
a|
* `int`

| *pthread_barrierattr_init*
| Initialize barrier attribute object
a|
* `pthread_barrierattr_t *attr`
a|
* `int`

| *pthread_barrierattr_destroy*
| Destroy barrier attribute object
a|
* `pthread_barrierattr_t *attr`
a|
* `int`

|===

==== mapping of pthread attributes ‚Üí actual kernel behavior
===== POSIX Pthread Attributes ‚Üí Linux Kernel Behavior (glibc / NPTL)

This section maps **each pthread attribute to its actual behavior on Linux**, clearly indicating what is **enforced by the kernel**, what is **partially honored**, and what is **ignored**, followed by **practical usage guidance**.

====== Thread Attributes (`pthread_attr_*`)

*Detach State*

*Kernel behavior*::
- `PTHREAD_CREATE_JOINABLE`  
  Thread exit status is retained until `pthread_join()` is called.
- `PTHREAD_CREATE_DETACHED`  
  Thread resources are released immediately on exit.

*Linux implementation*::
- Managed entirely by glibc/NPTL; no kernel-level join object exists.

*Usage*::
- Use detached threads for fire-and-forget workers.
- Avoid detached threads when return values or synchronization are required.

---

*Stack Size (`pthread_attr_setstacksize`)*

*Kernel mapping*::
- Stack allocated using `mmap()`
- Marked as `VM_STACK`
- Guard page enforced via `mprotect(PROT_NONE)`

*Linux notes*::
- Minimum size enforced by glibc (typically ‚â• 16 KB).
- Large stacks consume virtual address space, not physical memory.

*Usage*::
- Increase stack size for deep recursion or large local variables.
- Avoid excessively large stacks.

---

*Guard Size (`pthread_attr_setguardsize`)*

*Kernel behavior*::
- Guard pages cause `SIGSEGV` on stack overflow.

*Linux notes*::
- Implemented via `mprotect()`
- Fully enforced by the kernel.

*Usage*::
- Always keep guard pages enabled.
- Never disable in production.

---

*Scheduling Policy (`pthread_attr_setschedpolicy`)*

*Kernel mapping*::
- `SCHED_OTHER` ‚Üí CFS scheduler
- `SCHED_FIFO`, `SCHED_RR` ‚Üí Real-time scheduler

*Linux restrictions*::
- Requires `CAP_SYS_NICE`
- Non-privileged calls fail with `EPERM`

*Usage*::
- Use only in real-time systems.
- Avoid in general-purpose applications.

---

*Scheduling Parameters (`pthread_attr_setschedparam`)*

*Kernel behavior*::
- `sched_priority` maps to `task_struct.prio`

*Linux notes*::
- Ignored for `SCHED_OTHER`
- Meaningful only for RT policies

*Usage*::
- Only use together with real-time scheduling policies.

---

*Scheduling Inheritance (`pthread_attr_setinheritsched`)*

*Linux behavior*::
- `PTHREAD_INHERIT_SCHED` inherits creator‚Äôs scheduler.
- `PTHREAD_EXPLICIT_SCHED` uses explicitly set values.

*Linux note*::
- Has no effect unless scheduling policy is explicitly changed.

---

===== Mutex Attributes (`pthread_mutexattr_*`)

*Mutex Type (`pthread_mutexattr_settype`)*

*Kernel mapping*::
- Implemented using futexes.

*Types*::
- `PTHREAD_MUTEX_NORMAL` ‚Äì fastest, minimal checks
- `PTHREAD_MUTEX_ERRORCHECK` ‚Äì detects deadlocks
- `PTHREAD_MUTEX_RECURSIVE` ‚Äì tracks owner and lock count

*Usage*::
- Use NORMAL for performance.
- Use ERRORCHECK for debugging.
- Avoid RECURSIVE in hot paths.

---

*Priority Protocol (`pthread_mutexattr_setprotocol`)*

*Kernel behavior*::
- `PTHREAD_PRIO_INHERIT` is implemented via `rt_mutex`.
- `PTHREAD_PRIO_PROTECT` is mostly ignored.
- Default is `PTHREAD_PRIO_NONE`.

*Usage*::
- Use priority inheritance only in RT systems.
- Avoid in non-RT code.

---

===== Condition Variable Attributes (`pthread_condattr_*`)

*Clock Selection (`pthread_condattr_setclock`)*

*Kernel mapping*::
- Timeout handled using high-resolution timers (`hrtimer`).

*Clocks*::
- `CLOCK_REALTIME` ‚Äì affected by system time changes.
- `CLOCK_MONOTONIC` ‚Äì stable, preferred.

*Usage*::
- Always use `CLOCK_MONOTONIC` for timed waits.

---

===== RWLock Attributes (`pthread_rwlockattr_*`)

*Linux behavior*::
- Mostly ignored.
- RW locks are largely implemented in userspace with futex assistance.

*Usage*::
- Do not rely on rwlock attributes.
- Use defaults.

---

===== Barrier Attributes (`pthread_barrierattr_*`)

*Linux behavior*::
- Fully ignored.
- Barriers are purely userspace constructs.

*Usage*::
- Attributes provide no benefit on Linux.

---

===== Summary: What Linux Actually Enforces

* Fully enforced:
** Stack size and guard size
** Detach state
** Mutex type
** Mutex priority inheritance
** Condition variable clock

* Partially enforced / privileged:
** Scheduling policy
** Scheduling priority

* Ignored:
** RWLock attributes
** Barrier attributes

[cols="3,1,1,1", options="header"]
|===
| Attribute Category | Enforced | Partially | Ignored

| Stack / Guard
| ‚úÖ
| ‚Äî
| ‚Äî

| Detach State
| ‚úÖ
| ‚Äî
| ‚Äî

| Scheduling Policy
| ‚ö†
| ‚Äî
| ‚Äî

| Scheduling Priority
| ‚ö†
| ‚Äî
| ‚Äî

| Mutex Type
| ‚úÖ
| ‚Äî
| ‚Äî

| Mutex Priority Inheritance
| ‚úÖ
| ‚Äî
| ‚Äî

| Condition Clock
| ‚úÖ
| ‚Äî
| ‚Äî

| RWLock Attributes
| ‚Äî
| ‚ö†
| ‚úÖ

| Barrier Attributes
| ‚Äî
| ‚Äî
| ‚úÖ

|===

---

===== Practical Guidance

*Use attributes when*::
- Writing real-time or embedded systems
- Debugging concurrency issues
- Controlling memory layout precisely

*Avoid attributes when*::
- Writing general-purpose Linux applications
- Portability is a primary concern
- Default behavior is sufficient

*Rule of thumb*::
If an attribute does not affect **memory layout, scheduling, or futex behavior**, Linux likely ignores it.
